%\documentclass[12pt]{article}
\documentclass[10pt,preprint]{sigplanconf}
% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{amsthm}
\usepackage{ stmaryrd }
\usepackage{mathpartir}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\input{macros-catlam}

\usepackage{times}
\renewcommand{\ttdefault}{txtt}
\usepackage{alltt}
\usepackage{listings}
\lstset{language=ML,
showstringspaces=false,
basicstyle=\ttfamily\footnotesize,
morekeywords={newcase,extends}}

\usepackage{url}
\usepackage{todonotes}
\lefthyphenmin=5
\sloppy

\newcommand{\moutput}{^{\color{gray}+}}
\newcommand{\rulename}[1]{(#1)}
\def \TirNameStyle #1{\small\rulename{#1}}
\renewcommand{\MathparLineskip}{\lineskiplimit=.3\baselineskip\lineskip=.3\baselineskip plus .2\baselineskip}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{tyconinvariant}{Tycon Invariant}
\newenvironment{proof-sketch}{\noindent{\emph{Proof Sketch.}}}{\qed}
\makeatletter

% Heather-added packages for the fancy table
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{colortbl}%
\newcommand{\myrowcolour}{\rowcolor[gray]{0.925}}
\usepackage{wasysym}

\renewcommand\topfraction{0.85}
\renewcommand\bottomfraction{0.85}
\renewcommand\textfraction{0.1}
\renewcommand\floatpagefraction{0.85}

\AtBeginDocument{%
 \abovedisplayskip=4pt
 \abovedisplayshortskip=4pt
 \belowdisplayskip=4pt
 \belowdisplayshortskip=4pt
}

\usepackage[compact]{titlesec}  
\titlespacing{\section}{0pt}{6pt}{2pt}
\titlespacing{\subsection}{0pt}{3pt}{0pt}
\titlespacing{\subsubsection}{0pt}{3pt}{0pt}
\titlespacing{\paragraph}{0pt}{3pt}{5pt}

\begin{document}

\conferenceinfo{-}{-} 
\copyrightyear{-} 
\copyrightdata{[to be supplied]} 

%\titlebanner{{\tt \textcolor{Red}{{\small Under Review -- distribute within CMU only.}}}}        % These are ignored unless
%\preprintfooter{Distribute within CMU only.}   % 'preprint' option specified.

\title{Modular Type Constructors}
\subtitle{Conservatively Composing Typed Language Fragments}

\authorinfo{}{}{}
%\authorinfo{Cyrus Omar \and Jonathan Aldrich}
 %         {Carnegie Mellon University}
  %         {\{comar, aldrich\}@cs.cmu.edu}   

\maketitle
\begin{abstract}
Researchers often describe type systems as fragments or simple calculi, leaving to language designers the task of composing these to form complete programming  languages.  %comparable to those available for library-based embeddings packaged using a modern module system. 
This is not a systematic process: metatheoretic results must be established anew for each composition, guided only notionally by metatheorems derived for simpler systems.
As the language design space grows, mechanisms that provide stronger modular reasoning principles than this are needed.%, so that important metatheoretic results need only be established locally and, ultimately, so that language extensions can be treated like libraries. 
%,  Sometimes this fails because certain language features do not ``play well'' together, but precisely characterizing which is elusive given these practices.
%Recent work has started to address this problem, e.g. by showing how to modularly reason about desugarings atop a fixed type system. Our focus here is on safely composing type system fragments. We organize fragments around type constructors, as is usual practice when describing type systems, and sidestep the difficulties of composing abstract syntax (i.e. the expression problem)  by delegating control over a small, uniform abstract syntax in a type-directed manner to logic associated with a type constructor.

In this paper, we take a foundational approach, specifying an extensible typed translation semantics, @$\lambda$. Only the $\rightarrow$ type constructor is built in; all others (we discuss constrained strings and variants of record types) are defined by extending a \emph{tycon context}. Each tycon defines associated term-level \emph{opcons} (e.g. row projection) using a static language where types and translations are values. The semantics come with strong metatheoretic guarantees, notably \emph{type safety} and \emph{conservativity}: that \emph{tycon-specific  invariants} established in any ``closed world'' are conserved in the ``open world''. Remarkably, extension providers do not need to  provide the semantics with mechanized specifications or proofs. Instead, these guarantees arise from a form of translation validation that, taking inspiration from the ML module system, uses type abstraction to check that the opcons associated with a tycon respect the \emph{translation independence} of all others.% We can thus call them \emph{modular tycons}. 
\end{abstract}

%\category{D.3.2}{Programming Languages}{Language Classifications}[Extensible Languages]
%\category{D.3.4}{Programming Languages}{Processors}[Compilers]
%\category{F.3.1}{Logics \& Meanings of Programs}{Specifying and Verifying and Reasoning about Programs}[Specification Techniques]
%\keywords
%extensible languages; module systems; type abstraction; typed compilation; type-level computation
\section{Introduction}\label{intro}
Typed programming languages are generally described as being composed from \emph{fragments}, each contributing to the language's concrete syntax, abstract syntax, static semantics and dynamic semantics. 
In his textbook, Harper organizes such fragments around type constructors, describing each in a different chapter \cite{pfpl}. Languages are then identified by a set of these type constructors, e.g. $\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\keyw{1}}\,{\times}\,{+}\}$ is the language of partial function types, polymorphic types, recursive types, nullary and binary product types and binary sum types (its syntax is shown in Figure \ref{syntax-IL}, discussed below).
Another common practice is to describe a fragment using a simple calculus having a ``catch-all'' constant and base type to stand notionally for all other terms and types that may also be included in some future complete language (e.g. \cite{sanitation-psp14}).

In contrast, the usual metatheoretic reasoning techniques for programming languages  (e.g., rule induction) operate on complete language specifications. Each {combination} of fragments must formally be treated as its own monolithic language for which  metatheorems must be established anew, guided only informally by those derived for the smaller systems from which the language is notionally composed.

%In both cases, it is left as an extrinsic concern to ensure that the metatheory developed separately is actually conserved when fragments defined in these ways are composed to form a complete language.

This is not an everyday problem for programmers only because fragments like those mentioned above are ``general purpose'': they make it possible to \emph{isomorphically embed}  many other fragments as ``libraries''. For example, list types need not be built in because they are isomorphic to the type $\iforall{\alpha}{\imu{t}{\iunit + (\alpha \times t))}}$ (datatypes in ML combine these into a single declaration construct). %Languages providing datatypes al\'a ML  are perhaps most directly oriented around  embeddings into this core. % Leaving the core language simple makes it easier to establish its metatheory and verify that tools, like compilers, are implemented correctly.% They also have encouraging connections to logic. %For example, we do not need to define the type constructor $\fvar{list}$ (indexed by a type) as a fragment (though it is possible to do so) because there is a user-defined datatype constructor, $\texttt{list}$, parameterized by a type, that is isomorphic. If we added the $\fvar{list}$ fragment to the language, it would be entirely redundant semantically: every well-typed term of a type constructed by $\fvar{list}$ corresponds to a well-typed term and a polymorphic recursive sum type such th%Although strings and numbers can be embedded as recursive sum types, it is recognized that this is impractical, so these are also usually included as primitives.
%Establishing an isomorphic embedding of a desirable fragment in terms of general-purpose fragments is not always possible, nor are such embeddings always practical. 

Universality properties (e.g. ``Turing completeness'') are often enough to guarantee that an embedding that preserves a desirable fragment's dynamic semantics can be constructed, but an isomorphic embedding must also preserve the static semantics and, if defined, performance bounds specified by a cost semantics. This is not always possible. For example, in $\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\keyw{1}}\,{\times}\,{+}\}$, it is impossible to introduce record types as a library because this requires introducing row projection operators,  written $\opname{\#}\conclbl{lbl}$ in ML, one for each of the infinite set of row labels $\conclbl{lbl}$ ($\opname{\#}$  is thus an \emph{operator constructor}). Each time such a situation occurs,  a new \emph{dialect} is needed. The ML lineage, like others, faces a proliferation of dialects:%, showing that this is not an isolated example:
%Reynolds, in a remark that recalls the ``Turing tarpit'' of Perlis \cite{Perl82a}, summarizes the issue \cite{Reynolds94anintroduction}: 
%\begin{quote}
%To say that any reasonable function can be expressed by some program is not to say that it can be expressed by the most reasonable program. It is clear that the language requires a novel programming style. Moreover, it is likely that certain important functions cannot be expressed by their most efficient algorithms.
%\end{quote}
%%You can only use $\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\keyw{1}}\,{\times}\,{+}\}$ as a \emph{target of compilation} for a language with record types (they can be implemented using nested binary products,  lists, or other types). %(and attempting to simulate records via a hash table over strings plus a refinement type system to check that the lookup operation will not fail will not satisfy a cost semantics prescribing $\mathcal{O}(1)$ lookup; see Sec. \ref{prior-work}).

%Embeddings are also sometimes too \emph{complex}, as measured by the cost of the extralinguistic computations that are needed to map in and out of the embedding and, if these  must be performed mentally by programmers, considering various human factors. %We will discuss specific examples below.
%When an embedding is too complex, abstraction providers have a few options, discussed in Sec. \ref{prior-work}. When an embedding is not possible, however, or when these options are  insufficient, 
%providers are often compelled to introduce these fragments by extending an existing language, thereby forming a new \emph{dialect}. To save effort, they may do so by forking existing artifacts or leverage tools like compiler generators or language frameworks, also reviewed in Sec. \ref{prior-work}. %Indeed, the proliferation of language dialects constructed for this reason might be taken as an evidence that the core language is not as ``general'', when considered comprehensively, as might be hoped. 


\begin{enumerate}
\vspace{-5px}
%\compresslist
\item 
\textbf{General Purpose Fragments:} 
Many variations on product types, for example, exist: 
$n$-ary tuples, 
labeled tuples, 
records (identified up to reordering), and 
records with width and depth subtyping \cite{Cardelli:1984:SMI:1096.1098}, functional update operators\footnote{The Haskell wiki notes that ''No, extensible records are not implemented in GHC. The problem is that the record design space is large, and seems to lack local optima. [...] As a result, nothing much happens.'' \cite{GHCFAQ}} \cite{ocaml-manual}, 
 mutable fields \cite{ocaml-manual}, 
%field delegation \cite{atlang-gpce14} \todo{gpce submission} 
and 
 ``methods'' (i.e. pure objects) \cite{TSLs}. 
 Sum-like types are also exposed variously: standard datatypes, 
open datatypes \cite{conf/ppdp/LohH06,journals/toplas/MillsteinBC04}, 
polymorphic variants \cite{ocaml-manual} and 
exception types \cite{harper1997programming}. Combinations occur as class-based object systems \cite{ocaml-manual}. 

\item
\textbf{Specialized Fragments:} Fragments that track specialized static invariants are also frequently introduced in dialects, e.g. for distributed programming \cite{Murphy:2007:TDP:1793574.1793585}, reactive programming \cite{mandel2005reactiveml}, databases \cite{Ohori:2011:MSM:2034773.2034815},  units of measure \cite{conf/cefp/Kennedy09} and regular string sanitation \cite{sanitation-psp14}, amongst many others. %\verb|sprintf| in the OCaml dialect statically distinguishes format strings from strings.% All of these are implemented as dialects of existing languages, presumably because a strong encoding was not feasible.
\vspace{-3px}
\item
\textbf{Foreign Fragments:} A safe and natural foreign function interface (FFI) can be valuable (particularly given this proliferation of dialects). This requires enforcing the type system of the foreign language in the calling language. %Using  a FFI that does not do this can lead to safety issues, even when both languages are separately known to be safe. 
%Safe FFIs generally require direct extensions to the language. 
For example, MLj builds in a safe FFI to Java \cite{Benton:1999:IWW:317636.317791}.
\end{enumerate}
\vspace{-5px}


This \emph{dialect-oriented} state of affairs is unsatisfying. %Language designers are burdened with needing to understand the complete metatheory of every fragment they add to their language to make sure it is not incompatible with existing fragments, so languages change rarely. This decreases the impact of many potentially useful innovations designed by fragment providers (e.g. many academic researchers) by making them unavailable to programmers. 
While programmers can choose from dialects supporting, e.g., a principled approach to distributed programming, or one that builds in support for statically reasoning about units of measure, one that supports both fragments may not be available. Using different dialects separately for different components of a program is untenable: components written in different dialects cannot always interface safely (i.e. a safe FFI, item 3 above, is needed between every pair of dialects). %And as just mentioned, composing dialects is non-trivial.

These problems do not arise for fragment expressed as an isomorphic embedding (i.e. as a library) because modern \emph{module systems} can enforce abstraction barriers that ensure that the isomorphism needs only to be established in the ``closed world'' of the module. For example, a module defining sets in ML can hold the representation of sets abstract, ensuring that any invariants necessary to maintain the isomorphism need only be maintained by the functions in the module (e.g. uniqueness, if using a list representation). These then continue to hold no matter which other modules are in use by a client \cite{harper1997programming}. Other languages expose similar forms of \emph{abstract data types} that also localize reasoning \cite{liskov1974programming}. %Mechanisms that can help decrease the complexity of an embedding without violating abstraction barriers are thus valuable, and we will lead into our work in Sec. \ref{prior-work} by summarizing them. 

The alarming proliferation of dialects above suggests that mechanisms  that make it possible to define and  reason in a similarly modular, localized manner about  direct extensions to the semantics of a language are needed. For example, if a language is extended with \emph{regular string types} as described in \cite{sanitation-psp14}, all terms having a regular string type like $\sty{\tcvar{rstr}}{\concrx{.+}}$ should continue to behave as non-empty strings no matter which other extensions are in use. %Such a mechanism could ultimately be integrated directly into the language, blurring the distinction between fragments and libraries and decreasing the need for new dialects.% Importing fragments that introduce new semantics   would be as easy and reasonable as importing a new module is in ML today. %In the limit, the community could rely on modularly mechanized metatheory and compiler correctness results.% rather than requiring heroic efforts from individual research groups that consider an entire language at once. %Recent work has shown progress on modularly introducing new concrete syntax, reviewed in Sec. \ref{desugaring}. Our focus is on the problem of introducing new semantics.




\paragraph{Contributions} In this paper, we take foundational steps toward this goal by  constructing a simple but surprisingly powerful core calculus, @$\lambda$ (the ``actively typed'' lambda calculus). %Despite its minimality, it can host a variety of practical semantic extensions like those described above, while maintaining strong metatheoretic guarantees and, crucially, providing modular reasoning properties. %a calculus introduced briefly in recent work on \emph{active type constructors}  \cite{atlang-gpce14}\todo{TR? Arxiv?}, reviewed in Sec. \ref{overview}. 
Its semantics are structured like those of many modern languages, consisting of an \emph{external language} (EL) governed by a {typed translation semantics} targeting a much simpler \emph{internal language} (IL). 
Rather than building in a monolithic set of external type constructors, however, the  semantics are indexed by a \emph{tycon context}. Each tycon, e.g. $\tcvar{lprod}$ defining labeled products or $\tcvar{rstr}$ defining regular strings, determines the semantics of its associated opcons, e.g. $\opname{\#}$ for row projection, or $\opname{conc}$ for concatenation, via \emph{static functions}, i.e. functions written in a \emph{static language} (SL), where types and translations are values. In other words, the semantics are controlled by a form of static code generation.

We call a tycon associated with opcon definitions a \emph{modular tycon} because all translation invariants maintained by the opcons associated with a tycon in any ``closed world'', e.g. the regular string invariant just mentioned under the tycon context defining only $\tcvar{rstr}$, are necessarily maintained in any further extended tycon context, i.e. in the ``open world'', due to a simple static check that maintains \emph{translation independence} between tycons using type abstraction in the IL, the same fundamental mechanism underlying representation independence in ML-style module systems. As in ML, mechanized specifications and proofs are not needed: the modularity guarantee is for all invariants.%, which provides a form of type-level computation. %The authors demonstrate the expressive power of this technique primarily with an implementation and a number of examples of fragments as libraries, and their core calculus .% (unlike systems that treat the type system as a ``bag of rules'', where non-determinism can arise). 

% We will begin by giving an overview of  @$\lambda$ in Sec. \ref{atlam}, introducing our two main examples, one defining labeled product types (i.e. record types maintaining an ordering for simplicity) with a functional update operator, and the other regular string types, based on a recent core calculus style specification \cite{sanitation-psp14}. In Sec. \ref{types}, we detail how tycons are defined and how types arise from tycons. We then describe how tycons control the semantics of their term-level operator constructors (opcons) in Sec. \ref{external-terms}.  We next give the key metatheoretic properties of the calculus in Sec. \ref{metatheory}, including \emph{type safety} and a key modularity result, which we call  \emph{conservativity}: any invariants that can be established about all values of a type under \emph{some} tycon context (i.e. in some  ``closed world'') are conserved in any further extended tycon context (i.e. in the ``open world'').  Interestingly, type system providers need not provide mechanized proofs to maintain such guarantees. Instead, the approach we take relies on type abstraction in the internal language. As a result, we are able to use the same parametricity results that underly modular reasoning in simply-typed languages like ML to reason modularly about typed language fragments. We conclude with  related and future work in Sec. \ref{prior-work}.

\section{Overview of @$\lambda$}\label{atlam}\label{overview}
\begin{figure}[t]
\small
\hspace{-5px}\input{syntax-table-IL}
\caption{Syntax of {$\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\iunit}\,{\times}\,{+}\}$}, our internal language (IL). Metavariable $x$ ranges over term variables and $\alpha$ and $t$ (distinguished only for stylistic reasons) over type variables.}
\label{syntax-IL}
\vspace{-10px}
\end{figure}
\paragraph{Internal Language} 
%Internal terms, $\iota$, together with internal types, $\tau$, form the \emph{typed internal language}. 
Our approach requires a typed internal language supporting type abstraction (i.e. universal quantification over types) \cite{Reynolds94anintroduction}. We use {$\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\iunit}\,{\times}\,{+}\}$}, the syntax for which is shown in Figure \ref{syntax-IL}, as representative of a typical intermediate language for a typed language. %Note that the IL has a fixed semantics; changes to the IL induce different dialects of @$\lambda$. 

We assume the statics of the IL are specified in the standard way by judgements for  type formation {$\iDelta \vdash \tau$}, typing context formation { $\iDelta \vdash \iGamma$} and type assignment {$\iDelta~\iGamma \vdash \iota : \tau\moutput$}. 
%The typing and type formation contexts obey standard structural properties (i.e. weakening, exchange and contraction). 
The internal dynamics are specified as a structural operational semantics with a stepping judgement {\small $\iota \mapsto \iota\moutput$} and a value judgement {$\iota~\mathtt{val}$}. Both the static and dynamic semantics of the IL can be found in any standard textbook covering typed lambda calculi (we directly follow \cite{pfpl}), so we assume familiarity and omit the details.

\paragraph{External Language}
Programmers interface with @$\lambda$ by writing \emph{external terms}, $e$. The abstract syntax of external terms is shown in Figure \ref{syntax-EL} and we will introduce various concrete desugarings in Sec. \ref{external-terms}. %We will describe useful syntactic desugarings atop this syntax as we go on and review recent techniques that permit modularly introducing  desugarings like these in Sec. \ref{desugaring}. Our main focus here is on semantic extensions. 
The semantics are specified as a \emph{bidirectionally typed translation semantics}, i.e. the key judgements have the form:\[\esynX{e}{\st\moutput}{\iota\moutput} ~~~~~\text{and}~~~~~ \eanaX{e}{\st}{\iota\moutput}\]
\noindent
These are pronounced ``$e$ (synthesizes / analyzes against) type $\sigma$ and has  translation $\iota$ under typing context $\Upsilon$ and tycon context $\Phi$''. Our specifications are intended to be algorithmic: we indicate ``outputs'' when introducing judgement forms by \emph{mode annotations}, $\moutput$. Note that the type is an ``output'' only for the synthetic judgement.%, i.e. it can only be derived when the term itself has enough information in it to determine its type.

We choose bidirectional typechecking, also sometimes called \emph{local type inference} \cite{Pierce:2000:LTI:345099.345100}, for two main reasons. The first is once again to justify the practicality of our approach: local type inference is increasingly being used in modern languages (e.g. Scala \cite{OdeZenZen01}) because it eliminates the need for type annotations in many situations while being simpler and decidable in more situations than whole-function type inference and providing what are widely perceived to be higher quality error messages \cite{journals/jfp/JonesVWS07}. Secondly, it will  give us a clean way to reuse the generalized intro form $\eintro{\st}{\es}$ and its desugarings at many types \cite{TSLs}. For example, regular string types can use standard string literal syntax and variants of record types can share   forms like $\{\conclbl{lbl1}=e_1, \conclbl{lbl2}=e_2\}$.

Elaboration semantics distinguishing the EL from the IL have previously been found suitable for full-scale language specifications, e.g. the Harper-Stone semantics for Standard ML \cite{Harper00atype-theoretic}. The IL is purposely kept small, e.g. defining only binary products, to simplify metatheoretic reasoning and compilation. The EL then specifies various useful higher-level constructs, e.g. record types, by translation to the IL. In @$\lambda$, the EL builds in only function types. All other external constructs will arise from the \emph{tycon context}, $\Phi$. 

This specification style is perhaps even more directly comparable to a specification for the initial stage of a type-directed compiler, e.g. the TIL compiler for Standard ML \cite{tarditi+:til-OLD}, here lifted ``one level up'' into the semantics of the language itself. The reason is that in the Harper-Stone elaboration semantics, external and internal terms were governed by a common type system, but in @$\lambda$ each type, $\st$, maps onto an internal type, $\tau$, called the \emph{type translation} of $\st$. This mapping is specified by the  type translation judgement, $\vdash_\Phi \st \leadsto \tau$, which will be described in Sec. \ref{sec:type-translations}. For example, regular string types will translate to internal string types, abbreviated $\keyw{str}$, and labeled product types will translate to nested binary product types, though we will emphasize that there are other valid choices, and that the choice of type translation should have only local impact. %This will be maintained using a form of translation validation, which also has historically been studied in the literature on compilers \cite{Pnueli-Siegel-Singerman98}.

External typing contexts, $\Upsilon$, map variables to types, so we also need the judgement $\vdash_\Phi \Upsilon \leadsto \Gamma$. 

%We will return to how we control the semantics of the single introductory form in the abstract syntax, $\eintro{\st}{\es}$, on the basis of the type it is being analyzed against, sidestepping issues related to the \emph{expression problem} as a result \todo{cite other paper}\cite{wadler1998expression}. The form $\eother{\iota}$ is a technical device used to quantify over all terms that may arise in a ``future'' tycon context, serving an analagous role to the  ``catchall'' base constants commonly found in minimal calculi, and we will return to its semantics later as well.%We will see how we leverage this bidirectionality when we discuss literal forms below. % The judgement is only well-defined for \emph{valid} tycon contexts, written judgementally as $\vdash \Phi$ and \emph{valid} typing contexts, $\vdash_\Phi \Upsilon$, which we also specify below.
%  \begin{figure*}[t!]
% \small
% \hspace{-5px}\input{example-table}
% %\\~\\
% %$\begin{array}{rcl}
% %e\cdot\conclbl{lbl}(\conclbl{lbl}_1=e_1, \cdots, \conclbl{lbl}_n=e_n)  :=  \etarg{(\conclbl{lbl}; \svar{list}n[\klbl]~\conclbl{lbl}_1~\cdots~\conclbl{lbl}_n)}{e}{e_1; \cdots; e_n}
% %\end{array}$
% \caption{An example external term, $e_{ex}$, in concrete syntax (left), desugared to abstract syntax (right), with static terms shown in green in examples (only). The notation ${\small \scolor{\desugar{...}}}$ stands for an embedding of the indicated ${\small \scolor{\conclbl{label}}}$, ${\small \scolor{\concrx{regular expression}}}$, ${\small \scolor{\concstr{string}}}$ or numeral into the SL, with derived kinds  $\klbl$, $\krx$,  $\kstr$ and $\knat$, not shown. The signatures of helper functions, e.g. ${\small \scolor{\svar{nil}}}$ and ${\small \scolor{\svar{list}n}}$, are shown in Figure \ref{helper-sigs}.}
% \label{example}
% \end{figure*}


\begin{figure}[t]
\small
\hspace{-5px}\input{syntax-table-EL}
\caption{Syntax of the external language (EL).}
\label{syntax-EL}\vspace{-5px}
\end{figure}



% We also here define a syntax for simultaneous substitutions, of terms for variables, $\gamma$, and of types for polymorphic type variables, $\delta$, and assume standard judgements ensuring that these are valid with respect to a valid context, $\Delta \vdash \gamma : \Gamma$ and $\vdash \delta : \Delta$. We apply these substitutions to terms, types and typing contexts using the syntax $[\gamma]\iota$, $[\delta]\iota$, $[\delta]\tau$ and $[\delta]\Gamma$ (in some prior work, application of a substitution like this is written using a hatted form, e.g. $\hat\gamma(\iota)$; we intend the same semantics but use a notation more consistent with standard substitution, e.g. $[\iota/x]\iota'$). We will omit  leading $\emptyset$ and $\cdot$ in examples; in specifications, the former is used for  metatheoretic finite mappings, the latter for metatheoretic ordered lists.

\paragraph{Static Language}
\begin{figure}[t]
\small
\hspace{-5px}\input{syntax-table-SL}
\caption{Syntax of the static language (SL). Metavariable $\sx$ ranges over static term variables, $\kalpha$ and $\kvar$ over kind variables and $n$ over natural numbers.}\vspace{-8px}
\label{syntax-SL}
\end{figure}
The main novelty of @$\lambda$ is the \emph{static language} (SL), which itself forms a typed lambda calculus where  
%External terms are classified by (external) \emph{types}. 
\emph{kinds}, $\kappa$, serve as the ``types'' of \emph{static terms}, $\sigma$.  The syntax of the SL is given in Figure \ref{syntax-SL}. The portion of the SL covered by the first row of kinds and static terms, some of which are elided for concision, forms an entirely standard total functional programming language consisting of total functions, polymorphic kinds, inductive kinds, and products and sums  \cite{pfpl}. The reader can consider these as forming a total subset of ML \cite{harper1997programming} or a simply-typed subset of Coq \todo{citation}and we will assume standard conveniences in examples (e.g. let bindings and inference of type parameters) for concision. Only three new  kinds are needed for the SL to serve its role as the language used to   control typing and translation: $\kty$, classifying types (Sec. \ref{types}), $\kity$, classifying \emph{quoted translational internal types} (used to compute type translations in Sec. \ref{sec:type-translations}) and $\kitm$, classifying \emph{quoted translational internal terms} (used to compute term translations in Sec. \ref{sec:introop}). The forms $\sana{n}{\st}$ and $\ssyn{n}$ will serve to link the dynamics of the SL with external type synthesis and analysis (Sec. \ref{external-terms}).

The kinding judgement takes the form $\sofkX{\st}{\kappa\moutput}$, where $\kDelta$ and $\kGamma$ are analagous to $\iDelta$ and $\iGamma$ and analagous kind and kinding context formation judgements $\kDelta \vdash \kappa$ and $\kDelta \vdash \kGamma$ are  defined. All such contexts in @$\lambda$ are identified up to exchange and contraction and obey weakening \cite{pfpl}. The natural number $n$ is used as a technical device in our semantics to ensure that the forms shown as being indexed by $n$ arise in a controlled manner to prevent ``out of bounds'' issues, as we will discuss; they would have no corresponding concrete syntax so $n$ can be assumed $0$ in user-defined terms. 

The dynamic semantics of static terms is defined as a structural operational semantics by a stepping judgement $\sstep{\st}{\argEnv}{\st\moutput}$, a value judgement $\sval{\st}{\argEnv}$ and an error raised judgement $\serr{\st}{\argEnv}$. $\argEnv$ ranges over \emph{argument environments}, which we will  return to when considering opcons in Sec. \ref{external-terms}. The multi-step judgement $\smanystep{\st}{\argEnv}{\st\moutput}$ is the reflexive, transitive closure of the stepping judgement. The normalization judgement $\seval{\st}{\argEnv}{\st'}$ is derivable iff $\smanystep{\st}{\argEnv}{\st'}$ and $\sval{\st'}{\argEnv}$. %${\st}{\memD}{\memG}{\aCtx}{\st\moutput}{\memD\moutput}{\memG\moutput}$, where $\memD$, $\memG$ and $\aCtx$ are also technical devices that will be described later; they too can be ignored from the perspective of user code. We write $\st \Downarrow \st'$ iff $\snorm{\st}{\emptyset}{\cdot}{{\rightharpoonup}; \emptyset; \emptyset}{\st'}{\emptyset}{\cdot}$. \emph{Static values} are normalized static terms. Normalization can also raise an error (to indicate a type error in an external term, or a problem in a tycon definition, as we will discuss), indicated by the judgement $\serrX{\st}$. We omit error propagation rules.%We will refer to the relevant rules as we proceed. 


\section{Types}\label{types}

\begin{figure*}\begin{mathpar}
\small
\inferrule[k-parr-ty]{
    \sofkX{\st}{\kprod{\kty}{\kty}}
}{
    \sofkX{\sty{\rightharpoonup}{\st}}{\kty}
}

\inferrule[k-ext-ty]{
    \tcdef{\tc}{\tcsig{\ktyidx}{\chi}}{\theta} \in \Phi\\
    \sofkX{\st}{\ktyidx}
}{
    \sofkX{\sty{\tc}{\st}}{\kty}
}

\inferrule[k-other-ty]{
    \sofkX{\sttyidx}{\kappa \times \kity}
}{
    \sofkX{\sty{\keyw{other}[m; \kappa]}{\sttyidx}}{\kty}
}\vspace{-10px}
\end{mathpar}
\caption{Kinding rules for types, which take the form $\sty{c}{\sttyidx}$ where $c$ is a tycon and $\sttyidx$ is the type index.}
\label{fig:types}\vspace{-10px}
\end{figure*}
\begin{figure}[t]
\small
$\begin{array}{lrcl}
\textbf{tycons} & c & ::= & \rightharpoonup ~|~ \tc ~|~ \keyw{other}[m; \kappa]\\
\textbf{tycon contexts} & \Phi & ::= & \cdot ~|~ \Phi, \tcdef{\tc}{\psi}{\theta}\\
\textbf{tycon structures} & \theta & ::= & \tcstruct{\st}{\omega} \\
\textbf{opcon structures} & \omega & ::= & \tcstructn{\st} ~|~ \tcstructc{\omega}{op}{\st}\\
\textbf{tycon sigs} & \psi & ::= & \tcsig{\kappa}{\chi}\\
\textbf{opcon sigs}& \chi & ::= & \introsig{\kappa} ~|~ \opsigS{\chi}{op}{\kappa}\\
\end{array}$
\caption{Syntax of tycons. Metavariables $\tc$ and $\opname{op}$ range over extension tycon and opcon names, respectively, and $m$ ranges over natural numbers.}
\label{syntax-TC}\vspace{-8px}
\end{figure}

\noindent
External types, or simply \emph{types}, are static values of kind $\kty$. We write $\istype{\st}{\Phi}$  iff $\sofkn{\emptyset}{\emptyset}{\Phi}{0}{\st}{\kty}$ and $\sval{\st}{\cdot;\emptyset;\Phi}$. The introductory form for kind $\kty$ is $\sty{c}{\st}$, where $c$ is a \emph{tycon} and $\st$ is the \emph{type index}. The dynamics are simple: the index is eagerly normalized and errors propagate  (the supplement gives the full set of rules for this paper). 

The syntax  for tycons given in Figure \ref{syntax-TC} specifies that $c$ can have one of three forms, so there are three corresponding kinding rules for types, shown in Figure \ref{fig:types}. 

\paragraph{Function Types} The rule \rulename{k-parr-ty} specifies that the type index of partial function types must be a pair of types. We thus say that $\rightharpoonup$ has \emph{index kind} $\kty \times \kty$. In practice, we would introduce a desugaring from $\st_1 \rightharpoonup \st_2$ to $\sty{\rightharpoonup}{(\st_1, \st_2)}$. %Here, we will benefit by treating it uniformly.


\paragraph{Extension Types} For types constructed by an \emph{extension tycon}, written in small caps, $\tc$, the rule \rulename{k-ext-ty} extracts the index kind of $\tc$ from the tycon context, $\Phi$.

For example,  $\tcvar{rstr}$ specifies the index kind $\krx$, which classifies static regular expression patterns (defined as an inductive sum kind in the usual way). Types constructed by $\tcvar{rstr}$ will classify terms that behave as \emph{regular strings}, i.e. strings statically known to be in the specified regular language \cite{sanitation-psp14}. For example, $\stx{title} := \sty{\tcvar{rstr}}{\concrx{.+}}$, which was shown in Sec. \ref{intro}, will classify non-empty strings and $\stx{conf} := \sty{\tcvar{rstr}}{\concrx{[A-Z]+ \digit\digit\digit\digit}}$ will classify strings having the format of a conference name.  The type indices are  here written using standard concrete syntax for concision; recent work has specified how to define type-specific (or here, kind-specific) syntax like this composably \cite{TSLs}. %We define the tycon context containing only the definition of $\tcvar{rstr}$, $\Phi_\text{rstr} := \tcdef{\tcvar{rstr}}{\tcsig{\krx}{\chi_\text{rstr}}}{\theta_\text{rstr}}$.

Our second example is $\tcvar{lprod}$, which defines a variant of labeled product type (labeled products are like record types, but maintain a row  ordering; records are definable in a manner discussed in Sec. \ref{sec:introop}, but an ordering simplifies our discussion). We choose the index kind of $\tcvar{lprod}$ to be $\klist{\klbl \times \kty}$, where list kinds are defined as inductive sums in the usual way, and $\klbl$ classifies static representations of row labels.  In a tycon context defining both tycon definitions, we can define a type classifying conference papers, $\stx{paper} := \sty{\tcvar{lprod}}{\{\conclbl{title} : \stx{title}, \conclbl{conf} : \stx{conf}\}}$. 
%Note that $\istype{\stx{paper}}{\Phi_\text{rstr}\Phi_\text{lprod}}$ and 
We again use kind-specific syntax, in this case for $\klist{\klbl \times \kty}$.

Two tycon contexts, each defining just one of the tycons just mentioned, are shown in Figure \ref{fig:example-tycons}.\footnote{In examples, we omit leading $\emptyset$, used as the base case for finite mappings, and $\cdot$, used as the base case for finite sequences, for concision.} We write the tycon context containing both  $\Phi_\text{rstr}\Phi_\text{lprod}$. 
The syntax of tycon contexts in Figure \ref{syntax-TC} specifies that they are simply lists of tycon definitions, $\tcdef{\tc}{\psi}{\theta}$. The tycon structure, $\theta$, contains a \emph{translation schema} (Sec. \ref{sec:type-translations}) and opcon definitions in an \emph{opcon structure}, $\omega$ (Sec. \ref{external-terms}). \emph{Tycon signatures}, $\psi$,  have the form $\tcsig{\ktyidx}{\chi}$, where $\ktyidx$ is the tycon's index kind and $\chi$ is the \emph{opcon signature}, defining index kinds for the opcons in the opcon structure. The rule (k-ext-ty) only needs the tycon index kind.

The tycon context well-definedness judgement, written $\vdash \Phi$, requires that all tycon names are unique and performs three additional checks, shown in Figure \ref{fig:tycon-context-well-definedness} and described below (we omit the trivial base case, (tcc-emp)).

\paragraph{Other Types}
The rule (k-otherty) governs types constructed by a tycon of the form $\keyw{other}[m; \kappa]$. These will serve only as technical devices to stand in for types other than those in a given tycon context in Sec. \ref{metatheory}. The index must pair a term of kind $\kappa$  with a static term of kind $\kity$, discussed in Sec. \ref{sec:type-translations}. These can be thought of as a technical realization of the informal practice of including a ``catch-all'' or token base type (e.g. unit or nat) in a calculus. %The natural number $m$ serves to create arbitrarily many of these tycons.






\subsection{Type Case Analysis}

\begin{figure}\small
\hspace{-10px}\begin{tabular}{ll}
$\begin{array}{l}\Phi_\text{rstr} := \tcdef{\tcvar{rstr}}{\psi_\text{rstr}}{\\
\quad\tcstruct{\stx{rstr/trans}}{\\
\quad\tcstructn{\stx{rstr/intro}}};\\
\quad\keyw{syn}~\opname{conc} = \stx{rstr/conc};\\
\quad\keyw{syn}~\opname{case} = \stx{rstr/case};\\
\quad\cdots}\end{array}$ & $\begin{array}{l}\Phi_\text{lprod} := \tcdef{\tcvar{lprod}}{\psi_\text{lprod}}{\\
\quad\tcstruct{\stx{lprod/trans}}{\\
\quad\tcstructn{\stx{lprod/intro}}};\\
\quad\keyw{syn}~\opname{\#} = \stx{lprod/prj};\\
\quad\keyw{syn}~\opname{conc} = \stx{lprod/conc};\\
\quad\cdots}\end{array}$
\end{tabular}\vspace{3px}\\
$\psi_\text{rstr} := \tcsig{\krx}{\introsig{\keyw{Str}}; \opname{conc}[\kunit]; \opname{case}[\keyw{StrPattern}]; ...}$\\
$\psi_\text{lprod} := \tcsig{\klist{\klbl\times\kty}}{\introsig{\klist{\klbl}}; \opname{\#}[\klbl]; \opname{conc}[\kunit]; ...}$
\caption{Example tycon contexts and signatures.}
\label{fig:example-tycons}\vspace{-8px}
\end{figure}
\noindent
Types here can be thought of as arising from a distinguished ``open datatype'' \cite{conf/ppdp/LohH06} defined by the tycon context. Consistent with this view, a type $\st$ can be case analyzed using $\stycase{c}{\st}{\sx}{\st_1}{\st_2}$. If the value of $\st$ is constructed by $c$, its type index is bound to $\sx$ and the branch $\st_1$ is taken. For totality, a default branch, $\st_2$, must also be provided.  For example, the kinding rule for extension tycons is: 
\begin{mathpar}
\small
\inferrule[k-tccase]{
    \sofkX{\st}{\kty}\\
    \tcdef{\tc}{\tcsig{\ktyidx}{\chi}}{\theta} \in \Phi\\
    \sofk{\kDelta}{\kGamma, \sx :: \ktyidx}{\Phi}{\st_1}{\kappa}\\
    \sofkX{\st_2}{\kappa}
}{
    \sofkX{\stycase{\tc}{\st}{\sx}{\st_1}{\st_2}}{\kappa}
}
\end{mathpar}
% \begin{mathpar}\small
% \inferrule[s-tycase-fail-2]{ }{
%     \sstep{\stycase{c}{\sotherty{m}{\tau}}{x}{\st_1}{\st_2}}{\argEnv}{\st_2}
% }
% \end{mathpar}
The rule for $c{=}{\rightharpoonup}$ is analagous, but, importantly, no rule for case analyzing against $c{=}\keyw{other}[m; \kappa]$ is defined (these types thus always take the default branch). %The dynamics (see supplement) are straightforwardly consistent with these intuitions.
We will see an example of its use in an opcon definition in Sec. \ref{sec:targops}.

\begin{figure}[t]
\vspace{-10px}
\begin{mathpar}
\small\inferrule[tcc-ext]{
    \vdash \Phi\\
    \tc \notin \text{dom}(\Phi)\\
    \keq{\emptyset}{\ktyidx}\\
    \sofkz{\emptyset}{\emptyset}{\Phi}{\stx{schema}}{\ktyidx \rightarrow \kity}\\\\
    \vdash_{\Phi,  \tcdef{\tc}{\tcsig{\ktyidx}{\chi}}{\tcstruct{\stx{schema}}{\omega}}} \omega \sim \tcsig{\ktyidx}{\chi}
}{
    \vdash \Phi, \tcdef{\tc}{\tcsig{\ktyidx}{\chi}}{\tcstruct{\stx{schema}}{\omega}}
}\vspace{-10px}
\end{mathpar}
\caption{Tycon context well-definedness.}
\label{fig:tycon-context-well-definedness}
\vspace{-8px}
\end{figure}
\noindent
% \begin{figure}\hfill \fbox{$\vdash \Phi$}\vspace{-25px}
% \caption{Tycon context well-definedness. We omit the trivial case for when $\Phi=\cdot$ for concision.}
% \label{fig:tycon-ctxs}
% \vspace{-8px}
% \end{figure}

\subsection{Type Equivalence}\label{sec:type-equivalence}
\noindent The first check in (tcc-ext) simplifies type equivalence: type index kinds must be \emph{equality kinds}, i.e. those for which semantically equivalent values are syntactically equal. We define these by the judgement $\keq{\kDelta}{\kappa}$ (see supplement).  Arrow kinds are not equality kinds, so type indices cannot contain static functions. 
Equality kinds are exactly analagous to equality types as found in Standard ML \cite{Tofte:89:TheDefinitionOfStandardML}.


\subsection{Type Translations}\label{sec:type-translations}
\noindent 
Recall that every type $\st$ has a type translation, $\tau$. Extension tycons {compute} translations for the types they construct as a function of each type's index by specifying a \emph{translation schema} in the {tycon structure}, $\theta$. A tycon with index kind $\ktyidx$ must define a translation schema of kind $\ktyidx \rightarrow \kity$, checked by (tcc-ext). 

Terms of kind $\kity$ are introduced by a quotation form, $\sqity{\qity}$, where $\qity$ is a \emph{translational internal type}. Each form of internal type, $\tau$,  corresponds to a form of translational internal type, $\qity$. For example,  $\tcvar{rstr}$ ignores the type index and translate regular strings to strings, of internal type abbreviated $\keyw{str}$. Abbreviating the corresponding translational internal type $\hat{\keyw{str}}$, we define $\stx{rstr/trans} := \slam{\krx}{\svar{tyidx}}{\sqity{\hat{\keyw{str}}}}$. The kinding and dynamics for these shared forms is simply recursive (maintaining the quotation marker), e.g.
\begin{mathpar}
\small
\inferrule[k-ity-prod]{
    \sofkX{\sqity{\qity_1}}{\kity}\\
    \sofkX{\sqity{\qity_2}}{\kity}
}{
    \sofkX{\sqity{\qity_1 \times \qity_2}}{\kity}
}
\end{mathpar}
%The operational semantics for shared forms are also straightforwardly recursive (see supplemental material).

The syntax for $\qity$ further includes an ``unquote'' form,  $\qtuq{\st}$, so that they can be constructed compositionally, as well as a form, $\srep{\st}$, that allows one type's translation to refer to another's: \begin{mathpar}\small
\inferrule[k-ity-unquote]{
    \sofkX{\st}{\kity}   
}{
    \sofkX{\sqity{\qtuq{\st}}}{\kity}
}

\inferrule[k-ity-trans]{
    \sofkX{\st}{\kty}
}{
    \sofkX{\sqity{\srep{\st}}}{\kity}
}
\end{mathpar}
The unquote form is eliminated during normalization, while references to type translations are retained in values:
\begin{mathpar}
\small
\inferrule[s-ity-unquote-elim]{
    \sval{\sqity{\qity}}{\argEnv}
}{
    \sstep{\sqity{\qtuq{\sqity{\qity}}}}{\argEnv}{\sqity{\qity}}
}

\inferrule[s-ity-trans-val]{
    \sval{\st}{\argEnv}
}{
    \sval{\sqity{\srep{\st}}}{\argEnv}
}
\end{mathpar}
These forms are needed in the translation schema for $\tcvar{lprod}$, which generates nested binary product types (though we could also have used, e.g., an internal list) by folding over the type index and referring to the translations of the types therein. $\stx{lprod/trans}:=$
\[\small
\begin{array}{l}
\slam{\klist{\klbl \times \kty}}{\svar{tyidx}}{\svar{fold}~\svar{tyidx}~\sqity{\iunit}~\\
  \quad (\lambda \svar{h}{:}\klbl \times \kty.\lambda \svar{r}{:}\kity.\sqity{\srep{{\ssnd{\svar{h}}}}\times\qtuq{\svar{r}}}}
\end{array}\]
%We assume a standard $\small\svar{fold} :: \kforall{\kalpha_1}{\kforall{\kalpha_2}{\klist{\kalpha_1}\rightarrow\kalpha_2\rightarrow(\kalpha_1\rightarrow\kalpha_2\rightarrow\kalpha_2)\rightarrow\kalpha_2}}$ in defining this translation schema.

Applying this translation schema to the index of $\stx{paper}$, for example, produces  $\stx{paper/trans} := \sqity{\qity_\text{paper/trans}}$ where $\small\qity_\text{paper/trans} := \srep{\stx{title}}\times(\srep{\stx{conf}}\times \iunit)$. Note that our schema did not remove the trailing unit type for simplicity (and to again emphasize that this choice is local). It may be helpful to derive that $\stx{paper/trans} :: \kity$ by the rules above.%For simplicity, and to make the point that the choice of representation has only tycon-local implications, our translation schema does not optimize away  

\subsubsection{Selective Type Translation Abstraction}\label{sec:selective-type-translation-abstraction}
\noindent
References to type translations are maintained in values of kind $\kity$ like this to  allow us to selectively hold them abstract, which will be the key to translation independence. This can be thought of as analagous to the process in ML by which the true identity of an abstract type in a module is held abstract outside the module until after typechecking. The judgement $\small\tdeabs{\Phi}{c}{\qity}{\memD}{\ity\moutput}{\memD\moutput}$ relates a normalized translational internal type $\qity$ to an internal type $\tau$, called a \emph{selectively abstracted type translation} because references to translations of types constructed by a tycon other than the \emph{delegated tycon}, $c$, are replaced by a corresponding type variable, $\alpha$. The \emph{type translation store} $\memD ::= \emptyset ~|~ \memD, \st \leftrightarrow \ity/\alpha$ maintains this correspondence between types, their actual translations and the distinct type variables which appear in their place, e.g. $\small\tdeabs{\Phi_\text{rstr}\Phi_\text{lprod}}{\tcvar{lprod}}{\qity_\text{paper/trans}}{\emptyset}{\tau_\text{paper/abs}}{\memD_\text{paper/abs}}$
where $\small\tau_\text{paper/abs} := \alpha_1 \times (\alpha_2 \times \iunit)$ and 
$\memD_\text{paper/abs}  := \stx{title} \leftrightarrow \keyw{str}/\alpha_1, \stx{conf} \leftrightarrow \keyw{str}/\alpha_2$. 

%This process of selectively holding the translations of all types constructed by any tycon other than the ``delegated tycon'' abstract will be critical for conservativity to hold below. Here, the translation of the type is being held abstract, rather than the type itself, and type translations are computed from indices, rather than declared ``literally''. 


The judgement $\memD \leadsto \delta\moutput : \Delta\moutput$ constructs the $n$-ary \emph{type substitution}, $\delta ::= \emptyset ~|~ \delta, \tau/\alpha$, and corresponding internal type formation context, $\Delta$, implied by the type translation store $\memD$. For example, $\memD_\text{paper/abs} \leadsto \delta_\text{paper/abs} : \Delta_\text{paper/abs}$ where $\delta_\text{paper/abs} := \keyw{str}/\alpha_1, \keyw{str}/\alpha_2$ and $\Delta_\text{paper/abs} := \alpha_1, \alpha_2$. 
We can apply type substitutions to internal types, terms and typing contexts, written $[\delta]\ity$, $[\delta]\iota$ and $[\delta]\Gamma$, respectively. For example, $[\delta_\text{paper/abs}]\tau_\text{paper/abs}$ is $\tau_\text{paper} := \keyw{str} \times (\keyw{str} \times \iunit)$, i.e. the actual type translation of $\stx{paper}$. Indeed, we can now give the rule defining the type translation judgement, $\vdash_\Phi \st \leadsto \tau$, mentioned in Sec. \ref{overview}. We simply determine any selectively abstract translation, then apply the induced substitution:
\begin{mathpar}\small
\inferrule[ty-trans]{
    \istype{\st}{\Phi}\\
    \tdeabs{\Phi}{c}{\srep{\st}}{\emptyset}{\tau}{\memD}\\
    \memD \leadsto \delta : \Delta
}{
    \vdash_\Phi \st \leadsto [\delta]\tau
}
\end{mathpar}


The rules for the selective type translation abstraction judgement recurse generically over shared forms in $\qity$. Only sub-terms of form $\srep{\st}$ are interesting. 
The translation of an extension type  is determined by calling the translation schema and checking that the type translation it generates is closed except for type variables tracked by $\memD'$. If constructed by the delegated tycon, it is not held abstract:
\begin{mathpar}
\small
\inferrule[abs-tc-delegated]{
    \tcdef{\tc}{\psi}{\tcstruct{\stx{schema}}{\omega}} \in \Phi\\
    \seval{\stx{schema}(\sttyidx)}{\cdot;\emptyset;\Phi}{\sqity{\qity}}\\\\
    \tdeabs{\Phi}{\tc}{\qity}{\memD}{\ity}{\memD'}\\
    \memD' \leadsto \delta : \Delta\\
    \Delta \vdash \tau
}{
    \tdeabs{\Phi}{\tc}{\srep{\sty{\tc}{\sttyidx}}}{\memD}{\tau}{\memD'}
}
\end{mathpar}
Otherwise, it is held abstract via a fresh type variable added to the store (the supplement has the rule (abs-ty-stored) for retrieving it if already there):
\begin{mathpar}
\small
\inferrule[abs-tc-not-delegated-new]{
    c \neq \tc\\
    \sty{\tc}{\sttyidx} \notin \text{dom}(\memD)\\
    \tcdef{\tc}{\psi}{\tcstruct{\stx{schema}}{\omega}} \in \Phi\\\\
    \seval{\stx{schema}(\sttyidx)}{\cdot;\emptyset;\Phi}{\sqity{\qity}}\\
    \tdeabs{\Phi}{\tc}{\qity}{\memD}{\ity}{\memD'}\\\\
    \memD' \leadsto \delta : \Delta\\
    \Delta \vdash \tau\\
    (\alpha~\text{fresh})
}{
    \tdeabs{\Phi}{c}{\srep{\sty{\tc}{\sttyidx}}}{\memD}{\alpha}{\memD', \sty{\tc}{\sttyidx} \leftrightarrow [\delta]\tau/\alpha}
}
\end{mathpar}
The translation of an ``other'' type is given in its index:
\begin{mathpar}
\small
\inferrule[abs-other-delegated]{
    \tdeabs{\Phi}{\keyw{other}[m;\kappa]}{\qity}{\memD}{\tau}{\memD'}
}{
    \tdeabs{\Phi}{\keyw{other}[m;\kappa]}{\srep{\sty{\keyw{other}[m;\kappa]}{(\st, \sqity{\qity}}}}{\memD}{\tau}{\memD'}
}
\end{mathpar}
Rule (abs-other-not-delegated-new) is in the supplement.

The translations of function types are not held abstract, so that lambdas can be the sole binding construct in the EL:
\begin{mathpar}
\small
\inferrule[abs-parr]{
    \tdeabs{\Phi}{c}{\srep{\st_1}}{\memD}{\ity_1}{\memD'}\\
    \tdeabs{\Phi}{c}{\srep{\st_2}}{\memD'}{\ity_2}{\memD''}
}{
    \tdeabs{\Phi}{c}{\srep{\sty{\rightharpoonup}{(\st_1, \st_2)}}}{\memD}{\tau_1 \rightharpoonup \tau_2}{\memD''}
}
\end{mathpar}


% \paragraph{Summary}
% In summary, types (e.g. $\stx{paper}$) are constructed by applying tycons to type indices. Extension tycons compute quoted type translations ($\stx{paper/trans}$), which then can determine many selectively abstracted type translations depending on the delegated tycon (e.g. $\tau_\text{paper/abs}$). From any such, the actual type translation is determined ($\tau_\text{paper}$).


%\vspace{-5px}
 %We have that external typing contexts obey the standard structural congruences: weakening, exchange and contraction.
\section{External Terms}\label{external-terms}
\noindent 
Now that we have established how types are constructed and how selectively abstracted and from there actual type translations are computed by static functions, we are ready to give the semantics for external terms, shown in Figure \ref{typing}.

Because we are defining a bidirectional type system, a subsumption rule is needed to allow synthetic terms to be analyzed against an equivalent type. Per Sec. \ref{sec:type-equivalence}, equivalent types must be  syntactically identical at normal form, and we consider analysis only if $\istype{\st}{\Phi}$, so the rule (subsume) is straightforward. To use an analytic term in a synthetic position, the programmer must provide a type ascription, written $e : \st$. The ascription is kind checked and normalized to a type before being used for analysis by rule (ascribe).

Variables and functions behave in the standard manner given our definitions of types and type translations (used to generate ascriptions in the IL). We use Plotkin's fixpoint operator for general recursion (cf. \cite{pfpl}), and define both lambdas and fixpoints only analytically for simplicity.





\subsection{Generalized Introductory Operations}\label{sec:introop}
The translation of the \emph{generalized intro operation}, written $\small\eintro{\sttmidx}{\es}$, is determined by the tycon of the type it is being analyzed against as a function of the type's index, the \emph{term index}, $\sttmidx$, and the \emph{argument list}, $\es$.

Before discussing rules (ana-intro) and (ana-intro-other), we note that we can recover a variety of standard concrete introductory forms by a purely syntactic desugaring to this abstract form (and thus allow their use at more than one type). For example, for regular strings we can use the string literal form, $\texttt{"s"}$, which desugars to $\eintro{\texttt{"s"}_\text{SL}}{\cdot}$, i.e. the term index is the corresponding static value of kind $\keyw{Str}$, indicated by a subscript for clarity. Similarly, for labeled products, records, objects and so on, we can define a generalized labeled collection form, $\{\mathtt{lbl}_1=e_1, \ldots, \mathtt{lbl}_n=e_n\}$, that desugars to $\eintro{\texttt{[}\mathtt{lbl}_1, \ldots, \mathtt{lbl}_n\texttt{]}}{e_1; \ldots; e_n}$, i.e. a list constructed from the row labels is the term index and the corresponding row values are the arguments. In both cases, the term index captures  static portions of the concrete form and the arguments capture all external sub-terms. Additional desugarings are shown in the supplement and a technique based on \cite{TSLs} could be introduced to allow tycon providers to define more such desugarings  composably. 



\begin{figure*}[t]
\small\fbox{$\eanaX{e}{\st}{\iota}$}~
\fbox{$\esynX{e}{\st}{\iota}$}\vspace{-3px}
\begin{mathpar}
\inferrule[subsume]{
    \esynX{e}{\st}{\iota}
}{
    \eanaX{e}{\st}{\iota}
}

\inferrule[ascribe]{
  \sofkz{\emptyset}{\emptyset}{\Phi}{\st}{\kty}\\
  \st \Downarrow_{\cdot; \emptyset; \Phi} \st'\\\\
  \eanaX{e}{\st'}{\iota}
}{
  \esynX{\easc{e}{\st}}{\st'}{\iota}
}

\small\inferrule[syn-var]{
  x \Rightarrow \st \in \Upsilon
}{
  \esynX{x}{\st}{x}
}

\inferrule[ana-fix]{
  \eana{\Upsilon, x \Rightarrow \st}{\Phi}{e}{\st}{\iota}\\\\
  \sfinalrepX{\st}{\tau}
}{
  \eanaX{\efix{x}{e}}{\st}{\ifix{\tau}{x}{\iota}}
}

\inferrule[ana-lam]{
    \eana{\Upsilon, x \Rightarrow \st_1}{\Phi}{e}{\st_2}{\iota}\\
    \sfinalrepX{\st_1}{\tau_1}
}{
    \eanaX{\eanalam{x}{e}}{\sty{\rightharpoonup}{(\st_1, \st_2)}}{\ilam{\tau_1}{x}{\iota}}
}

\inferrule[syn-ap]{
  \esynX{e_1}{\sty{\rightharpoonup}{(\st_1, \st_2)}}{\iota_1}\\
  \eanaX{e_2}{\st_2}{\iota_2}
}{
  \esynX{\eap{e_1}{e_2}}{\st_2}{\iap{\iota_1}{\iota_2}}
}

\inferrule[ana-intro]{
  \tcdef{\tc}{\tcsig{\_}{\chi}}{\tcstruct{\_}{\omega}} \in \Phi\\\\
  \introsig{\klitidx} \in \chi\\
  \sofkz{\emptyset}{\emptyset}{\Phi}{\stmidx}{\klitidx}\\\\
  \keyw{ana~intro}={\stx{def}} \in \omega\\
  |\es| = n\\
    \keyw{args}(n)=\stx{args}\\\\
  \stx{def}(\sttyidx)(\stmidx)(\stx{args}) \Downarrow_{\es;\Upsilon;\Phi} \sqitm{\qitm}\\\\
  \trvalidate{\es;\Upsilon;\Phi}{\tc}{\qitm}{\srep{\sty{\tc}{\sttyidx}}}{\iota}
  %\validate{1}{2{3}{4}{5}
  %\snorm{\stx{def}~\sttyidx~\stmidx~\stx{args}}{\emptyset}{\memG_0}{{\rightharpoonup}, \tc; \Upsilon; \Phi}{\sqitm{\itm_\text{abs}}}{\memD}{\memG}\\\\
  %\snorm{\srep{\sty{\tc}{\sttyidx}}}{\memD}{\cdot}{{\rightharpoonup}, \tc;\emptyset;\Phi}{\sqity{\ity_\text{abs}}}{\memD'}{\cdot}\\\\
}{
  \eanaX{\eintro{\stmidx}{\es}}{\sty{\tc}{\sttyidx}}{\iota}
}

\inferrule[syn-targ]{
  \esynX{e_\text{targ}}{\sty{\tc}{\sttyidx}}{\iota_\text{targ}}\\\\
  \tcdef{\tc}{\tcsig{\_}{\chi}}{\tcstruct{\_}{\omega}} \in \Phi\\\\
  \opsig{\opname{op}}{\klitidx} \in \chi\\
  \sofkz{\emptyset}{\emptyset}{\Phi}{\stmidx}{\klitidx}\\\\
  \keyw{syn~}\opname{op}={\stx{def}} \in \omega\\
  |e_\text{targ}; \es| = n\\
    \keyw{args}(n)=\stx{args}\\\\
  \stx{def}(\sttyidx)(\stmidx)(\stx{args}) \Downarrow_{(e_\text{targ}; \es);\Upsilon;\Phi} (\st, \sqitm{\qitm})\\\\
  \trvalidate{(e_\text{targ}; \es);\Upsilon;\Phi}{\tc}{\qitm}{\srep{\st}}{\iota}
  %\snorm{\stx{def}~\sttyidx~\stmidx~\stx{args}}{\emptyset}{\memG_0}{{\rightharpoonup}, \tc; \Upsilon; \Phi}{\sqitm{\itm_\text{abs}}}{\memD}{\memG}\\\\
  %\snorm{\srep{\sty{\tc}{\sttyidx}}}{\memD}{\cdot}{{\rightharpoonup}, \tc;\emptyset;\Phi}{\sqity{\ity_\text{abs}}}{\memD'}{\cdot}\\\\
}{
    \esynX{\etarg{\opname{op}}{\sttmidx}{e_\text{targ}}{\es}}{\st}{\iota}
}

\inferrule[ana-intro-other]{
    \sofkz{\emptyset}{\emptyset}{\Phi}{\stx{def}}{\kargs \rightarrow \kitm}\\\\
    |\es| = n\\
    \keyw{args}(n)=\stx{args}\\
    \stx{def}(\stx{args}) \Downarrow_{\es; \Upsilon; \Phi} \sqitm{\qitm}\\\\
    \trvalidate{\es;\Upsilon;\Phi}{\keyw{other}[m;\kappa]}{\qitm}{\srep{\sty{\keyw{other}[m;\kappa]}{\sttyidx}}}{\iota}
}{
    \eanaX{\eintro{\stx{def}}{\es}}{\sty{\keyw{other}[m;\kappa]}{\sttyidx}}{\iota}
}
~~~~~~
\inferrule[syn-targ-other]{
\esynX{e_\text{targ}}{\sty{\keyw{other}[m;\kappa]}{\sttyidx}}{\iota_\text{targ}}\\\\
  \sofkz{\emptyset}{\emptyset}{\Phi}{\stx{def}}{\kargs\rightarrow(\kty\times\kitm)}\\\\
  |e_\text{targ}; \es| = n\\
    \keyw{args}(n)=\stx{args}\\
  \stx{def}(\stx{args}) \Downarrow_{(e_\text{targ}; \es);\Upsilon;\Phi} (\st, \sqitm{\qitm})\\\\
  \trvalidate{(e_\text{targ}; \es);\Upsilon;\Phi}{\keyw{other}[m; \kappa]}{\qitm}{\srep{\st}}{\iota}
  %\snorm{\stx{def}~\sttyidx~\stmidx~\stx{args}}{\emptyset}{\memG_0}{{\rightharpoonup}, \tc; \Upsilon; \Phi}{\sqitm{\itm_\text{abs}}}{\memD}{\memG}\\\\
  %\snorm{\srep{\sty{\tc}{\sttyidx}}}{\memD}{\cdot}{{\rightharpoonup}, \tc;\emptyset;\Phi}{\sqity{\ity_\text{abs}}}{\memD'}{\cdot}\\\\
}{
     \esynX{\etarg{\opname{op}}{\stx{def}}{e_\text{targ}}{\es}}{\st}{[\delta][\gamma]\iota_\text{abs}}
}\vspace{-5px}
\end{mathpar}
\caption{Typing}
\label{typing}
\end{figure*}
\begin{figure*}[t]
\small\fbox{$\vdash_\Phi \omega \sim \psi$}
\vspace{-25px}
\begin{mathpar}\small
\hspace{50px}\inferrule[ocstruct-intro]{
    \introsig{\klitidx} \in \chi\\
    \emptyset \vdash \klitidx\\\\
    \sofkz{\emptyset}{\emptyset}{\Phi}{\stx{def}}{\ktyidx \rightarrow \klitidx \rightarrow \kargs \rightarrow \kitm}
}{
    \vdash_\Phi \tcstructn{\stx{def}} \sim \tcsig{\ktyidx}{\chi}
}

\inferrule[ocstruct-targ]{
    \vdash_\Phi \omega \sim \tcsig{\ktyidx}{\chi}\\
    \opname{op}\notin\text{dom}(\chi)\\
    \emptyset \vdash \klitidx\\\\
    \sofkn{\emptyset}{\emptyset}{\Phi}{0}{\stx{def}}{\ktyidx \rightarrow \klitidx \rightarrow \kargs \rightarrow (\kty \times \kitm)}
}{
    \vdash_\Phi \tcstructc{\omega}{op}{\stx{def}} \sim \tcsig{\ktyidx}{\chi, \opsig{\opname{op}}{\klitidx}}
}\vspace{-5px}
\end{mathpar}
\caption{Opcon structure kinding against tycon signatures}
\label{ocstruct}\vspace{-5px}
\end{figure*}


Let us now derive $\small\eana{\Upsilon_\text{ex}}{\Phi_\text{rstr}\Phi_\text{lprod}}{e_\text{ex}}{\stx{paper}}{\iota_\text{ex}}$ where $\small\Upsilon_\text{ex}  := title \Rightarrow \stx{title}$ and $\small e_\text{ex} := \{\conclbl{title}=title, \conclbl{conf}=\texttt{"EXMPL 2015"}\}$. The  translation will be $\small\iota_\text{ex} := (title, (\texttt{"EXMPL 2015"}_\text{IL}, ()))$, where $\small\texttt{"EXMPL 2015"}_\text{IL}$ is an internal string (of internal type $\keyw{str}$). 

The first premise of (ana-intro) extracts the tycon definition for the tycon of the type the intro form is being analyzed against. In this example, this is $\tcvar{lprod}$. We will use this as the {delegated tycon} in the final premises of the rule.

The second premise extracts the \emph{intro term index kind}, $\klitidx$, from the \emph{opcon signature}, $\chi$, and the third premise checks the provided term index against this kind. This is simply the kind of term index expected by the tycon, e.g. in Figure \ref{fig:example-tycons}, $\tcvar{lprod}$ specifies $\klist{\klbl}$, so that it can use the labeled collection form, while $\tcvar{rstr}$ specifies an intro index kind of $\keyw{Str}$, so that it can use the string literal form. %\[\small\begin{array}{lcl}
%\chi_\text{lprod} & := & \introsig{\klist{\klbl}}, \chi_\text{lprod/targops}\\
%\chi_\text{rstr} & := & \introsig{\keyw{Str}}, \chi_\text{rstr/targops}
%\end{array}
%\]


The fourth premise extracts the \emph{intro opcon definition} from the \emph{opcon structure}, $\omega$, of the tycon structure, calling it $\stx{def}$. This is a static function that is applied, in the seventh premise, to determine whether the term is well-typed, raising an error if not or determining the translation of the term if so. The function has access to the type index, the term index and an interface to the list of arguments, and its kind is checked by the  judgement $\vdash_\Phi \omega \sim \psi$, which appeared as the final premise of the rule (tcc-ext) and is defined in Figure \ref{ocstruct}. For example, the intro opcon definition for $\tcvar{rstr}$ is $\small\stx{rstr/intro}:=$
\[\small\begin{array}{l}
    \slam{\krx}{\svar{tyidx}}{\slam{\kstr}{\svar{tmidx}}{\slam{\kargs}{\svar{args}}{\\
\quad \keyw{let}~\svar{aok} :: \kunit = \svar{arity0}~\svar{args}~\keyw{in}~\\
\quad \keyw{let}~\svar{rok} :: \kunit = \svar{rmatch}~\svar{tyidx}~\svar{tmidx}~\keyw{in}~\svar{str\_of\_Str}~\svar{tmidx}
}}}
\end{array}\]
Because regular strings are implemented as strings, this intro opcon definition is straightforward. It begins by making sure that no arguments were passed in (we will return to arguments and the fifth and sixth premises of (ana-intro) with the next example), using the helper function $\svar{arity0} :: \kargs \rightarrow \kunit$ defined such that any non-empty list will raise an error, via the static term $\sraise{\kunit}$. In practice, the tycon provider would specify an error message here. 
Next, it checks the string provided as the term index against the regular expression given as the type index using $\svar{rmatch} :: \krx \rightarrow \keyw{Str} \rightarrow \kunit$, which we assume is defined in the usual way and again raises an error on failure. Finally, the \emph{translational internal string} corresponding to the static string provided as the term index is generated via the helper function $\svar{str\_of\_Str} :: \keyw{Str} \rightarrow \kitm$.% and $\svar{nat\_of\_Nat} :: \keyw{Nat} \rightarrow \kitm$ to generate  translational internal terms corresponding to the provided static terms and $\svar{len} :: \keyw{Str} \rightarrow \keyw{Nat}$ to statically compute the length of the string. 

The introductory form for kind $\kitm$ is also a quotation form, $\sqitm{\qitm}$, where $\qitm$ is a \emph{translational internal term}. It is analagous to the form $\sqity{\qity}$ for $\kity$ in Sec. \ref{sec:type-translations}. Each form in the syntax of $\iota$ has a corresponding form in the syntax for $\qitm$ and the kinding rules and dynamics simply recurse through these in the same manner as in Sec. \ref{sec:type-translations}. There is also an analagous unquote form, $\quq{\st}$. %The supplement gives the analagous rules.
The final two forms of translational internal term are $\anatrans{n}{\st}$ and $\syntrans{n}$. These stand in for the translation of argument $n$, the first if it arises via analysis against type $\st$ and the second if it arises via type synthesis.  Before giving the rules, let us motivate the mechanism with the intro opcon definition for $\tcvar{lprod}$, shown in Figure \ref{fig:lprod-intro}.

\begin{figure}\vspace{-10px}
\[\small
\begin{array}{l}
\lambda\svar{tyidx}{:}\klist{\klbl\times\kty}.\lambda\svar{tmidx}{:}\klist{\klbl}.\lambda\svar{args}{:}\kargs.\\
\quad \keyw{let}~\svar{inhabited}:\kunit=\svar{uniqmap}~\svar{tyidx}~\keyw{in}\\
\quad \svar{fold3}~\svar{tyidx}~\svar{tmidx}~\svar{args}~\sqitm{\itriv}\\
\quad\quad \lambda \svar{rowtyidx}{:}\klbl\times\kty.\lambda \svar{rowtmidx}{:}\klbl.\lambda\svar{rowarg}{:}\karg.\lambda \svar{r}{:}\kitm.\\
\quad\quad\quad \keyw{letpair}~(\svar{rowlbl}, \svar{rowty})=\svar{rowtyidx} ~\keyw{in}\\
\quad\quad\quad \keyw{let}~\svar{lok}::\kunit=\svar{lbleq}~\svar{rowlbl} ~\svar{rowtmidx}~\keyw{in}\\
\quad\quad\quad \keyw{let}~\svar{rowtr} :: \kitm = \svar{ana}~\svar{rowarg}~\svar{rowty}~\keyw{in}\\
\quad\quad\quad \sqitm{(\quq{\svar{rowtr}}, \quq{\svar{r}})
}\end{array}\]\vspace{-10px}
\caption{The intro opcon definition for $\tcvar{lprod}$.}
\label{fig:lprod-intro}\vspace{-8px}
\end{figure}

The first line checks that the type provided is inhabited, in this case by checking that there are no duplicate labels via the helper function $\svar{uniqmap} :: \klist{\klbl \times \kty}\rightarrow \kunit$, raising an error if there are. An alternative strategy may have been to use an abstract kind that ensured that such type indices could not have been constructed, but to be compatible with our equality kind restriction, this would require support for abstract equality kinds, analagous to abstract equality types in SML. We chose not to formalize these for simplicity, and to demonstrate this general technique. An analagous technique could be used to implement record types by requiring that the index be sorted (see supplement).%, leaving indices where the labels did not appear sorted uninhabited. In both cases, we could provide a static helper function of kind $\klist{\klbl\times\kty}\rightarrow \kty$ that checked well-formedness immediately before constructing the requested type.

The rest of this opcon definition folds over the three lists provided as input: the list mapping labels to types provided as the type index, the list of labels provided as the term index, and the list of argument interfaces. We assume a straightforward helper function, $\svar{fold3}$, that raises an error if the three lists are not of the same length. The base case is the translational empty product. The recursive case checks, for each row, that the label provided in the term index matches the label provided in the type index, using a helper function $\svar{lbleq} :: \klbl \rightarrow \klbl \rightarrow \kunit$. Then, we request type analysis of the corresponding argument, $\svar{rowarg}$, against the type in the type index, $\svar{rowty}$, by writing $\svar{ana}~\svar{rowarg}~\svar{rowty}$. Here, $\svar{ana}$ is a helper function defined below that triggers type analysis of the provided argument. If this succeeds, it evaluates to an translational internal term  of the form $\sqitm{\anatrans{n}{\st}}$, where $n$ is the position of $\svar{rowarg}$ in $\svar{args}$ and $\st$ is the value of $\svar{rowty}$. If analysis fails, it raises an error. The final line constructs a nested tuple based on this translation and the recursive result. Taken together, the translational internal term that will be generated for our example involving $e_\text{ex}$ above is $\small\qitm_\text{ex} := (\anatrans{0}{\stx{title}}, (\anatrans{1}{\stx{conf}}, ()))$, i.e. it simply recalls that the two arguments were analyzed against $\stx{title}$ and  $\stx{conf}$, without yet inserting their translations directly. 
This is done by the \emph{translation validation judgement}, which occurs in the final premise. We describe argument interfaces and translation validation next.

\subsection{Argument Interfaces} 
\noindent
The kind of \emph{argument interfaces} is $\karg := (\kty \rightarrow \kitm) \times (\kunit \rightarrow \kty\times\kitm)$, i.e. a  product of functions, one for analysis and the other for synthesis. The helpers $\svar{ana}$ and $\svar{syn}$ only project them out, e.g. $\svar{ana} := \slam{\karg}{\svar{arg}}{\keyw{fst}(\svar{arg})}$. To actually perform analysis or synthesis, we must provide a link between the dynamics of the static language and the typing rules. This is purpose of the static forms $\sana{n}{\st}$ and $\ssyn{n}$. We provide each opcon definition with an \emph{argument interface list}, which  a static list of length $n$ where the $i$th entry is $(\slam{\kty}{\svar{ty}}{\sana{i}{\svar{ty}}}, \slam{\kunit}{\_}{\ssyn{i}})$. It is generated by the judgement $\keyw{args}(n)=\stx{args}$, where $n$ is the length of the actual argument list, written $|\es|=n$.

%The $\kargs$ given where the $n$th entry is simply a pair of functions, the first of which allows for analysis of the $n$th argument of the operation against a type, and the second of which requests type synthesis for the $n$th argument. The helper functions simply project out the appropriate functions and invoke them.

Recall that the kinding judgement is indexed by $n$. This is an upper bound on the argument index of terms of the form $\sana{n}{\st}$ and $\ssyn{n}$, enforced in Figure \ref{fig:kinding-ana-syn}. Thus, if $\keyw{args}(n)= \stx{args}$ then $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\stx{args}}{\kargs}$. The rule (ocstruct-intro) ruled out writing either of these forms explicitly in an opcon definition by checking against the bound $n=0$. This is to prevent out-of-bounds errors: tycon providers can only access these forms via the argument interface list, which must have the correct length.


\begin{figure}\vspace{-5px}
\begin{mathpar}\small
\inferrule[k-ana]{
    n' < n\\
    \sofkX{\st}{\kty}
}{
    \sofkX{\sana{n'}{\st}}{\kitm}
}

\inferrule[k-syn]{
    n' < n
}{
    \sofkX{\ssyn{n'}}{\kty\times\kitm}
}\vspace{-6px}
\end{mathpar}
\caption{Kinding for the SL-EL interface.}
\label{fig:kinding-ana-syn}
\vspace{-8px}
\end{figure}
 %This is  safe because $n$ is only an upper bound, so it can be relaxed safely.

For $\sana{n}{\st}$, after normalizing $\st$, the argument environment, which contains the arguments themselves and the typing and tycon contexts, $\argEnv ::= \es; \Upsilon; \Phi$, is consulted to retrieve the $n$th argument and analyze it against $\st$. If this succeeds, $\sqitm{\anatrans{n}{\st}}$ is generated:
\begin{mathpar}\small
\inferrule[s-ana-success]{
    \sval{\st}{\es;\Upsilon;\Phi}\\
    \keyw{nth}[n](\es) = e\\
    \eana{\Upsilon}{\Phi}{e}{\st}{\iota}
}{
    \sstep{\sana{n}{\st}}{\es;\Upsilon;\Phi}{\sqitm{\anatrans{n}{\st}}}
}
\end{mathpar}
If it fails, an error is raised:
\begin{mathpar}\small
\inferrule[s-ana-fail]{
    \sval{\st}{\es;\Upsilon;\Phi}\\
    \keyw{nth}[n](\es) = e\\
    [\Upsilon \vdash_\Phi e \nLeftarrow \st]
}{
    \serr{\sana{n}{\st}}{\es;\Upsilon;\Phi}
}
\end{mathpar}
We write $\small[\Upsilon \vdash_\Phi e \nLeftarrow \st]$ to indicate that $e$ fails to analyze against $\st$. We do not define this  inductively, so we also allow that this premise be omitted, leaving a non-deterministic semantics nevertheless sufficient for our metatheory. 

The dynamics for $\ssyn{n}$ are analagous, evaluating to a pair $(\st, \small\sqitm{\syntrans{n}})$ where $\st$ is the synthesized type. 

The kinding rules also prevent these forms  from being well-kinded when $n = 0$. Like the form $\srep{\st}$, these  are retained in values of kind $\kitm$, as shown in $\qitm_\text{ex}$.

\subsection{Translation Validation}
\noindent
The judgement $\trvalidate{\argEnv}{c}{\qitm}{\srep{\st}}{\itm}$, defined by a single rule in Figure \ref{fig:translation-validation} and appearing as the final premise of (ana-intro), can be pronounced ``translational internal term $\qitm$ generated by an opcon associated with $c$ under argument environment $\argEnv$ is a valid translation for an external term of type $\st$, so the actual translation $\iota$ is produced''. For example, $$\trvalidate{(title; \concstr{EXMPL 2015}); \Upsilon_\text{ex}; \Phi_\text{rstr}\Phi_\text{lprod}}{\tcvar{lprod}}{\qitm_\text{ex}}{\srep{\stx{paper}}}{\iota_\text{ex}}$$

\noindent
The purpose of this rule is to check that the generated translation will be well-typed \emph{no matter what the translations of types other than those constructed by $c$ are}. This \emph{translation independence} property will be the key to our conservativity theorem in Sec. \ref{metatheory}. 

The first premise generates the selectively abstracted type translation for $\st$ given that $c$ was the delegated tycon as described in Sec. \ref{sec:selective-type-translation-abstraction}. In our running example, this is $\tau_\text{abs/paper}$, i.e. $\alpha_0 \times (\alpha_1 \times \kunit)$.

\begin{figure}
\small\fbox{$\trvalidate{\argEnv}{c}{\qitm}{\srep{\st}}{\iota\moutput}$}\vspace{-5px}
\begin{mathpar}\small
\inferrule[validate-tr]{
  \tdeabs{\Phi}{c}{\srep{\st}}{\emptyset}{\ity_\text{abs}}{\memD}\\
  \edeabs{c}{{\es;\Upsilon;\Phi}}{\qitm}{\memD}{\emptyset}{\iota_\text{abs}}{\memD'}{\memG}\\
  \memD' \leadsto \delta : \Delta_\text{abs}\\
  \memG \leadsto \gamma : \Gamma_\text{abs}\\
  \Delta_\text{abs}~\Gamma_\text{abs} \vdash \iota_\text{abs} : \tau_\text{abs}
}{
\trvalidate{\es;\Upsilon;\Phi}{c}{\qitm}{\srep{\st}}{[\delta][\gamma]\iota_\text{abs}}
}\end{mathpar}\vspace{-9px}
\caption{Translation Validation}
\label{fig:translation-validation}\vspace{-8px}
\end{figure}

The judgement $\edeabs{c}{\argEnv}{\qitm}{\memD}{\memG}{\itm\moutput}{\memD\moutput}{\memG\moutput}$, appearing as the next premise, relates a translational internal term $\qitm$ to an internal term $\itm$ called a \emph{selectively abstracted term translation}, because all references to the translation of an argument (having any type) are replaced with a corresponding variable and, as in Sec. \ref{sec:selective-type-translation-abstraction}, all references to the translation of a type constructed by an extension tycon other than the ``delegated tycon'' $c$ are replaced with a corresponding abstract type variable. The type translation store, $\memD$, discussed previously, and term translation store, $\memG$, track these correspondences. Term translation stores have  syntax $\memG ::= \emptyset ~|~ \memG, n : \st \leadsto \iota/x : \tau$. Each entry can be read ``argument $n$ having type $\st$ and translation $\iota$ appears as variable $x$ with type $\tau$'', e.g. $\small\edeabs{\tcvar{lprod}}{(title; \texttt{"EXMPL 2015"});\Upsilon;\Phi_\text{rstr}\Phi_\text{lprod}}{\qitm_\text{ex}}{\memD_\text{paper/abs}}{\emptyset}{\iota_\text{ex/abs}}{\memD_\text{paper/abs}}{\memG_\text{ex/abs}}$ where $\itm_\text{ex/abs} := (x_0, (x_1, ()))$ and $\memG_\text{ex/abs} := 0 : \stx{title} \leadsto title/x_0 : \alpha_0, 1 : \stx{conf} \leadsto \texttt{"EXMPL 2015"}_\text{IL}/x_1 : \alpha_1$ and $\memD_\text{paper/abs}$ is from Sec. \ref{sec:selective-type-translation-abstraction}.


This judgement proceeds recursively along shared forms, like the selectively abstracted type translation judgement in Sec. \ref{sec:selective-type-translation-abstraction}. The key rule for references to argument translations derived via analysis is below ($\syntrans{n}$ is analagous; the full rules are in the supplement).% Note that we are rederiving the translation already determined in (s-ana-success) for simplicity (in practice, this might be cached):
\begin{mathpar}\small
\inferrule[abs-anatrans-new]{
    n \notin \text{dom}(\memG)\\
    \keyw{nth}[n](\es) = e\\
    \eanaX{e}{\st}{\iota}\\
    \tdeabs{\Phi}{c}{\srep{\st}}{\memD}{\ity}{\memD'}\\
    (x~\text{fresh})
}{
    \edeabs{c}{\es;\Upsilon;\Phi}{\anatrans{n}{\st}}{\memD}{\memG}{x}{\memD'}{\memG, n : \st \leadsto \iota/x : \tau}
}
\end{mathpar}


The third premise of (validate-tr) generates the type substitution and type formation contexts from the final type translation store,  $\memD' \leadsto \delta_\text{paper/abs} : \Delta_\text{paper/abs}$ as described in Sec. \ref{sec:selective-type-translation-abstraction}.  Like type translation stores, each term translation store induces an internal term substitution,  $\gamma ::= \emptyset ~|~ \gamma, \iota/x$, and corresponding internal typing context $\Gamma$ by the judgement $\memG \leadsto \gamma : \Gamma$, appearing as the next premise. Here, $\small
\gamma_\text{ex/abs} := title/x_0, \texttt{"EXMPL 2015"}_\text{IL}/x_1$ and $\small\Gamma_\text{ex/abs} := x_0{:}\alpha_0, x_1{:}\alpha_1$. 

Finally, the fifth premise checks the abstracted term translation against the abstracted type translation according to the internal statics. Here, $\Delta_\text{paper/abs}~\Gamma_\text{ex/abs} \vdash \iota_\text{ex/abs} : \tau_\text{ex/abs}$, i.e.: 
\[\small(\alpha_0, \alpha_1)~(x_0 : \alpha_0, x_1 : \alpha_1) \vdash (x_0, (x_1, ())) : \alpha_0 \times (\alpha_1 \times \kunit)\]
So, the translation of the labeled product $e_\text{ex}$ generated by $\tcvar{lprod}$ is checked with the references to term and type translations of regular strings replaced by variables and type variables, respectively. But because our  definition treated arguments parametrically, the check succeeds.% We  describe  an ill-behaved operator in Sec. \ref{metatheory}.

Applying the substitutions $\gamma_\text{ex/abs}$ and $\delta_\text{paper/abs}$ in the conclusion of the rule, we arrive at the actual term translation $\iota_\text{ex}$, defined previously. Note that $\iota_\text{ex}$ has type $\tau_\text{paper}$ under the translation of $\Upsilon_\text{ex}$, i.e. $\vdash \Upsilon_\text{ex} \leadsto \Gamma_\text{ex}$ where $\Gamma_\text{ex} := title : \keyw{str}$.




\subsection{Generalized Targeted Operations} \label{sec:targops}
All non-introductory operations go through another generalized form, in this case for \emph{targeted operations}, $\etarg{\opname{op}}{\sttmidx}{e_\text{targ}}{\es}$, where $\opname{op}$ ranges over opcon names, ${\sttmidx}$ is the term index, $e_\text{targ}$ is the \emph{target argument} and $\es$ are the remaining arguments. Concrete desugarings for this form include $e_\text{targ}.\opname{op}\langle\sttmidx\rangle(\es)$ (and variants where the term index or arguments are omitted), projection syntax for use by record-like types, $e_\text{targ}\opname{\#}\conclbl{lbl}$, which desugars to $\etarg{\opname{\#}}{\conclbl{lbl}}{e_\text{targ}}{\cdot}$, and $e_\text{targ} \cdot e_\text{arg}$, which desugars to $\etarg{\opname{conc}}{\striv}{e_\text{targ}}{e_\text{arg}}$. We show other desugarings, including case analysis, in the supplement.


Whereas introductory operations were analytic, targeted operations are synthetic in @$\lambda$. The type and translation are determined by the tycon of the type synthesized by the target argument. The rule (syn-targ) is otherwise similar to (ana-intro) in its structure. The first premise synthesizes a type, $\sty{\tc}{\sttyidx}$, for the target argument. The second premise extracts the tycon definition for $\tc$ from the tycon context. The third extracts the \emph{operator index kind} from its opcon signature, and the fourth checks the term index against it. 
% :\[\small
% \begin{array}{lcl}
% \chi_\text{rstr} & := & \keyw{intro}[\keyw{Str}], \opsig{\opname{conc}}{\kunit}, \opsig{\opname{case}}{\klist{\keyw{StrPattern}}},  \\
% & & \opsig{\opname{coerce}}{\krx}, \opsig{\opname{check}}{\krx}, \opsig{\opname{replace}}{\krx}\\
% \chi_\text{lprod} & := & \keyw{intro}[\klist{\klbl}], \opsig{\opname{prj}}{\klbl}, \opsig{\opname{conc}}{\kunit}, \opsig{\opname{drop}}{\klist{\klbl}}
% \end{array}
% \]

Figure \ref{fig:example-tycons} showed portions of the opcon signatures of $\tcvar{rstr}$ and $\tcvar{lprod}$. The opcons associated with $\tcvar{rstr}$ are taken directly from Fulton et al.'s specification of regular string types \cite{sanitation-psp14}, with the exception of $\opname{case}$, which generalizes case analysis as defined there to arbitrary string patterns, which we discuss in the supplement. The opcons associated with $\tcvar{lprod}$ are also straightforward: $\opname{\#}$ projects out the row with the provided label and $\opname{conc}$ concatenates two labeled products (updating common rows with the value from the right argument). Note that both $\tcvar{rstr}$ and $\tcvar{lprod}$ can define concatenation without conflict. %Targeted operations use no new mechanisms, so we only show regular string concatenation; others are in the supplement.

The fifth premise of (syn-targ) extracts the \emph{targeted opcon definition} of $\opname{op}$ from the opcon structure, $\omega$. Like the intro opcon definition, this is a static function that generates a translational internal term on the basis of the target tycon's type index, the term index and an argument interface list. Targeted opcon definitions additionally synthesize a type. The rule (ocstruct-targ) in Figure \ref{ocstruct} ensures that no tycon defines an opcon twice and that the opcon definitions are well-kinded. For example, $\stx{rstr/conc} :=$
\[\small
\begin{array}{l}
\keyw{syn}~\opname{conc}=\slam{\krx}{\svar{tyidx}}{
    \slam{\kunit}{\svar{tmidx}}{
        \slam{\kargs}{\svar{args}}{\\
            \quad \keyw{letpair}~(\svar{arg1}, \svar{arg2}) = \svar{arity2}~\svar{args}~\keyw{in}\\
            \quad \keyw{letpair}~(\_, \svar{tr1}) = \svar{syn}~\svar{arg1}~\keyw{in}~  \\\quad\keyw{letpair}~(\svar{ty2}, \svar{tr2}) = \svar{syn}~\svar{arg2}\\
            \quad \stycase{\tcvar{rstr}}{\svar{ty2}}{\svar{tyidx2}}{\\
                \quad\quad (\sty{\tcvar{rstr}}{\svar{rxconcat}~\svar{tyidx}~\svar{tyidx2}}, \sqitm{sconcat~\quq{\svar{tr1}}~\quq{\svar{tr2}}})\\
                \quad
            }{\sraise{\kty\times\kitm}}
        }
    }
}
\end{array}
\]
The helper function $\svar{arity2}$ checks that two arguments, including the target argument, were provided. We then request synthesis of both arguments. We can ignore the type synthesized by the first because by definition it is a regular string type with type index $\svar{tyidx}$. We case analyze the second against $\tcvar{rstr}$, extracting its index regular expression if so and raising an error if not. We then synthesize the resulting regular string type, using the helper function $\svar{rxconcat} :: \krx \rightarrow \krx \rightarrow \krx$ which generates the synthesized type index by concatenating the indices of the argument's types, and finally the translation, using an internal helper function $sconcat : \keyw{str} \rightarrow \keyw{str} \rightarrow \keyw{str}$, the translational term for which we assume has been substituted in directly.

The last premise of (syn-targ) again performs translation validation as described above. The only difference relative to (ana-intro) is that that we check the term translation against the type translation of the synthesized type, but the delegated tycon is that of the type synthesized by the target argument.

\subsection{Operations Over Other Types}
The rule (ana-intro-other) is used to introduce terms of a type constructed by an ``other'' tycon. The term index, rather than the tycon context, directly specifies the static function that maps the arguments to a translation. In all other respects, it is analagous. It is used as a technical device in Sec. \ref{metatheory}.

Like (ana-intro-other), rule (syn-targ-other) is used when the target synthesizes an ``other'' type. The mapping from the arguments to a type and translation is again given directly in the term index (the op name is ignored).



% \section{Metatheory}\label{metatheory}
% %This judgement is only defined for values of kind $\kty$. We write $\st~\texttt{type}_\Phi$ iff $\vdash \Phi$ and $\sofkn{\emptyset}{\emptyset}{\Phi}{0}{\st}{\kty}$ and $\sval{\st}$.

% \begin{definition}[Argument Environment Formation] $\argEnvOK{n}{\es;\Upsilon;\Phi}$ iff $|\es|=n$ and $\vdash \Phi$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$.\end{definition}

% \subsection{Static Language}

% %\begin{lemma}
% %If $\kDelta \vdash \kGamma$ and $\vdash \Phi$ and $\sofkn{\kDelta}{\kGamma}{\Phi}{n}{\st}{\kappa}$ then $\kDelta \vdash \kappa$.\todo{don't think we need this}
% %\end{lemma}

% \begin{theorem}[Static Canonical Forms] If $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\st}{\kappa}$ and $\argEnvOK{n}{\argEnv}$ and $\sval{\st}{\argEnv}$ then:
% \begin{enumerate}
% \item TODO: arrow
% \item TODO: unit
% \item TODO: product
% \item TODO: sum
% \item TODO: inductive
% \item TODO: universal
% \item If $\kappa = \kty$ then $\istype{\st}{\Phi}$ and 
%     \begin{enumerate}
%     \item $\st = \sty{\rightharpoonup}{(\st_1, \st_2)}$ and $\istype{\st_1}{\Phi}$ and $\istype{\st_2}{\Phi}$; or
%     \item $\st = \sty{\tc}{\sttyidx}$ and $\tcdef{\tc}{\tcsig{\ktyidx}{\chi}}{\omega} \in \Phi$ and $\sofkn{\emptyset}{\emptyset}{\Phi}{0}{\st}{\ktyidx}$ and $\svalNA{\sttyidx}$; or
%     \item $\st = \sotherty{m}{\tau}$ and $\emptyset \vdash \tau$.
%     \end{enumerate}
% \item If $\kappa = \kity$ then $\st=\sqity{\qity}$ and $\sofkn{\emptyset}{\emptyset}{\Phi}{0}{\st}{\kity}$ and $\svalNA{\st}$ and $\qtuq{\st'} \notin \qity$ and 
%     \begin{enumerate}
%     \item The outer form of $\qity$ is shared with $\tau$ and if $\qity'$ is a sub-term of $\qity$ then $\sofkn{\emptyset}{\emptyset}{\Phi}{0}{\sqity{\qity'}}{\kity}$ and $\svalNA{\sqity{\qity'}}$; or
%     \item $\qity = \srep{\st'}$ and $\istype{\st'}{\Phi}$
%     \end{enumerate}
% \item If $\kappa = \kitm$ then $\st=\sqitm{\qitm}$ and no sub-terms of $\qitm$ have the form $\quq{\st'}$ and 
%     \begin{enumerate}
%     \item The outer form of $\qitm$ is shared with $\itm$ and if $\qitm'$ is a sub-term of $\qitm$ then $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\sqitm{\qitm'}}{\kitm}$ and $\sval{\sqitm{\qitm'}}{\argEnv}$ and if $\qity$ is a sub-term of $\qitm$ then $\sofkn{\emptyset}{\emptyset}{\Phi}{0}{\sqity{\qity}}{\kity}$ and $\svalNA{\sqity{\qity}}$; or
%     \item $\qitm=\anatrans{n'}{\st'}$ and $n' < n$ and $\istype{\st'}{\Phi}$; or
%     \item $\qitm=\syntrans{n'}$ and $n' < n$.
%     \end{enumerate}
% \end{enumerate}
% \end{theorem}

% \begin{theorem}[Static Progress]
% If $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\st}{\kappa}$ and $\vdash \Phi$ and $\vdash^n \argEnv$ then $\sval{\st}{\argEnv}$ or $\serr{\st}{\argEnv}$ or $\sstep{\st}{\argEnv}{\st'}$.
% \end{theorem}
% \begin{proof}We proceed by rule induction on the kinding judgement.
% (k-parr)

% (k-tc)

% (k-otherty)

% (k-tccase-parr)

% (k-tccase)

% (k-ity-lam)

% (k-ity-alpha)

% (k-ity-unquote)

% (k-ity-trans)

% (k-raise)

% (k-itm-var)

% (k-itm-lam)

% (k-itm-unquote)

% (k-itm-anatrans)

% (k-itm-syntrans)
% \end{proof}

% \begin{theorem}[Static Preservation]
% If $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\st}{\kappa}$ and $\vdash \Phi$ and $\vdash^n \argEnv$ and $\sstep{\st}{\argEnv}{\st'}$ then $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\st'}{\kappa}$.
% \end{theorem}
% \begin{proof}
% (s-ty-step) (2 cases)
% (s-tycase-step) (2 cases)
% (s-tycase-match) (2 cases)
% (s-tycase-fail-1)
% (s-tycase-fail-2)
% (s-ity-lam-step-1)
% (s-ity-lam-step-2)
% (s-ity-unquote-step)
% (s-ity-unquote-elim)
% (s-ity-trans-step)
% (s-itm-lam-step-1)
% (s-itm-lam-step-2)
% (s-itm-unquote-step)
% (s-itm-unquote-elim)
% (s-ana-step)
% (s-ana-success)
% (s-syn-success) (relies on type synthesis theorem)
% (s-itm-anatrans-step)
% \end{proof}

% % \begin{lemma}[Kinding Stability]
% % If $\vdash \Phi$ and $\vdash \Phi, \tcdef{\tc}{\psi}{\theta}$ and $\kDelta \vdash \kGamma$ and $\sofkn{\kDelta}{\kGamma}{\Phi}{n}{\st}{\kappa}$ then $\sofkn{\kDelta}{\kGamma}{\Phi, \tcdef{\tc}{\psi}{\theta}}{n}{\st}{\kappa}$.
% % \end{lemma}
% % \begin{proof}
% % all cases straightforward by induction
% % \end{proof}

% % TODO: static normalization stability inside A?

% \subsection{Types}
% \begin{lemma}[Type Substitution Application]\label{thm:type-substitution-application}
% If $\vdash \delta : \Delta$ and $\Delta \vdash \tau$ then $\emptyset \vdash [\delta]\tau$.
% \end{lemma}

% \begin{lemma}[Selective Type Abstraction]\label{thm:selective-type-abstraction}
% If $\vdash \Phi$ and $\sofkz{\emptyset}{\emptyset}{\Phi}{\sqity{\qity}}{\kity}$ and $\svalNA{\sqity{\qity}}$ and $\memD \leadsto \delta : \Delta$ and $\vdash \delta : \Delta$ and $\tdeabs{\tc}{\Phi}{\qity}{\memD}{\tau}{\memD'}$ then $\memD' \leadsto \delta' : \Delta'$ and $\delta \subseteq \delta'$ and $\Delta \subseteq \Delta'$ and $\vdash \delta' : \Delta'$ and $\Delta' \vdash \tau$.
% \end{lemma}
% \begin{proof}
% (shared forms) (trans(st))
% \end{proof}

% \begin{lemma}[Type Translation]\label{type-translation}
% If $\vdash \Phi$ and $\istype{\st}{\Phi}$ and $\vdash_\Phi \st \leadsto \tau$ then $\emptyset \vdash \tau$.
% \end{lemma}
% \begin{proof} By Lemma \ref{thm:type-substitution-application} and Lemma \ref{thm:selective-type-abstraction}.
% \end{proof}

% \begin{lemma}[Typing Context Translation]
% If $\vdash \Phi$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ then $\emptyset \vdash \Gamma$.\end{lemma}
% \begin{proof} We proceed by rule induction on typing context translation. Empty case trivial. Extended case follows by Lemma \ref{type-translation} and definition of internal typing context formation.\end{proof}

% \subsection{External Terms}
% \begin{theorem}[Type Synthesis]
% If $\vdash \Phi$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ and $\esynX{e}{\st}{\iota}$ then $\istype{\st}{\Phi}$ and $\vdash_\Phi \st \leadsto \tau$. \end{theorem}
% \begin{proof}
% (var) (ap) (syn-targ)
% \end{proof}

% \begin{lemma}[Selective Term Abstraction]
% If $\vdash \Phi$ and $\vdash^n \argEnv$ and $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\sqitm{\qitm}}{\kitm}$ and $\sval{\sqitm{\qitm}}{\argEnv}$ and $\memD \leadsto \delta : \Delta$ and $\vdash \delta : \Delta$ and $\emptyset \vdash \Gamma_\text{out}$ and $\memG \leadsto \gamma : \Gamma$ and $\Delta~\Gamma_\text{out} \vdash \gamma : \Gamma$ and $\edeabs{\tc}{\argEnv}{\qitm}{\memD}{\memG}{\itm}{\memD'}{\memG'}$ then $\memD' \leadsto \delta' : \Delta'$ and $\memG' \leadsto \gamma' : \Gamma'$ and $\delta \subseteq \delta'$ and $\Delta \subseteq \Delta'$ and $\gamma \subseteq \gamma'$ and $\Gamma \subseteq \Gamma'$ and $\vdash \delta' : \Delta'$ and $\Delta'~\Gamma_\text{out} \vdash \gamma' : \Gamma'$.
% \end{lemma}
% \begin{proof} By rule induction on selective term abstraction judgement.

% (shared forms) (unquote form not possibly by canonical forms) (anatrans) (syntrans)
% \end{proof}

% \begin{theorem}[Type-Preserving Translation]
% If $\vdash \Phi$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ and either
% \begin{enumerate}
% \item $\eanaX{e}{\st}{\iota}$ and $\istype{\st}{\Phi}$; or 
% \item $\esynX{e}{\st}{\iota}$
% \end{enumerate}
% then $\vdash_\Phi \st \leadsto \tau$ then $\emptyset~\Gamma \vdash \iota : \tau$.
% \end{theorem}
% \begin{proof} By rule induction on typing rules.

% (var) (lam) (ap) (fix) (subsume) (ascribe) (ana-intro) (syn-targ) (other)
% \end{proof}

% TODO: stability of typing
% TODO: Unicity of typing
% TODO: hygiene? 

% TODO: Make definitions out of these.
% The standard judgement $\Gamma \vdash \gamma : \Gamma'$ states that every binding $x : \tau$ in $\Gamma'$ has a corresponding substitution $\iota/x$ in $\gamma$ such that $\Gamma \vdash \iota : x$.  

% , and  the judgement $\vdash \delta : \Delta$ checks that every type variable in $\Delta$ has a well-formed substitution in $\delta$

% \subsection{Conservativity}
% \begin{theorem}[Conservativity]
% If $\vdash \Phi$ and $\istype{\sty{\tc}{\sttyidx}}{\Phi}$ and for all $e$, if $\eana{\emptyset}{\Phi}{e}{\sty{\tc}{\sttyidx}}{\iota}$ and $\iota \Downarrow \iota'$ then $P(\iota')$, then if $\vdash \Phi, \tcdef{\tc'}{\psi}{\theta}$ then for all $e$, if $\eana{\emptyset}{\Phi, \tcdef{\tc'}{\psi}{\theta}}{e}{\sty{\tc}{\sttyidx}}{\iota}$ and $\iota \Downarrow \iota'$ then $P(\iota')$.
% \end{theorem}

% \begin{lemma}[Other Substitution]
% If $\vdash \Phi, \tcdef{\tc}{\psi}{\theta}$ and $\istype{\st}{\Phi}$ and $\eana{\Upsilon}{\Phi, \tcdef{\tc}{\psi}{\theta}}{e}{\st}{\iota}$ then there exists $e'$ such that $\eana{\Upsilon}{\Phi}{e'}{\st}{\iota}$.
% \end{lemma}

\section{Metatheory}\label{metatheory}
\noindent We will now state the key metatheoretic properties of @$\lambda$. The full proofs are in the supplement. 

\paragraph{Kind Safety} Kind safety ensures that normalization of well-kinded static terms cannot go wrong. We can take a standard progress and preservation based approach. 
\begin{theorem}[Static Progress]\label{thm:static-progress}
If $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\st}{\kappa}$ and $\vdash \Phi$ and $|\es|=n$ then $\sval{\st}{\es; \Upsilon; \Phi}$ or $\serr{\st}{\es; \Upsilon; \Phi}$ or $\sstep{\st}{\es; \Upsilon; \Phi}{\st'}$.
\end{theorem}

\begin{theorem}[Static Preservation]\label{thm:static-preservation}
If $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\st}{\kappa}$ and $\vdash \Phi$ and $|\es|=n$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ and $\sstep{\st}{\es; \Upsilon; \Phi}{\st'}$ then $\sofkn{\emptyset}{\emptyset}{\Phi}{n}{\st'}{\kappa}$.
\end{theorem}\noindent
The case in the proof of Theorem \ref{thm:static-preservation} for $\st=\ssyn{n}$ requires that the following theorem be mutually defined. %The mutual induction is well-founded because the total number of intro and targeted sub-terms being considered decreases.
\begin{theorem}[Type Synthesis]
If $\vdash \Phi$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ and $\esynX{e}{\st}{\iota}$ then $\vdash_\Phi \st \leadsto \tau$ (and thus $\istype{\st}{\Phi}$). 
\end{theorem}
\paragraph{Type Safety}
Type safety in a typed translation semantics requires that well-typed external terms translate to well-typed internal terms. Type safety for the IL \cite{pfpl} then implies that evaluation cannot go wrong. To prove this, we must in fact prove a stronger theorem: that a term's translation has its type's translation (the analagous notion is \emph{type-preserving compilation} in type-directed compilers \cite{tarditi+:til-OLD}):% produced as the translation of the external type. Note that we need only to state our top-level theorems for the analysis judgement  because of the subsumption rule.

\begin{theorem}[Type-Preserving Translation]
If $\vdash \Phi$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ and $\vdash_\Phi \st \leadsto \tau$ and $\eanaX{e}{\st}{\iota}$ then $\emptyset \vdash \Gamma$ and $\emptyset \vdash \tau$ and $\emptyset~\Gamma \vdash \iota : \tau$.
\end{theorem}
\begin{proof-sketch}
The interesting cases are (ana-intro), (ana-intro-other), (syn-trans) and (syn-trans-other); the latter two arise via subsumption. The result follows directly from the final premise of each rule, combined with lemmas that state that if $\memD \leadsto \delta : \Delta_\text{abs}$ and $\memG \leadsto \gamma : \Gamma_\text{abs}$ then $\emptyset \vdash \delta : \Delta_\text{abs}$ and $\Delta_\text{abs}~\Gamma \vdash \gamma : \Gamma_\text{abs}$, i.e. that all variables in $\Delta_\text{abs}$ and $\Gamma_\text{abs}$ have well-formed/well-typed substitutions in $\delta$ and $\gamma$,  and so applying them gives a well-typed term.
\end{proof-sketch}

\paragraph{Hygienic Translation} 
Note above that the domains of $\Upsilon$ (and thus $\Gamma$)  and $\Gamma_\text{abs}$ are disjoint. This serves to ensure \emph{hygienic translation} -- translations cannot refer to variables in the surrounding scope directly, so uniformly renaming a variable cannot change the meaning of a program. Variables in $\Upsilon$ can  occur in arguments (e.g. $title$ in the earlier example), but the translations of the arguments only appear \emph{after} the substitution $\gamma$ has been applied. We assume that substitution is capture-avoiding in the usual manner. %(i.e. consistent with a locally nameless implementation).


\paragraph{Stability}\todo{if space, bring unicity back}
Extending the tycon context does not change the meaning of any terms that were previously well-typed.
\begin{theorem}[Stability]
Letting $\Phi' := \Phi, \tcdef{\tc}{\psi}{\theta}$, if $\vdash \Phi'$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ and $\vdash_\Phi \st \leadsto \tau$ and $\eanaX{e}{\st}{\iota}$ then $\vdash_{\Phi'} \Upsilon \leadsto \Gamma$ and $\vdash_{\Phi'} \st \leadsto \tau$ and $\eana{\Upsilon}{\Phi'}{e}{\st}{\iota}$.
\end{theorem}

\paragraph{Conservativity} 
Extending the tycon context also conserves all \emph{tycon invariants} maintained in any smaller tycon context. An example of a tycon invariant is the following:

\begin{tyconinvariant}[Regular String Soundness]
If $\eana{\emptyset}{\Phi_\text{rstr}}{e}{\sty{\tcvar{rstr}}{\concrx{r}}}{\iota}$ and $\iota \Downarrow \iota'$ then $\iota'=\texttt{"s"}$ and $\texttt{"s"}$ is in the regular language $\mathcal{L}(\texttt{r})$.
\end{tyconinvariant}
\begin{proof-sketch} The proof is not unusually difficult because we have fixed the tycon context $\Phi_\text{rstr}$, so we can essentially treat the calculus like a type-directed compiler for a calculus with only two tycons, $\rightharpoonup$ and $\tcvar{rstr}$, plus one ``other'' one. Such a calculus and compiler specification was given in \cite{sanitation-psp14}, so we must simply show that the opcon definitions in $\tcvar{rstr}$ adequately satisfy these specification using standard techniques for the SL, a simply-typed functional language \cite{conf/pldi/Chlipala07}. The only ``twist'' is that the rule (syn-targ-other) can synthesize a regular string type. But if so, $\iota_\text{abs}$ will be checked against $\tau_\text{abs}=\alpha$. Thus, the invariants cannot be violated by direct application of relational parametricity in the IL (i.e. a ``free theorem'') \cite{WadlerThms}. \end{proof-sketch}

The reason why (syn-targ-other) is never a problem in proving a tycon invariant -- \emph{translation independence} of tycons -- turns out to be the same reason extending the tycon context conserves all tycon invariants. A newly introduced tycon defining a targeted operator that synthesizes a regular string type, e.g. $\stx{paper}$, and generating a translation that is not in the corresponding regular language, e.g. $\texttt{""}$, could be defined, but when used, the rule (syn-targ) would check the translation against $\tau_\text{abs}=\alpha$, which would fail. %Type abstraction is the basis for conservatively composing type system fragments, just like it is the basis for composing ML-style modules.

 We can state this more generally:

\begin{theorem}[Conservativity] If $\vdash \Phi$ and $\tc \in \text{dom}(\Phi)$ and a tycon invariant for $\tc$ holds under $\Phi$: \begin{itemize}
\item For all $\Upsilon, e, \sttyidx$, if $\eana{\Upsilon}{\Phi}{e}{\sty{\tc}{\sttyidx}}{\iota}$ and $\vdash_\Phi \Upsilon \leadsto \Gamma$ and $\vdash_\Phi {\sty{\tc}{\sttyidx}} \leadsto \tau$  then $P(\Gamma, \sttyidx, \iota)$.
\end{itemize} then for all $\Phi' = \Phi, \tcdef{\tc'}{\psi'}{\theta'}$ such that $\vdash \Phi'$, the same tycon invariant holds under $\Phi'$: \begin{itemize}
\item For all $\Upsilon, e, \sttyidx$, if $\eana{\Upsilon}{\Phi'}{e}{\sty{\tc}{\sttyidx}}{\iota}$ and $\vdash_{\Phi'} \Upsilon \leadsto \Gamma$ and $\vdash_{\Phi'}{\sty{\tc}{\sttyidx}}\leadsto{\tau}$ then $P(\Gamma, \sttyidx, \iota)$.
\end{itemize}
(if proposition $P(\Gamma, \st, \iota)$ is \emph{modular}, defined below)
\end{theorem}
\begin{proof-sketch}
The proof maps every well-typed term under $\Phi'$ to a well-typed term under $\Phi$ with the same translation, and if the term has a type constructed by a tycon in $\Phi$, e.g. $\tc$, the new term has a type constructed by that tycon with the same type translation, and only a slightly different type index. In particular, the mapping's effect on static terms is to replace all types constructed by $\tc'$ with a type constructed by $\keyw{other}[m;\ktyidx']$ for some $m$ corresponding to $\tc'$ and the index kind of $\tc'$. If $P(\Gamma, \st, \iota)$ is preserved under this transformation then we can simply invoke the existing proof of the tycon invariant. We call such propositions \emph{modular}. Non-modular propositions are uninteresting because they distinguish tycons ``from the future''. 

On external terms, the mapping replaces all intro and targeted terms associated with $\tc'$ with an equivalent one that passes through the rules (ana-intro-other) and (syn-targ-other) by partially applying the intro and targeted opcon definitions to generate the term indices. Typing, kinding, static normalization and selective translation abstraction are preserved under the mapping, defined inductively in the supplement. Note that for this reason all propositions decidable by the SL are modular. \end{proof-sketch}
%Had the translational term generated by the intro opcon definition been, e.g., $(\anatrans{0}{\stx{title}}, (\texttt{"TEST"}, ())$, then the corresponding abstract term would be $(x_0, (\texttt{"TEST"}, ())$ and the check would fail because $(\alpha_0, \alpha_1)~(x_0 : \alpha_0) \nvdash \texttt{"TEST"} : \alpha_1$. Had we not gone through the machinations of holding types and terms abstract, however, the check would succeed, because $\keyw{str}$ would no longer have been held abstract as $\alpha_1$. This form of ``type translation independence'' is precisely analagous to the representation independence properties that underly abstraction theorems for module systems based on  abstract types and will similarly serve to ensure that the type invariants maintained by $\tcvar{rstr}$ are conserved when new tycons are defined. In this case the invariant is that only strings that are in the regular language specified in the type index can be generated from an external term of regular string type. Note that $\texttt{"TEST"}$ is not in the regular language specified by $\concrx{[A-Z]+ \digit\digit\digit\digit}$, so it is critical that it not be allowed, despite being consistent with the type translation of $\stx{conf}$, i.e. it is a string. Such an invariant could not be maintained if the type system were treated as merely a ``bag of rules''.\todo{put conservativity here?} %We consider this more rigorously in Sec. \ref{metatheory}.

\section{Related Work and Discussion}\label{prior-work}
%\paragraph{Term Rewriting}
Language-integrated static term rewriting systems, like Template Haskell \cite{SheardPeytonJones:Haskell-02} and Scala's static macros \cite{ScalaMacros2013}, can be used to decrease complexity when an isomorphic embedding into the underlying type system is possible. % If each metaprogram is invoked like a function, inner macros are expanded before outer macros, and the mechanism enforces abstraction barriers by ensuring that the rewriting logic is hygienic, cannot modify surrounding code, and does not depend on shared state, then compositional reasoning is possible: the meaning of a term depends only on its subterms, not on the specific position it appears in a program.% Complex macros can, however, still be difficult to reason about, so many static macro systems impose further constraints (e.g. Scala recommends using only its ``black box'' macros, which  enforce a function-like typing discipline). These do not permit extensions to the underlying type system.
%Rewriting systems that permit global pattern-based dispatch, however, do not admit strong modular reasoning principles (the same term might match multiple patterns, defined separately, creating ambiguities). 
Similarly, when an embedding that preserves a desired static semantics exists, but a different  embedding preserves the cost semantics, term rewriting  can also be used to perform ``optimizations'', achieving an isomorphism. Care is needed when this changes the type of a term. Type abstraction has been used for  this purpose in \emph{lightweight modular staging} (LMS) \cite{Rompf:2012:LMS}. In both cases, the type system is fixed (e.g. in LMS, Scala's).

% \paragraph{Desugaring}\label{desugaring}
% Sometimes, dialects address issues of complexity by introducing new syntax. Indeed, most dialects do build in syntax for a few privileged abstractions (e.g. list literals are nearly ubiquitous, monad comprehensions support a key feature of Haskell). %For example, even though lists can be defined using datatypes, most languages build in special ``literal forms'' that make introducing and pattern matching over lists less tedious. Different dialects may similarly choose to  provide concise forms for working with other types of data (e.g. option types, regular expressions or SQL queries). Types not in the language's standard library, however, must typically use a more uniform syntax. 
% %Evidence suggests that this leads programmers to choose less semantically meaningful representations (e.g. representing SQL queries as strings), which can cause a number of issues \cite{TSLs}. 
% To make the situation less asymmetric, systems that introduce new  ``desugarings'' atop a base language have been developed. For example, %Camlp4 is a language-external system used by the OCaml community. 
% dialects of languages built using Sugar* \cite{erdweg2013framework} allow syntax extensions to be packaged separately and combined using language-integrated declarations. If the desugaring logic obeys the same constraints described above for macros, semantic reasoning can be performed compositionally (and recent work has shown how reasoning about type correctness can be automated \cite{conf/icfp/LorenzenE13}). 

% These systems do not guarantee that the composition of unambiguous grammars will remain unambiguous (and realistic examples of ambiguous combinations are not uncommon, e.g. XML and HTML). %Every combination of desugarings is thus a  dialect with respect to syntactic reasoning. 
% Recent work has made progress in addressing this problem by restricting how syntax extensions can interface with the host syntax. Schwerdfeger and Van Wyk require a globally unique start token and describe checkable conditions pertaining to the follow sets of host language non-terminals to guarantee that extensions can be composed unambiguously \cite{conf/pldi/SchwerdfegerW09}. This suffices for modularly adding new keyword-prefixed forms, but literal forms are awkward to define in this way. Omar et al. describe a language-integrated technique specifically for this purpose, using a technique with parallels to the one we are building on \cite{TSLs}. Literal parsing logic is directly associated with extension types, forming \emph{type-specific languages} (TSLs), and local type inference controls invocation of TSL parsing logic, guaranteeing that composition is unambiguous. The mechanism guarantees hygiene and inner terms cannot be inspected, so  fully modular reasoning is possible.

%We assumed these desugaring techniques were available to solve issues strictly related to complexity and convenience.

%\paragraph{Optimization}
 %This does not enable new type systems, i.e. in \cite{Rompf:2012:LMS}, Scala's type system is used. %As long as transformations are meaning-preserving (which can also be shown mechanically), they can be applied in any order. %The metaprograms can modularly be shown not to violate the type system of the host language, Modular reasoning is possible as long as dispatch is explicit (e.g. macros) or based on an unambiguous language mechanism (e.g. trait composition in LMS) and made simpler by the fact that transformations must be type-preserving.

%\paragraph{Refinement Types}
%When T, the solution is less clear. 
When new static distinctions are needed within an existing type, but new operators are not necessary, one solution is to develop an overlying system of \emph{refinement types} \cite{Freeman91}. For example, a refinement of  integers might distinguish negative integers. Proposals for ``pluggable type systems'' describe composing such   systems \cite{Brac04a,Andreae:2006:FIP:1167473.1167479}. Refinements of abstract types can be used for representation independence, but note that the type being refined is not held abstract. %Because the dynamic semantics are fixed,  composition is safe. Although our regular string types could perhaps be approximated with refinements, this does not permit the advanced operations, optimizations and representation independence results we enable.
Were it to be, the system could be seen in ways as a degenerate mode of use of our work: we further cover the cases when new  operators are needed. 
For example, labeled tuples couldn't be seen as refinements of nested pairs because the row projection operators don't exist. %Labeled products are not refinements of binary products because the label projection operator is not defined for binary products.%Regular string types could also support an operation that the internal string type would not (e.g. extraction of a subgroup defined in the type index).% The regular string type's group projection operator had a specialized cost semantics, so a simple refinement would not be able to approximate it. Scala's type system does not track the necessary invariants, so LMS-like optimizations would also not be appropriate.%For example, projection operators,  written concretely in some dialects of ML as \verb|#label|, can be seen as applications of \verb|#|, the projection operator constructor, to \verb|label|, a static index. The type of an operation like \verb|#label e| (sometimes written postfix as \verb|e#label|) is a function of the index of the type of \verb|e|. The dynamics also needs (some trace of) this information to determine how the operation is evaluated.

%Many of the other examples mentioned above as motivations for new dialects have a similar flavor. We will detail another example in the next section of a type constructor that tracks strings known statically to be in a regular language, using it to dispense with unnecessary run-time checks in cases where they are not strictly necessary (affecting the cost component of the dynamic semantics).

%An important observation is that while it is possible to \emph{implement} records by translation to tuples, or nested binary pairs, or lists, or many other types arising from a language like $\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\keyw{1}}\,{\times}\,{+}\}$, it is not possible to isomorphically embed them as such (and thus rely on purely syntactic sugar) because there is no unique inverse mapping from, e.g., unlabeled product types to the corresponding record type (many combinations of labeled products may be implemented using the same unlabeled product). This is a common pattern even in more complex situations: a simple internal language (IL) generally suffices as a target for implementing the dynamics of an external language (EL) with a richer static semantics, but an embedding is not possible. This observation underlies the design of most compilers.



 % like SML, Specifying languages in a manner analagous to how they are usually implemented can simplify reasoning about the core of the language. 

% and typed translation semantics (the internal terms have a different type system).% Translation semantics 

%These types do not admit an isomorphic embedding using only unlabeled tuples because the field projection family of operators, indexed by labels (field projection is written concretely as \verb|#label(e)|, \verb|e#label| or \verb|e.label| in different languages), must be capable of operating on all values of such types. % with the labels removed would not admit a unique inverse. 
%(each equivalence class of record types could be implemented as an abstract type using tuples at considerable inconvenience but there would still be no way to define a projection operator that operated uniformly over all records having a particular field). 

% (i.e. fragments defining families of operations that cannot simply be written as a finite collection of functions, examples of which we will discuss). If there are a finite number of operations, and they can be typechecked without inspecting the structure of the types of the arguments, then an abstract type can be used. new primitive operations (including new variants of existing operations) that require static knowledge of these invariants, however, this too is infeasible. take advantage of the invariants being maintained, and these operations must be usable throughout a program, . %At best, annotations might be written as ``comments'' or metadata for use by these tools.

Many \emph{language frameworks} exist that can simplify  dialect implementation (cf. \cite{erdweg2013state}). %Some \emph{extensible compilers} are also actually language frameworks, because they permit the introduction of new constructs, not just new meaning-preserving optimizations. The maintainers of many compilers, e.g. Haskell's GHC, treat their compiler as a ``laboratory'' for new fragments, toggled by flags or special comments. \emph{Logical frameworks} focus more specifically on helping language designers mechanize the metatheory of language and logic specifications. T
These sometimes do not support forming  languages from fragments due to the ``expression problem'' (EP) \cite{wadler1998expression,Reynolds75}. We sidestep the most serious consequences of the EP by leaving our abstract syntax entirely fixed, instead  delegating to tycons. Fewer tools require knowledge of all external tycons in a typed translation semantics. As discussed previously, our treatment of concrete syntax in both the EL and SL defers to recent work on \emph{type-specific languages}, which takes a similar bidirectional approach for composably introducing syntactic desugarings \cite{TSLs}. Some language frameworks do address the EP, e.g. by encoding terms and types as open datatypes \cite{conf/ppdp/LohH06}, but this makes it quite difficult to reason modularly, particularly about metatheoretic properties specific to typed languages, like type safety and tycon invariants. Our key insight is to instead associate term-level opcons with tycons, which then become the fundamental constituents of the semantics (consistent with Harper's informal notation \cite{pfpl}).

Proof assistants can be used to specify and mechanize the metatheory of languages, but also usually require a complete specification (this  has been identified as a key challenge \cite{aydemir05tphols}). Techniques for composing specifications and proofs exist \cite{conf/popl/DelawareOS13,Delaware11,conf/plpv/SchwaabS13}, but they  require additional proofs at composition-time and provide no guarantees that \emph{all} fragments having some modularly checkable property can safely and  conservatively be composed, as in our work. %For example, Delaware et al. develop a technique based on software product lines that requires specifying assumptions at feature boundaries and providing proofs of these for each composition \cite{Delaware11}. In later work based on Mendler-style $f$-algebras, this problem of \emph{feature interactions} persists for many lemmas, including those related to tycon invariants (e.g. canonical forms) \cite{conf/popl/DelawareOS13}. 
The authors, along with Chlipala  \cite{Chlipala10}, suggest  proof automation as a heuristic solution to the problem. 

In contrast, in @$\lambda$, fragment providers need not provide the semantics with mechanized proofs to benefit from modular reasoning principles. Instead, under a fixed tycon context, the calculus can be reasoned about like a very small type-directed compiler \cite{tarditi+:til-OLD,conf/pldi/Chlipala07}. Errors in reasoning can only lead to failure at typechecking time, via a novel form of \emph{translation validation} \cite{Pnueli-Siegel-Singerman98}. Incorrect opcon definitions (relative to a specification, e.g. \cite{sanitation-psp14} for regular strings) can at worst weaken expected invariants at that tycon, like incorrectly implemented modules in ML. Thus, modular tycons can reasonably be tested ``in the field'' without concern about the reliability of the system as a whole. 
To eliminate even these localized failure modes for ``reliability-critical'' tycons, we plan to introduce \emph{optional} proof mechanization into the SL (by basing it on a dependently typed language like Coq). %We also hope to mechanize the metatheory of @$\lambda$ itself in future work.%Every combination of fragments is a new dialect, and must be reasoned about monolithically. %Efforts to reason modularly areeed, several problems can come up when fragments are combined, so . 
% Let us briefly review difficulties that arise. 

% %Concrete syntax known to be separately unambiguous might not be unambiguous when combined, as was already discussed above. 
% If the abstract syntax needs to be extended to support a new fragment, problems also arise. In monolithic settings, terms can be implemented using finite recursive sums (i.e. term constructors are often implemented as ML-style datatype constructors), but this does not permit extension, so open sum types or products of functions (i.e. objects \cite{conf/oopsla/Aldrich13}) must instead be used. This can present issues when one wishes to modularly define new functionality that should exhaustively cover all terms in the language (e.g. pretty-printers for expressions). Reynolds first identified this problem \cite{Reynolds75} and Wadler named it the \emph{expression problem}. 
% %A number of language frameworks (e.g. JastAdd \cite{Ekman:2007:JEJ:1297027.1297029} and Silver \cite{VanWyk:2010:SEA}) use extensible \emph{attribute grammars}. These can be seen as a form of open sum, where attributes correspond to functions performing traversals (more specifically, \emph{catamorphisms} \cite{catamorphisms}\todo{citation / remove?}). %When a new term constructor is added, the logic determining how some existing attributes (e.g. typechecking and translation) should handle the new case is also provided. \emph{Forwarding} can be used to attempt to delegate responsibility over attributes other than those explicitly defined to another term constructor, eventually leading to one in the fixed internal language \cite{VanWyk:2010:SEA}. This can address some aspects of the expression problem, but creates a bigger problem: the internal implementation details  of a fragment are necessarily exposed, violating an abstraction barrier that, as we will discuss, is critical.
% In this work, we sidestep the problems of syntax, instead leaving it fixed and relying on a bidirectional type system delegating to a relevant tycon. %For example, projection operators do not require adding a corresponding term constructor to the abstract syntax. Instead, projection is categorized as a \emph{targeted operation}, so  the concrete term \verb|#label e| desugars to an abstract term like $\keyw{targ}[(\desugar{\conclbl{\#}}, \desugar{\conclbl{\texttt{label}}})](e)$ (where $\desugar{\cdot}$ denotes an encoding of constant labels, here a label corresponding to the operator constructor and the field label itself, into the static language). The type constructor of the type synthesized by the target argument, here $e$, determines the semantics of the term. If a new record-like fragment is added (e.g. they give the example of one with prototypic inheritance), it can reuse the same concrete and abstract syntax directly, precluding conflicts and avoiding the need to define the behavior of tools like pretty printers for every new fragment. As in their work on type-specific languages, described above, the semantics of literal forms are controlled by the type constructor of the type the literal is being analyzed against. 
% %We will discuss this in the next section.

% Once syntactic issues have been addressed, however, there are a host of semantic guarantees  that must be established before a language can be relied upon, as we saw. % The most basic guarantee is \emph{type safety}: that the dynamics are well-defined for all terms accepted as well-typed by the statics, and preserve the statics during evaluation. Each fragment can then build on type safety to establish additional \emph{type invariants} stating properties about all types that it constructs, which  clients can then use to reason about programs (e.g. value induction for eager sum types relies on finiteness of the canonical forms). In practice, typechecking is expected be \emph{deterministic} and \emph{decidable} (i.e. terminating) and, except in circumstances where non-determinism is explicitly exposed to programmers, the dynamic semantics are also expected to be deterministic. %Precise formulations of these properties depend on how the semantics are specified.%, so we will make this more precise below.
% Modern language frameworks guarantee few or none of these properties about the dialects they produce. More alarmingly, even when these properties have been established for two dialects  (either in the metatheory or mechanically using a logical framework), and syntactic conflicts are addressed, there is no guarantee that merging the dialects together will conserve these properties. % If our goal is to integrate fragment composition into the language, these tools are thus of limited utility. Clients would have to take on the burden of reasoning about these basic properties for every combination of ``libraries'' they chose to import.% Improving this state of affairs, so that it more resembles reasoning about separately defined modules (we assume an ML-style module system), is the general topic of this paper.

% %Though we have not mechanically proven these properties for our design, we have given strong evidence that these properties are maintained modularly. That is, we need only establish the semantics of the opcon structures in isolation. %We can then rely on, e.g., the Conservativity theorem above. 

%We note that is increasingly being deployed in contemporary languages (e.g. Scala) due to the perception that it enables good error messages, and to support semantics where  non-local type inference is undecidable. Permitting extensibility together with non-local type inference is an open problem.

Type abstraction, encouragingly, also underlies modular reasoning in ML-like languages \cite{pfpl,harper1997programming} and languages with other forms of ADTs \cite{liskov1974programming} like Scala \cite{conf/oopsla/AminRO14}. Indeed, proofs of tycon invariants can rely on existing parametricity theorems \cite{WadlerThms}. 
Our work is reminiscent of  work on elaborating an ML-style module system into System $\mathbf{F}_\omega$ \cite{conf/tldi/RossbergRD10}.  Unlike in module systems, type translations (analagous to the choice of representation for an abstract type) are statically \emph{computed} based on a type index, rather than statically \emph{declared}. Moreover, there can be arbitrarily many operators because they arise by providing a term index to an opcon, and their  semantics can be complex because a static function computes the types and translations that arise. In contrast, modules and ADTs can only specify a fixed number of operations, and each must have function type. Note however that these are not competing mechanisms: we did not specify quantification over external types here for simplicity, but we conjecture that it is complementary and thus @$\lambda$ could serve as the core of a language with an ML-style module system. Another related direction is \emph{tycon functors}, which would abstract over tycons with the same signature to support tunable cost semantics. % This permits us tocode targeting a separate IL, and permit operations whose types cannot simply be written as functions. %Both examples we used here would be quite difficult to fully embed using only modules.


A limitation of our approach is that it supports only  fragments with the standard ``shape'' of typing judgement. Fragments that require new forms of scoped contexts (e.g. symbol contexts \cite{pfpl}) or unscoped declarations cannot presently be defined. Relatedly, the language controls variable binding, so, for example, linear type systems, cannot be defined. Another limitation is that opcons cannot directly invoke one another (e.g. a \opname{len} opcon on regular strings could not construct a natural number). We conjecture that these are not fundamental limitations and expect @$\lambda$ to serve as a minimal foundation for future efforts that increase  expressiveness while maintaining the strong guarantees, like type safety and conservativity, established  here. %(tycons could define new contexts which are threaded opaquely through ``outside'' operations). 



%The most immediate direction for future work is to finishing mechanizing the metatheory for this  technique, and to embed it into an existing proof assistant (using a continuation passing style to simulate the stores in our normalization semantics). Despite this, we believe that the technique as described semi-formally above is compelling. Adding support for tycon-specific typing contexts, implicit coercions and direct opcon calls can all be taken up by the community. Delving further into the question of when can two tycons with the same signature be substituted for one another, using a technique based on admissible relations, is also an avenue we wish to explore \cite{pfpl}.

%We combine several interesting type theoretic techniques, applying them to novel ends: 1)  a bidirectional type system permits flexible reuse of a fixed syntax; 2) the SL serves both as an extension language and as the type-level language; we give it its own statics (i.e. a \emph{kind system}); 3) we use a typed intermediate language and leverage corresponding \emph{typed compilation} techniques, here lifted into the semantics of the EL; 4) we leverage internal type abstraction implicitly as an effect during normalization of the SL to enforce abstraction barriers between type constructors. 
%As a result, conservativity follows from the same elegant parametricity results that underly  abstraction theorems for module systems. 
%Like modules, reasoning about these \emph{modular type constructors} does not require  mechanized specifications or proofs: correctness issues in the type constructor logic necessarily causes typechecking to fail, so even extensions that are not proven correct can be distributed and ``tested'' in the wild without compromising the integrity of an entire program (at worst, only values of types constructed by the tycon being tested may exhibit undesirable properties). 

%\acks
%The author is grateful to Jonathan Aldrich, Robert Bocchino and anonymous referees for their useful suggestions. This work was funded by the DOE Computational Science Graduate Fellowship under grant number DE-FG02-97ER25308.
%Acknowledgments, if needed.

\newpage
% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrv}

% The bibliography should be embedded for final submission.

\bibliography{../research}
%\softraggedright
%P. Q. Smith, and X. Y. Jones. ...reference text...

\end{document}
