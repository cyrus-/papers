%\documentclass[12pt]{article}
\documentclass[10pt,preprint]{sigplanconf}
\newcommand{\lamAce}{\lambda_{\text{Ace}}}
% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.
\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
\usepackage{mathpartir}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\usepackage{ stmaryrd }
\usepackage{verbatimbox}
\input{../att-icfp14/macros-atlam}
\usepackage{alltt}
\renewcommand{\ttdefault}{txtt}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{light-gray}{gray}{0.5}
\usepackage{listings}
\usepackage{wasysym}
    \makeatletter
%
% \btIfInRange{number}{range list}{TRUE}{FALSE}
%
% Test if int number <number> is element of a (comma separated) list of ranges
% (such as: {1,3-5,7,10-12,14}) and processes <TRUE> or <FALSE> respectively
%
        \newcount\bt@rangea
        \newcount\bt@rangeb

        \newcommand\btIfInRange[2]{%
            \global\let\bt@inrange\@secondoftwo%
            \edef\bt@rangelist{#2}%
            \foreach \range in \bt@rangelist {%
                \afterassignment\bt@getrangeb%
                \bt@rangea=0\range\relax%
                \pgfmathtruncatemacro\result{ ( #1 >= \bt@rangea) && (#1 <= \bt@rangeb) }%
                \ifnum\result=1\relax%
                    \breakforeach%
                    \global\let\bt@inrange\@firstoftwo%
                \fi%
            }%
            \bt@inrange%
        }

        \newcommand\bt@getrangeb{%
            \@ifnextchar\relax%
            {\bt@rangeb=\bt@rangea}%
            {\@getrangeb}%
        }

        \def\@getrangeb-#1\relax{%
            \ifx\relax#1\relax%
                \bt@rangeb=100000%   \maxdimen is too large for pgfmath
            \else%
                \bt@rangeb=#1\relax%
            \fi%
        }

%
% \btLstHL{range list}
%
        \newcommand{\btLstHL}[1]{%
            \btIfInRange{\value{lstnumber}}{#1}%
            {\color{black!10}}%
            {\def\lst@linebgrd}%
        }%

%
% \btInputEmph[listing options]{range list}{file name}
%
        \newcommand{\btLstInputEmph}[3][\empty]{%
            \lstset{%
                linebackgroundcolor=\btLstHL{#2}%
                \lstinputlisting{#3}%
            }% \only
        }

% Patch line number key to call line background macro
        \lst@Key{numbers}{none}{%
            \def\lst@PlaceNumber{\lst@linebgrd}%
            \lstKV@SwitchCases{#1}{%
                none&\\%
                left&\def\lst@PlaceNumber{\llap{\normalfont
                \lst@numberstyle{\thelstnumber}\kern\lst@numbersep}\lst@linebgrd}\\%
                right&\def\lst@PlaceNumber{\rlap{\normalfont
                \kern\linewidth \kern\lst@numbersep
                \lst@numberstyle{\thelstnumber}}\lst@linebgrd}%
            }{%
                \PackageError{Listings}{Numbers #1 unknown}\@ehc%
            }%
        }

% New keys
        \lst@Key{linebackgroundcolor}{}{%
            \def\lst@linebgrdcolor{#1}%
        }
        \lst@Key{linebackgroundsep}{0pt}{%
            \def\lst@linebgrdsep{#1}%
        }
        \lst@Key{linebackgroundwidth}{\linewidth}{%
            \def\lst@linebgrdwidth{#1}%
        }
        \lst@Key{linebackgroundheight}{\ht\strutbox}{%
            \def\lst@linebgrdheight{#1}%
        }
        \lst@Key{linebackgrounddepth}{\dp\strutbox}{%
            \def\lst@linebgrddepth{#1}%
        }
        \lst@Key{linebackgroundcmd}{\color@block}{%
            \def\lst@linebgrdcmd{#1}%
        }

% Line Background macro
        \newcommand{\lst@linebgrd}{%
            \ifx\lst@linebgrdcolor\empty\else
                \rlap{%
                    \lst@basicstyle
                    \color{-.}% By default use the opposite (`-`) of the current color (`.`) as background
                    \lst@linebgrdcolor{%
                        \kern-\dimexpr\lst@linebgrdsep\relax%
                        \lst@linebgrdcmd{\lst@linebgrdwidth}{\lst@linebgrdheight}{\lst@linebgrddepth}%
                    }%
                }%
            \fi
        }

 % Heather-added packages for the fancy table
 \usepackage{longtable}
 \usepackage{booktabs}
 \usepackage{pdflscape}
 \usepackage{colortbl}%
 \newcommand{\myrowcolour}{\rowcolor[gray]{0.925}}
 \usepackage{wasysym}
 
    \makeatother

\lstset{
  language=Python,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=4,
  commentstyle=\itshape\color{light-gray},
  basicstyle=\ttfamily\scriptsize,
  morekeywords={lambda, self, assert, as, cls},
  numbers=left,
  numberstyle=\scriptsize\color{light-gray}\textsf,
  xleftmargin=2em,
  stringstyle=\color{mauve}
}
\lstdefinestyle{Bash}{
    language={}, 
    numbers=left,
    numberstyle=\scriptsize\color{light-gray}\textsf,
    moredelim=**[is][\color{blue}\bf\ttfamily]{`}{`},
}
\lstdefinestyle{OpenCL}{
	language=C++,
	morekeywords={kernel, __kernel, global, __global, size_t, get_global_id, sin, printf, int2}
}

\usepackage{float}
\floatstyle{ruled}
\newfloat{codelisting}{tp}{lop}
\floatname{codelisting}{Listing}
\setlength{\floatsep}{10pt}
\setlength{\textfloatsep}{10pt}


\usepackage{url}

\usepackage{todonotes}

\usepackage{placeins}

\usepackage{textpos}

\renewcommand\topfraction{0.85}
\renewcommand\bottomfraction{0.85}
\renewcommand\textfraction{0.1}
\renewcommand\floatpagefraction{0.85}

\begin{document}

\conferenceinfo{-}{-} 
\copyrightyear{-} 
\copyrightdata{[to be supplied]} 

%\titlebanner{{\tt \textcolor{Red}{{\small Under Review -- distribute within CMU only.}}}}        % These are ignored unless
%\preprintfooter{Distribute within CMU only.}   % 'preprint' option specified.

\newcommand{\Ace}{\textsf{Ace}}

\title{\Ace: Active Typechecking and Translation Inside a Python}

\authorinfo{~}{~}{~}
%\authorinfo{Cyrus Omar\and Nathan Fulton\and Jonathan Aldrich}
 %          {School of Computer Science\\
  %          Carnegie Mellon University}
   %        {\{comar, nfulton, aldrich\}@cs.cmu.edu}   

\maketitle
\begin{abstract}
%Evidence suggests that programmers are reluctant to adopt new languages to
%gain access to new abstractions, even when they agree that these abstractions
%could be valuable. This suggests a need for languages that satisfy two key
%growth criteria: compatibility with existing programming ecosystems; and
%internal extensibility, so that new abstractions do not require new
%languages.
%
%We introduce Ace, a language compatible with the popular Python ecosystem.
%While Python, like other similar languages, is satisfactory for simple
%scripting, Ace is designed for more complex situations where static
%typechecking is beneficial. Unlike most statically-typed languages, Ace's type
%system can be extended from within, by associating compile-time functions with
%type definitions. The compiler selectively invokes these according to a
%type-directed protocol, avoiding interference problems. Despite these design
%constraints, active types in Ace can express the static and dynamic semantics
%of a range of functional, object-oriented, parallel and domain-specific
%abstractions, all within libraries.
Programmers are justifiably reluctant to adopt new language dialects to access stronger type systems. This suggests a need for a language that is \emph{compatible} with existing libraries, tools and infrastructure and that has an \emph{internally extensible type system}, so that adopting and combining type systems requires only importing libraries in the usual way, without the possibility of link-time ambiguities or safety issues. 

We introduce Ace, an extensible statically typed language embedded within and compatible with Python, a widely-adopted dynamically typed language. Python serves as Ace's type-level language. Rather than building in a particular set of type constructors, Ace introduces a novel extension mechanism, \emph{active typechecking and translation}, organized around a bidirectional type system that inverts control over typechecking and translation to user-defined type constructors according to a protocol that cannot cause ambiguities and type safety issues at link-time. In addition to describing a full-scale language design, we give a simplified calculus that describes the foundations of this mechanism and show that, as implemented in Ace, it is flexible enough to admit practical library-based implementations of types drawn from functional, object-oriented, parallel and domain-specific languages. 
%We also show how to construct safe, natural and extensible foreign function interfaces using a novel form of ``staged'' type propagation from Python into Ace and then through to OpenCL. % can be used to enable type system extensions and demonstrates how type safety can be maintained by lifting a technique from the typed compilation literature into the language.%To show that the method scales, we give a full implementation of the OpenCL programming language as a library. %We briefly examine the type theoretic foundations of this approach and give modular criteria that guarantee that an extension is safe and modular. %Ace can be used from the shell or interactively from within Python\todo{take this sentence out?}.
%Researchers developing languages and abstractions for high-performance computing must consider a number of design criteria, including performance, verifiability, portability and ease-of-use. Despite the deficiencies of legacy tools and the availability of seemingly superior options, end-users have been reluctant to adopt new language-based abstractions. We argue that this can be largely attributed to a failure to consider three additional criteria: continuity, extensibility\- and interoperability. This paper introduces Ace, a language that aims to satisfy this more comprehensive set of design criteria. To do so, Ace introduces several novel compile-time mechanisms, makes principled design choices, and builds upon existing standards in HPC, particularly Python and OpenCL. OpenCL support, rather than being built into the language, is implemented atop an extensibility mechanism that also admits abstractions drawn from other seemingly disparate paradigms. The core innovation underlying this and other features of Ace is a novel reification of types as first-class objects at compile-time, representing a refinement to the concept of active libraries that we call \emph{active types}. We validate our overall design by considering a case study of a simulation framework enabling the modular specification and efficient execution of ensembles of neural simulations across a cluster of GPUs.
\end{abstract}

\category{D.3.2}{Programming Languages}{Language Classifications}[Extensible Languages]
%\category{D.3.4}{Programming Languages}{Processors}[Compilers]
%\category{F.3.1}{Logics \& Meanings of Programs}{Specifying and Verifying and Reasoning about Programs}[Specification Techniques]
%\keywords
%type-level computation, typed compilation
\section{Introduction}\label{intro}
Asking programmers to import a new library is simpler than asking them to adopt a new programming language. Indeed, recent empirical studies underscore the difficulties of driving languages into adoption, finding that extrinsic factors like compatibility with large existing code bases,  library support, team familiarity and tool support are at least as important as intrinsic features of the language  \cite{Meyerovich:2013:EAP:2509136.2509515,chen05,nguyen2010survey}. 

Unfortunately, researchers and domain experts aiming to provide potentially useful new abstractions can sometimes find it difficult to implement them as libraries, particularly when they require strengthening a language's type system. In these situations, abstraction providers often develop a new language or language dialect. Unfortunately, this \emph{language-oriented approach} \cite{journals/stp/Ward94} does not scale to large applications: using  components written in languages with different type systems can be awkward and lead to safety problems and performance overhead at interface boundaries. %To avoid this, it is necessary to combine the different languages and their associated tools, but this often requires significant effort and expertise (we will discuss this further in Sec. \ref{related-work}). 

This is a problem even when a dialect introduces only a small number of new constructs. For example, a recent  study \cite{cave2010comparing} comparing a Java dialect, Habanero-Java (HJ), with a comparable library, \verb|java.util.concurrent|, found that the language-based abstractions in HJ were easier to use and provided useful static guarantees. Nevertheless, it concluded that the library-based abstractions remained more practical outside the classroom because HJ, as a distinct dialect of Java with its own type system, would be difficult to use in settings where some developers had not adopted it, but needed to interface with code that had adopted it. It would also be difficult to use it in combination with other abstractions also implemented as dialects of Java. Moreover, its tool support is more limited. 
This suggests that today, programmers and development teams cannot use  the abstractions they might prefer because they are only available bundled with languages they cannot adopt \cite{Meyerovich:2012:SDR:2414721.2414724,Meyerovich:2013:EAP:2509136.2509515}. 

 %This issue was perhaps most succinctly expressed by a participant in a recent study by Basili et al. \cite{basili2008understanding} who stated ``I hate MPI, I hate C++. [But] if I had to choose again, I would probably choose the same.''
%Unfortunately, researchers and domain experts (collectively, \emph{providers}) who design potentially useful new abstractions often find it difficult to implement them in terms of the general-purpose abstraction mechanisms available in existing languages. 
%Using a foreign construct can be  unnatural and, if the languages have  different type systems, can also cause safety problems. 


Internally extensible languages promise to reduce the need for new dialects by giving abstraction providers more direct control over a language's syntax and static semantics from within libraries. %In   an internally extensible language, programmers  can \emph{granularly} import the primitive constructs best suited to each component of their application or library. Providers can thus more easily develop, deploy and evaluate new abstractions in the context of existing codebases, narrowing one of the gaps between research and practice \cite{basili2008understanding}. 
%Indeed, abstractions could compete on their individual merit, rather than  by the serious adoption barriers mentioned above.
Unfortunately, the mechanisms available today have several problems. First, they are themselves often available only within a dialect of an existing language and thus face a ``chicken-and-egg'' problem: a language like SugarJ \cite{erdweg2011sugarj} must overcome the same extrinsic issues as a language like HJ. Second, giving abstraction providers too much control over the language (by introducing an over-zealous ``solution'' to the \emph{expression problem}) can introduce type safety issues and ambiguities that only become apparent when extensions are combined, as we will discuss. Too little control, on the other hand, leaves it difficult to implement real-world abstractions.%It is difficult to guarantee that extensions are sound in isolation and more so to guarantee that they can be {safely} combined. %This requires further research before it would be appropriate to widely rely on them (which we will discuss as we go on).

In this paper, we describe an extensible statically-typed language, Ace, that is implemented entirely as a library within Python and flexibly repurposes its syntax to enable, for example, type annotations. This avoids the ``chicken-and-egg problem'' and occupies what we consider a ``sweet spot'' in a design space that includes the extrinsic criteria described above. We justify this by showing a variety of non-trivial type system fragments that can be embedded as composable libraries within Ace (contribution 1, Sec. \ref{usage}). 

Ace, together with a core lambda calculus, serves also as a vehicle for a more general contribution: a novel  underlying extension mechanism, which we call \emph{active typechecking and translation}. As suggested by the design of Ace, it sidestep the expression problem by leaving the forms of expressions fixed. Instead, abstraction providers extend the language by introducing new type constructors and operators using type-level functions. We show how, by  structuring the language as a bidirectional type system and integrating techniques from the typed compilation literature directly into the language, we can maintain expressiveness while  guaranteeing that link-time ambiguities cannot occur and type safety is maintained (contribution 2, Secs. \ref{att} and \ref{theory}).





%Ace can be used from the shell, producing source files that can be further compiled or executed by external means. Ace functions targeting a language with Python bindings (e.g. Python itself, as well as the others just described) can also be compiled and invoked interactively from Python. 
%Ace supports a form of staged \emph{ad hoc} polymorphism (based on singleton types) wherein the ``dynamic type'' of a Python value can determine a static type and trigger active typechecking and translation ``just-in-time'' when Python code calls a generic Ace function. We show how this can significantly streamline interactive workflows (e.g. in scientific computing) that use Python for orchestration and exploratory tasks (e.g. plotting) but call into a statically-typed  language via a foreign-function interface (FFI) for performance- and safety-critical portions. We provide a complete implementation of the OpenCL type system (a language for low-level data parallel programming on GPUs), show several higher-level extensions to it, and discuss bindings that bridge between the type systems of Ace, OpenCL and Python's standard numerics package, \verb|numpy|, all as a library (contribution 3, Sec. \ref{targets}).%We will discuss in detail as our primary realistic case study. 

%The remainder of the paper is organized as follows: in Sec. \ref{usage}, we describe the basic structure and usage of Ace with an example that uses several interesting Ace type libraries. In Sec. \ref{att}, we detail how active typing and translation delegate control, at compile-time, to active types and bases, and discuss how this represents a form of extensibility that side-steps the expression problem. In Sec. \ref{theory}, we reduce this mechanism to  a core lambda calculus, $\lamAce$,  clarify the relationship to   type-level computation and show how type safety can be maintained. In Sec. \ref{targets}, we discuss generic functions, FFIs and staging via a complete implementation of the OpenCL type system (and extensions to it) as a library.  

In Sec.  \ref{related}, we place Ace in the context of related work on language extensibility, term rewriting, staged  compilation, bidirectional typechecking, type-level computation and typed compilation. We conclude in Sec. \ref{discussion} by summarizing the key features needed by a host language to support active typechecking and translation, and discussing present limitations and potential future work. %The appendix has details omitted for concision as well as several more examples, including functional datatypes, flexible complex numbers and a previously little-explored type system for tracking string invariants using regular expressions statically (which we show being used in Listing \ref{example}, amongst  other examples). 

\section{Language Design and Usage}\label{usage}


%
Listing \ref{example} shows that the top-level of every Ace file is a \emph{compilation script} written directly in Python. Ace requires no modifications to the language (version 2.6+ or 3.0+) or features specific to the primary implementation, CPython (so Ace supports alternative implementations like Jython and PyPy). This choice pays immediate dividends on the first five lines: Ace's import mechanism is Python's import mechanism, so Python's build tools (e.g. \verb|pip|) and package repostories (e.g. \verb|PyPI|) are directly available for distributing Ace-based libraries, including those defining type systems. %Ace can be installed with a single command: \emph{omitted for review}. %Ace is agnostic about the particular design choices that we made in our examples.

%The top-level statements in an Ace file, like the \verb|print| statement on line 10, are executed to control the compile-time behavior, rather than the run-time behavior, of the program. %That is, Python serves as the \emph{compile-time metalanguage} (and, as we will see shortly, the \emph{type-level language}) of Ace. %For readers familiar with C/C++, Python can be thought of as serving a role similar to (but more general than) its preprocessor and template system (as we will see).

\subsection{Types}
\begin{codelisting}
\begin{lstlisting}
from examples.py import py, string
from examples.fp import record
from examples.oo import proto
from examples.num import decimal
from examples.regex import string_in

print "Hello, compile-time world!"

A = record['amount' : decimal[2]]
C = record[
  'name' : string, 
  'account_num' : string_in[r'\d{10}'],
  'routing_num' : string_in[r'\d{2}-\d{4}/\d{4}']]
Transfer = proto[A, C]

@py
def log_transfer(t):
  """Logs a transfer to the console."""
  {t : Transfer}
  print "Transferring %s to %s." % (
    [string](t.amount), t.name)

@py
def __toplevel__():
  print "Hello, run-time world!"
  common = {name: "Annie Ace", 
            account_num: "0000000001", 
            routing_num: "00-0000/0001"} (C)
  t1 = ({amount: 5.50}, common) (Transfer)
  t2 = ({amount: 15.00}, common) (Transfer)
  log_transfer(t1)
  log_transfer(t2)

print "Goodbye, compile-time world!"
\end{lstlisting}
\caption{[\texttt{listing\ref{example}.py}] An Ace compilation script.}
\label{example}
\end{codelisting}

Types are constructed programmatically during execution of the compilation script. This stands in contrast to many  contemporary statically-typed languages, where types (e.g. datatypes, classes, structs) can only be declared. Put another way, Ace supports \emph{type-level computation} and Python is its type-level language (discussed further in Sec. \ref{theory} and Sec. \ref{related}). 
%The compilation script can assign shorter names to types for convenience (removing the need for  facilities like \verb|typedef| in C or \verb|type| in Haskell). 
In our example, we see several types being constructed:
\begin{enumerate}
\item On line 9, we construct a functional record type with a single (immutable) field named \verb|amount| with type \verb|decimal[2]|, which classifies decimal numbers with two decimal places. \verb|record| and \verb|decimal| are \emph{indexed type constructors}. We will provide more details in Sec. \ref{att}, but note that syntactically, \verb|record| overloads subscripting and borrows Python's syntax for array slices (e.g. \verb|a[min:max]|) to approximate conventional functional notation for type annotations. Note also that the field name could equivalently have been written \lstinline{'a' + 'mount'}, again emphasizing that types and indices are constructed programatically by a Python script.
\item On lines 10-13, we construct another record type. The field \verb|name| has type \verb|string|, defined in \verb|examples.py|, while the fields \verb|account_num| and \verb|routing_num| have more interesting types, classifying strings guaranteed statically to be in a regular language specified statically by a  regular expression pattern (written, to avoid needing to escape backslashes, using Python's \emph{raw string literals}). The appendix will give the typechecking rules, based on \cite{fulton-thesis}. To our knowledge, this type system has not previously been implemented. We include it to emphasize that we aim to support specialized type systems,  not just general purpose constructs like records and strings.
\item 
%Prototypic inheritance  is a feature of some dynamically-typed languages (most notably, Self \cite{Ungar:Smith:oopsla:1987} and Javascript). 
On line 14, we construct a simple \emph{prototypic object type} \cite{Lie86}. The type \verb|Transfer| classifies terms consisting of a \emph{fore} of type \verb|A| and a \emph{prototype} of type \verb|C|. If a field cannot be found in the fore, the type system will delegate (here, statically) to the prototype. This makes it easy to share the values of the fields of a common prototype amongst many fores, here to allow us to describe multiple transfers differing only in amount.
\end{enumerate}
\subsection{Typed Functions}

\emph{Typed functions} implement the run-time behavior and are distinguished by the presence of a decorator specifying their  \emph{base semantics} (here \verb|py| from \verb|examples.py|; Sec. \ref{att}). These functions are, however, still written using Python's syntax, a choice that is again valuable for extrinsic reasons: users of Ace can use a variety of  tools designed to work with Python source code  without  modification, including code highlighters (like the one used in generating this paper), editor plugins, style checkers and documentation generators. We will see several examples of how, with a bit of cleverness, Python's syntax can be repurposed within these functions to support a variety of static and dynamic semantics that differ from Python's. Ace  leverages the \verb|inspect| and \verb|ast| modules in the Python standard library to extract abstract syntax trees for functions annotated in this way \cite{python}. 

On lines 16-21, we see the function \verb|log_transfer|. We follow Python conventions by starting with a documentation string  that informally specifies the behavior of the function. Before moving into the body, however, we  also write a \emph{type signature} (line 17) stating that the type of \verb|t| is \verb|Transfer|, the prototypic object type described above. As a result, we can assume on line 19 that \verb|t.amount| and \verb|t.name| have types \verb|decimal[2]| and \verb|string|, respectively. We will return to the details of \verb|print| and the form \verb|[string](t.amount)|, which performs an explicit conversion, in Sec. \ref{att}.

Line 19 repurposes Python's syntax for dictionary literals to approximate  conventional notation for type annotations.
%\footnote{In the rare situation where there are no type annotations and the first statement is a dictionary literal, users can use Python's \texttt{pass} keyword.} % (and following conventions for documentation strings, which also describe ``types'' at the end). % The ``keyword'' \verb|type| was chosen because it is the name of a built-in function in Python (and thus would not be used in the way shown for any other purpose).
 {In version 3.0 of Python, syntax for annotating arguments with arbitrary values directly was introduced \cite{pep3107}:
% \vspace{-3px}
\begin{lstlisting}[numbers=none]
def log_transfer(t : Transfer): 
  print ...
\end{lstlisting}
\vspace{-5px}
These annotations were initially intended to serve only as documentation (suggesting that users of dynamically typed languages see potential in a typing discipline, if not a static one), but they were also made available as metadata for use by unspecified  future libraries. Ace supports both notations when the compilation script is run using Python 3.\textit{x}, but we use the more universally available notation here in view of our extrinsic goals: the Python 2.\text{x} series  remains the most widely used by a large margin as of this writing.% (though we consider cross-compilation in Sec. \ref{targets}).

\subsection{Bidirectional Typechecking of Introductory Forms}
On lines 23-32, we define the function \verb|__toplevel__| (the base will consider this a special name for the purposes of compilation, discussed in Sec. \ref{compilation}). After printing a run-time greeting, we introduce a value of type \verb|C| on lines 26-28. Recall that \verb|C| is a record type, so we provide the names of the fields (here, without quotes because we are no longer in the type-level language) and values of the appropriate type using the syntactic form normally used for dictionary literals. We specify which record type the literal should be {analyzed} against by giving a \emph{literal ascription}, \verb|(C)|. This again repurposes existing  syntax: here function application. When the ``function'' is of literal form (dictionaries, tuples, lists, strings, numbers, booleans and  \verb|None|) and the argument is a type, we will treat it instead an \emph{ascribed introductory form} of that type. We see another such form used with Python's tuple syntax on lines 29-30 to introduce terms of type \verb|Transfer| by providing the fore and prototype.

%\subsection{Bidirectional Typechecking}
The string, number and dictionary literal forms inside the outermost forms on lines 26-30 are also  introductory, but do not have an ascription. This is because the outermost ascriptions completely determine which types they will need to have. As we will discuss in more detail shortly, Ace is built around a \emph{bidirectional type system} that distinguishes locations where an expression must \emph{synthesize} a type (e.g. to the right of bindings) from those where it can be \emph{analyzed} against a known type (e.g. as an argument to a function with known type) \cite{Lovas08abidirectional}. Unascribed literal forms must ultimately be analyzed against a type. 

If an unascribed literal is used in a synthetic location, the base can, however, specify a ``default ascription''. For example, if we had not specified the literal ascription on line 26, the base would cause the dictionary literal to be analyzed against the type \verb|dyn|, covering dynamically classified Python values. This would still lead to a type error because keys are then treated as expressions, as in Python, rather than field names, and the identifiers shown have not been bound in the top-level context (emphasizing that ``dynamically-typed languages'' can still have a static semantics, even if it only involves checking that top-level variables are bound).% When considered as a record, the keys were considered in a different class by the implementation (labels, rather than variables).

An ascription can also be a {type constructor} instead of a type. For example, we might write \verb|[1, 2] (matrix)| instead of \verb|[1, 2] (matrix[i32])| when using a base for which the default ascription for number literals is \verb|i32|. Because the arguments synthesize the appropriate type, the type constructor can synthesize an appropriate index based on the types of the subexpressions. If we wanted a matrix of \verb|dyn|s, then we would have to write either \verb|[1, 2] (matrix[f32])|, ascribe each inner literal so that it synthesizes the \verb|f32| type, \verb|[1(f32), 2(f32)] (matrix)|, or construct a base with a different default ascription. 

We will see how bidirectional typechecking  works in greater technical detail in Secs. \ref{att} and \ref{theory}, but note here that having the semantics of a form change depending on the type it is being analyzed against is a key to achieving expressiveness given our constraint that we cannot add  new forms to the language, for both extrinsic and intrinsic reasons.

\subsection{External Compilation}\label{compilation} 
\begin{codelisting}
\begin{lstlisting}[style=Bash]
$ `acec listing1.py`
Hello, compile-time world!
Goodbye, compile-time world!
[acec] _listing1.py successfully generated.
$ `python _listing1.py`
Hello, run-time world!
Transferring 5.50 to Annie Ace.
Transferring 15.00 to Annie Ace.
\end{lstlisting}
\caption{Compiling \texttt{listing\ref{example}.py} using \texttt{acec}. Both steps can be performed at once by writing \lstinline[style=Bash]{`ace listing1.py`} (line 3 will not be printed with this command).}
\label{external-compilation}
\end{codelisting}
\begin{codelisting}[t]
\begin{lstlisting}
import examples.num.runtime as __ace_0

def log_transfer(t):
  print ("Transferring %s to %s." % (
    __ace_0.decimal_to_str(t[0],2), t[1][0]))

print "Hello, run-time world!"
common = ("Annie Ace", "0000000001", "00-0000/0000")
t1 = ((5,50), common)
t2 = ((15,0), common)
log_transfer(t1)
log_transfer(t2)
\end{lstlisting}
\caption{[\texttt{\_listing\ref{example}.py}] The file generated in Listing \ref{external-compilation}.}
\label{example-out}
\end{codelisting}

To typecheck, translate and execute the code in Listing \ref{example}, we have two choices: do so externally at the shell, or perform compilation interactively (or implicitly) from within Python. We will begin with the former. Interactive and implicit compilation are implemented but we do lack space to detail it in the present paper, choosing here to focus on the core mechanism.% with our OpenCL examples. 

Listing \ref{external-compilation} shows how to invoke the \verb|acec| compiler at the shell to typecheck and translate \texttt{listing\ref{example}.py}, resulting in a file named \verb|_listing1.py|. This is then sent to the  Python interpreter for execution. Note that the {\texttt{print}} statements at the top-level of the  compilation script were evaluated during compilation only. These two steps can be combined by running \verb|ace listing1.py| (the intermediate file is not generated unless explicitly requested in this case).

The Ace compiler is itself a Python library, and \verb|acec| is a simple Python script that invokes it, operating in two steps:
\begin{enumerate}
\item It evaluates the compilation script to completion.
\item For any top-level bindings that are Ace functions in the final environment (instances of \verb|ace.TypedFn|, as we will discuss), it initiates active type-checking and translation (Sec. \ref{att}). If no type errors are discovered, the translations are collected (obeying order dependencies) and emitted, with file extension(s) determined by the target(s) in use, discussed further in Sec. \ref{att}. Here our target is  the default target associated with the base \verb|py|, which emits Python 2.6 files. If a type error is discovered, no file is emitted and the error is displayed on the console.
\end{enumerate}

In our example, there are no type errors, so the file \verb|_listing1.py|, shown in Listing \ref{example-out},  is generated. This file is meant only to be executed. The invariants necessary to ensure that execution does not ``go wrong'' were checked statically and entities having no bearing on execution, like field names and types themselves, were erased. Notice that:
\begin{enumerate}
\item The base recognized the function name \verb|__toplevel__| as special, placing the translation of its body  at the top level of the file. Ace does not have any special function names itself; this is a feature of the user-defined base.
\item Records with two or more fields translated to tuples of their values (e.g. \verb|common| on line 8). If there was only a single field, like the terms of type \verb|A| inside \verb|t1| and \verb|t2|, the value was passed around unadorned. 
\item Decimals translated to pairs of integers. Conversion to a string happened via a helper function defined in a ``runtime'' package imported with an internal name, \verb|__ace_0|, to avoid naming conflicts. 
\item Terms of type \verb|string_in[r"..."]| translated to strings. Checks for membership in the specified regular language were performed entirely statically by the type system.\footnote{Note that there are situations (e.g. if a string is read in from the console) that would necessitate an initial run-time check, but no further checks in downstream functions would be necessary. See the appendix for details.}
\item Prototypic objects are represented as pairs consisting of the fore and the prototype. Dispatch to the appropriate record based on the field name is static (line 5).
\end{enumerate}

% As a result, execution is essentially as fast as it could be given the target language chosen (Python). We will target an even faster language, \verb|OpenCL|, in Sec. \ref{targets}.

%Because \verb|t| will be statically guaranteed to have type \verb|Transfer|,  
\subsection{Type Errors}
\begin{codelisting}[t]
\begin{lstlisting}
from listing1 import py, A, C, log_transfer
from datetime import date
@py
def pay_oopsie(a):
  {a : A}
  print "Checking date..."
  if date.today().day == 1:
    common = {nome: "Oopsie Daisy",
              account_num: None,
              routing_num: "0-0000-0002"} (C)
    log_transfer((common, a))
    a.amount += 1000 
print str(date.today().day)
\end{lstlisting}
\caption{[\texttt{listing\ref{oops}.py}] Lines 9-13 each have type errors.}
\label{oops}
\end{codelisting}
\begin{codelisting}
\begin{lstlisting}[style=Bash]
$ `acec listing4.py`
2
[acec] TypeError in listing4.py (line 8, col 15): 
  [record] Invalid field name: nome
           Expected: name, account_num, routing_num
\end{lstlisting}
\caption{Compiling \texttt{listing\ref{oops}.py} using \texttt{acec} catches the errors statically (compilation stops at first error).}
\label{oops-compilation}
\end{codelisting}Listing \ref{oops} shows an example of code containing several type errors. Indeed, lines 8-12 each contain a type error. If analagous code were written in Python itself, these could only be found if the code was executed on the first day of the month (and, depending on the implementation, not all of the issues would  immediately result in run-time exceptions, possibly leading to quite subtle problems; for example, the unexpected use of \verb|None|\footnote{The notation \texttt{+T} constructs a \texttt{None}-able option type; see appendix.}). Static type checking allows us to find these errors during compilation. Listing \ref{oops-compilation} shows the result of attempting to compile this code. The compilation script completes (so that functions can refer to each other mutually recursively), then the typed functions in the top-level environment are typechecked. The typechecker raises an exception statically at the first error, and \verb|acec| prints it to the console as shown.




\section{Active Typechecking and Translation}\label{att}

%With monolithic languages, enabling a variety of tools is relatively straightforward. For example, one might declare functional datatypes with cases for every type constructor and term form, and then proceed by  exhaustive case analysis over term forms to implement this sort of functionality. The external visitor pattern for class-based object systems operates in essentially the same way. In both cases, the problem is that the types and terms are defined in one place, so  providers cannot extend the language modularly.

Enabling the addition of new forms to a language in an open tool ecosystem is difficult. Indeed, it is the canonical example of the long-studied \emph{expression problem} \cite{wadler1998expression}. Although a number of approaches have been developed, we argue that many of them are overly permissive, making it difficult to reason compositionally about metatheoretic issues (e.g. type safety) and avoid ambiguities when extensions are combined (see Sec. \ref{related}). %They are also often quite complex. %Requiring that abstraction clients have the expertise to resolve these ambiguities manually is undesirable, and heading them off results in an unbounded, combinatoric explosion of situations to consider.
Ace's term forms, as we have seen, are fixed by Python's grammar, so Ace sidesteps the expression problem almost entirely (and thus inherits broad tool support, as described earlier). The key insight is that leaving the forms fixed does not mean the semantics must also be fixed. Instead of taking a syntax-directed view of extensibility, we take a type-directed view: users cannot add new forms but they can add new type constructors, which we empower with  more control over the semantics of existing forms (e.g. literals, as discussed above, but also nearly every other form, as we will now discuss) in a controlled manner.  %In Ace (but not in the theory in Sec. \ref{theory}), users can also repurpose existing forms flexibly within a delimited scope. 
The key features that characterize active typechecking and translation are: \begin{itemize}
\item a \emph{dispatch protocol} that delegates responsibility for typechecking and translating a term to:
  \begin{itemize}
  \item a type or type constructor extracted from a subterm
  \item a per-function base semantics (or simply \emph{base}) for forms for which this is not possible  (variables, literals without ascriptions and most statements)
  \end{itemize}
\item a mechanism for allowing the user to define new types, type constructors and bases from within the language (in particular, \emph{from within the type-level language}) rather than fixing them ahead of time
\end{itemize}
\begin{codelisting}
\begin{lstlisting}
from listing1 import py, A, record
@py
def example(a):
  {a : A}
  x = a.amount
  x = x + x
  return {
    y: {amount: x} (A),
    remark: "Creating an anonymous record"} (record)
\end{lstlisting}
\caption{[\texttt{listing6.py}] The example detailed in Sec. \ref{att}.}
\label{att-example}
\end{codelisting}

To see how this works, let us trace through how Ace processes the simple example in Listing \ref{att-example}. 

\subsection{Base Decorators}
Decorators in Python (line 2) are syntactic sugar: we could equivalently have omitted line 3 and inserted the top-level statement \verb|example = py(example)| on the line after the function definition. Note, however, that \verb|py| is not a function but an instance of a class, \verb|examples.py.PyBase|, that inherits from \verb|ace.Base|. It can be used as a decorator because \verb|ace.Base| overloads the call operator, shown in Listing \ref{base}. The \verb|_process| function (not shown) operates as follows:
\begin{enumerate}
\item The Python standard library is used to extract an abstract syntax tree (\verb|AST|) from the function.
\item The closure of the function, together with the globals in the module it is defined in, are extracted to reify its static environment.
\item The argument annotations are processed. If provided in the body, as in our example, \verb|_process| checks that the argument names are spelled correctly and evaluates the type-level expressions (e.g. \verb|A| above) in the static environment to produce a mapping of argument names to types, called the argument signature. If Python 3's annotations were used, this is not necessary.
%\item Whether the function is generic or not is determined.  We defer discussion of generic functions until Sec. \ref{targets}.
\end{enumerate}
By the end of Listing \ref{att-example}, \verb|example| is thus an instance of \verb|ace.TypedFn|. It could  have been constructed directly by calling the constructor on line 2 of Listing \ref{base} (this would be inconvenient, but could be useful for metaprogramming). 

\subsection{Checking Typed Functions}
When the Ace compiler begins typechecking a \verb|TypedFn| (when asked to by \verb|acec|, or when it is  invoked interactively, not discussed in this paper), it proceeds as follows:
\begin{enumerate}
\item An \verb|ace.Context| is constructed with a reference to the function (Listing \ref{context}, lines 2-3).
\item The base is asked to initialize the context. This can be seen on lines 2-4 of Listing \ref{pybase}, where the \verb|PyBase| base adds an attribute tracking local bindings (initially only the arguments) and the return type, which will be synthesized but initially is not known. 
\item For each statement in the body (after the documentation string and type annotation), the compiler delegates to either the base, or of a type synthesized from a subterm, by calling a method named \verb|check_|$F$, where $F$ is the form of the statement (derived directly from Python's \verb|ast| package \cite{python}). These methods are responsible for checking that the statement is well-typed (raising an \verb|ace.TypeError| if not) and having any needed effect on the context. 
\item The base synthesizes a type for the function as a whole via the \verb|syn_FunctionDef_outer| method. Here, an \verb|arrow| type is synthesized (lines 26-29) based on the argument signature and the synthesized return type.
\end{enumerate}
The assignment statements on lines \ref{att-example}.5\footnote{In this section, we will need to refer to code in Listings \ref{att-example}, \ref{pybase}, \ref{context} and \ref{record}, so we adopt this syntactic convention to refer to line numbers for concision.} and \ref{att-example}.6 are checked by the base method \verb|check_Assign_Name| shown on lines \ref{pybase}.6-\ref{pybase}.14 and the return statement on line \ref{att-example}.7 is checked by the base's \verb|check_Return| on lines \ref{pybase}.19-\ref{pybase}.23. Both work similarly: if this is the first assignment to a particular name, or the first return statement seen, then the value must be able to synthesize a type in the current context. Otherwise, it is analyzed against the previously synthesized type.

\subsection{Bidirectional Typechecking}
\begin{codelisting}
\begin{lstlisting}
class TypedFn(object):
  def __init__(self, base, ast, static_env, arg_sig):
    # ...
  # ...
class Base(object):
  def __call__(self, f):
    (ast, static_env, arg_sig) = _process(f)
    return TypedFn(self, ast, static_env, arg_sig)
  # ...
\end{lstlisting}
\caption{A portion of the \texttt{ace} core showing how a base can be used as a decorator to construct a typed function.}
\label{base}
\end{codelisting}

\begin{codelisting}
\begin{lstlisting}
class PyBase(ace.Base):
  def init_ctx(self, ctx):
    ctx.locals = dict(ctx.fn.arg_sig)
    ctx.return_t = None

  def check_Assign_Name(self, ctx, s):
    x, e = s.target.id, s.value
    if x in ctx.locals:
      ctx.ana(e, ctx.locals[x])
    else:
      ty = ctx.syn(e)
      ctx.locals[x] = ty
	  
  def trans_Assign_Name(self, ctx, target, s):
    return target.direct_translation(ctx, s)
              
  def check_Return(self, ctx, s):
    if ctx.return_t == None:
      ctx.return_t = ctx.syn(s.value)
    else:
      ctx.ana(s.value, ctx.return_t)
      
  def trans_Return(self, ctx, target, s):
    return target.direct_translation(ctx, s)
          
  def syn_FunctionDef_outer(self, ctx, f):
    if ctx.return_t == None:
      ctx.return_t = unit
    return arrow[ctx.fn.arg_sig, ctx.return_t]
    
  def trans_FunctionDef_outer(self, ctx, target, f):
    if f.name == "__toplevel__":
      return target.Suite(ctx.trans(f.body))
    else:
      return target.direct_translation(f, ctx)

  def syn_Name(self, ctx, e):
    x = e.id
    if x in ctx.locals:
      return ctx.locals[x]
    elif x in ctx.fn.static_env:
      return self.syn_lifted(x)
    else:
      raise ace.TypeError("...var not bound...", e)
      
  def trans_Name(self, ctx, target, e):
    if e.id in ctx.locals:
      return target.direct_translation(ctx, e)
    else:
      return self.trans_lifted(ctx, target, e)
  
  default_Dict_asc = default_Str_asc = dyn
  # ...
  def init_target(ctx): return PyTarget()   
py = PyBase()
\end{lstlisting}
\caption{A portion of the base used in our examples thus far, defined in the \texttt{examples.py} package.}
\label{pybase}
\end{codelisting}

\begin{codelisting}
\begin{lstlisting}
class Context(object):
  def __init__(self, fn):
    self.fn = fn
    
  def syn(self, e):
    if instanceof(e, ast.Name):
      delegate = self.fn.base
      ty = delegate.syn_Name(self, e)
    elif instanceof(e, ast.Attribute):
      delegate = self.syn(e.value)
      ty = delegate.syn_Attribute(self, e)
    # ... other compound forms similar (cf appendix)
    elif isinstance(e, ast.Str):
      return self.ana(self, e, 
        self.fn.base.default_Str_asc)
    # ... other unascribed literal forms similar
    elif is_ascribed_Dict(e):
      lit, delegate = get_lit(e)
      if issubclass(delegate, Type): # tycon
        ty = delegate.syn_Dict(self, lit)
      else:
        return self.ana_Dict(lit, delegate)
    # ... other ascribed literal forms similar
    e.delegate = delegate
    e.ty = ty
    return ty
    
  def ana(self, e, ty):
    if instanceof(e, ast.Dict):
      ty.ana_Dict(self, e)
    # ... other literal forms similar
    else:
      syn = self.syn(e)
      if ty != syn:
        raise TypeError("...syn/ana mismatch...", e)
      return
    e.delegate = e.ty = ty
    
  def trans(self, target, e):
    d = e.delegate
    if instanceof(e, ast.Name): 
      trans = d.trans_Name(self, target, e)
    elif instanceof(e, ast.Attribute):
      trans = d.trans_Attribute(self, target, e)
	# ... other forms similar
	e.trans = trans
	return trans
\end{lstlisting}
\caption{The \texttt{ace.Context} class delegates typechecking and translation of expressions, depending on their form and sub-terms, to a base, a type or a type constructor.}
\label{context}
\end{codelisting}

%Active types are the primary means for extending Ace with new abstractions. An active type, as mentioned previously, is an instance of a class implementing the \verb|ace.Type| interface. Listing \ref{record} shows an example of such a class: the \verb|clx.Cplx| class used in Listing \ref{compscript}, which implements the logic of complex numbers. The constructor takes as a parameter any numeric type in \verb|clx| (line \ref{cplx}.2). 

\begin{codelisting}
\begin{lstlisting}
@slices_to_sig
class record(ace.Type):
  def __init__(self, sig, anon=False):
    self.sig, self.anon = sig, anon
    
  @classmethod 
  def syn_Dict(cls, ctx, e):
    sig = Sig((f, ctx.syn_ty(v)) 
      for f, v in zip(e.keys, e.values))
    return cls(sig, anon=True)
    
  def ana_Dict(self, ctx, e):
    for f, v in zip(e.keys, e.values):
      if f.id in self.sig:
        ctx.ana(v, self.sig[f.id])
      else:
        raise ace.TypeError("...extra field...", f)
    if len(self.sig) != len(e.keys):
      raise ace.TypeError("...missing field...", e)
      
  def trans_Dict(self, ctx, target, e):
    if len(self.sig) == 1:
      return ctx.trans(e.values[0])
    else:
      value_dict = dict(zip(e.keys, e.values))
	  return target.Tuple(
	    ctx.trans(target, value_dict[field])
	    for field, ty in self.sig)

  def syn_Attribute(self, ctx, e): 
    if e.attr in self.sig: 
      return self.sig[e.attr]
    else:
      raise ace.TypeError("...field not found...", e)
      
  def trans_Attribute(self, ctx, target, e): 
    if len(self.sig) == 1:
      return ctx.trans(target, e)
    else:
      idx = idx_of(self.sig, e.attr)
      return target.Subscript(
        ctx.trans(target, e.value), target.Num(idx))
        
  def __eq__(self, other):
    if isinstance(other, record): 
      if self.anon or other.anon: return self is other
      else: return self.sig == other.sig

  def trans_type(self, target):
    return target.dyn
\end{lstlisting}
%
%  def trans_type(self, target):
%    if target.supports(examples.fp.EagerNAryProdLR):
%      return target.EagerNAryProdLR()
%    else if isinstance(target, examples.py.Dyn):
%      return target.Dyn()
\caption{The \texttt{examples.fp.record} type constructor.}
\label{record}
\end{codelisting}
 Synthesis and analysis are mediated by the context via its \verb|syn| and \verb|ana| methods. To see how it works, let us begin at the first statement in our example, the assignment statement on line \ref{att-example}.5. As just stated, the method \verb|check_Assign_Name| on lines \ref{pybase}.6-\ref{pybase}.14 is called. Because the locals dictionary added to the context by the base does not yet have a binding for \verb|x|, the base asks to synthesize a type for the value being assigned, \verb|a.amount|, by calling \verb|ctx.syn|.

%The novel protocol for typechecking expressions that we will now describe (and formalize in Sec. \ref{theory}) is what enables us to extend the language with new semantics modularly while sidestepping the expression problem. Let us look more closely at how lines \ref{att-example}.5-\ref{att-example}.7 are typechecked.

%\paragraph{Active Type Synthesis} 
This method is defined on lines \ref{context}.5-\ref{context}.26. The relevant case in this method is the one on line \ref{context}.9, because \verb|a.amount| is of the form \verb|Attribute| according to Python's grammar. The context delegates to the type recursively synthesized for its value, \verb|a|. We recurse back into \verb|syn|, now taking the first branch for terms of the form \verb|ast.Name|. Synthesizing a type for a name is delegated to the base by calling its \verb|syn_Name| method. We can see its implementation for the base we are using on lines \ref{pybase}.37-\ref{pybase}.44. The identifier \verb|a| is an argument to the function, which the base included in the initial locals dictionary, so we hit a base case and the type \verb|A| is synthesized.

We can now pop back up to line \ref{context}.11, which can now delegates synthesis of a type for \verb|a.amount| to the type \verb|A| via the \verb|syn_Attribute| method. Recall that \verb|A| was constructed using the \verb|record| constructor in Listing \ref{example}. The definition of this type constructor is shown in Listing \ref{record}. In Ace, type constructors are classes inheriting from \verb|ace.Type| and types are instances of such classes. Constructor indices are given through the class constructor, called \verb|__init__| in Python. Here, \verb|record| requires a signature, which is a mapping of field names to types (an instance of \verb|examples.fp.Sig|, which performs well-formedness checking, not shown). The class decorator \verb|slices_to_sig| provides the convenient notation shown used in Listing \ref{example} (by adding a metaclass to override the class object's subscript operator, not shown). The \verb|syn_Attribute| method is shown on lines \ref{record}.30-\ref{record}.34. It simply looks in the signature for the provided field name, so \verb|decimal[2]| is synthesized. We can now go back up to the assignment statement that triggered this chain of calls and see on line \ref{pybase}.14 that a binding for \verb|x| is added to the locals dictionary and checking of this statement succeeds. 

The next statement also assigns to \verb|x|, but this time, the base asks to analyze the value against \verb|A| using the \verb|ana| method of \verb|Context|, shown on lines \ref{context}.28-\ref{context}.37. Analysis differs from synthesis only for unascribed literal forms. For any other form, the context simply asks for an equal type to be synthesized (we will discuss in Sec. \ref{discussion} future work on integrating subtyping, where this would be relaxed). For concision, we leave the details of the delegation protocol for binary operators to the appendix, which also shows the definition of the \verb|decimal| type constructor. The addition of two terms of type \verb|decimal[2]| has type \verb|decimal[2]|, so checking the second assignment statement also succeeds.

The final statement in \verb|example| is a return statement. Because it is the first return statement encountered, it too requires that the returned value synthesize a type. Here it is an ascribed literal. We can see on lines \ref{context}.17-\ref{context}.22 how this is handled. Because the ascription is a type constructor, \verb|record|, rather than a type, the literal synthesizes a type by calling a \emph{class method}, \verb|record.syn_Dict|, shown on lines \ref{record}.6-\ref{record}.10. This method constructs a record signature by synthesizing a type for each value (using comprehension syntax for concision) then returns a new instance of the class. It is marked as anonymous. Anonymous records differ only in how equality is decided, shown on lines \ref{record}.44-\ref{record}.47. We want anonymous records to be equal structurally, while records declared like \verb|A| are by default distinct even if they have the same signature (following the convention of most functional languages with record declarations). 

Synthesizing a type for the value of the field \verb|y|, also an ascribed literal, takes a different path because the ascription is a type, \verb|A|, not a type constructor. Type ascriptions, as we have seen in the previous examples, cause the literal to analyzed. This proceeds through the \verb|ana_Dict| method on lines \ref{record}.12-\ref{record}.19, which analyzes the value of each field against the corresponding type in the signature, raising a type error if there are missing or extra fields. %This succeeds because \verb|x| analyzes against \verb|decimal[2]| because the base synthesizes that type, as described above. 

Synthesizing a type for the value of the field \verb|remark|, an unascribed literal form, follows a third path, seen on lines \ref{context}.13-\ref{context}.15. Unascribed literals are treated as if they had been  given the default ascription specified by the base. Our base specifies the default ascription as \verb|dyn| on line \ref{pybase}.52, which accepts our  literal, so synthesis succeeds. The return type of \verb|example| is thus equal to:
\begin{lstlisting}[numbers=none]
record(Sig(('y', A), ('remark', dyn)), anon=True)
\end{lstlisting}
\vspace{-10px}
\subsection{Active Translation}
Once typechecking is complete, the compiler enters the translation phase. The base creates an instance of a class inheriting from \verb|ace.Target| for use during this phase via the \verb|init_target| method (line \ref{pybase}.54). This object provides methods for code generation and supports features like fresh variable generation, adding imports and so on. It's interface is not constrained by Ace (we will see what Ace requires of a target below) and the mechanics of code generation are orthogonal to the  focus of this paper, so we will discuss it relatively abstractly. The simplest API would be string-based code generation. For the Python target we use here, we generate ASTs. The API is based directly on the \verb|ast| library, with a few additional conveniences just mentioned.

This phase follows the same delegation protocol as the typechecking phase. Each \verb|check_|/\verb|syn_|/\verb|ana_|$X$  method has a corresponding \verb|trans_|$X$ method. The typechecking phase saved the entity that was delegated control, along with the type assignment, as attributes of each node, \verb|delegate| and \verb|ty| respectively, so that these need not be determined again. This protocol can be seen on lines \ref{context}.39-\ref{context}.47. 
Translation methods have access to the context and node, as during typechecking, as well as the target. 

Because we are targeting Python directly, most of our \verb|trans| methods are direct translations (factored out into a helper function). The main methods of interest that are not entirely trivial are \verb|trans_FunctionDef_outer| on lines \ref{pybase}.33-\ref{pybase}-37 and the two translation methods in Listing \ref{record}, which implement the logic described in Sec. \ref{compilation}. 

Each type constructor must also specify a \verb|trans_type| method. This is important when targeting a typed language (so that the translations of type annotations can be generated, for example). As we will see in the next two sections, this is also critical to ensuring type safety. The language \emph{checks} not only that translations having a given type are well-typed, but that they have the type specified by this method (requiring that the target provide a \verb|is_of_type| method). Here, because we are simply targeting Python directly, the \verb|trans_type| method on line \ref{record}.49-\ref{record}.50 simply generates \verb|target.dyn|. 

When this phase is complete, each node processed by the context will have a translation, available via the \verb|trans| attribute. In particular, each typed function has a translation. Note that some nodes are never processed by the context because they were reinterpreted by the delegate (e.g. the field names in a record literal), so they do not have translations (as expected, given our discussion above).

To support external compilation, the target must have an \verb|emit| method that takes the compilation script's file name and a reference to a string generator (an instance of \verb|ace.util.CG|) and emits source code. The string generator we provide can track indentation levels (to support Python code generation and make generation for other languages more readable, for the purposes of debugging). It allows non-local string generation via the concept of user-defined \emph{locations}.  Each file that needs to be generated is a location and there can also be locations within a file (e.g. the imports vs. the top-level code), specified by a target the first time it finds that a necessary location is not defined. A generated entity (e.g. an import, class definition or function definition) can only be added once at a location. We saw this in Listing \ref{example-out} for the decimal-to-string conversion. The API will be discussed further in the appendix. 
%When encountering a compound term (e.g. \verb|t.amount| in Listing \ref{example}), the compiler defers control over type  and translation to the type of a designated subexpression (e.g. \verb|t|) according to a fixed fixed \emph{dispatch protocol}. Below are examples of the choices made in Ace.\todo{full in appendix?}
%\begin{itemize}
%\item Responsibility over {\bf attribute access} (\texttt{e.attr}), {\bf subscripting} (\texttt{e[e1]}) and \textbf{calls} (\verb|e(e1, ..., en)|) and {\bf unary operations} (e.g. \verb|-e|) is handed to the type recursively assigned to \texttt{e}.
%\item Responsibility over {\bf binary operations} (e.g. \verb|e1 + e2|) is first handed to the type assigned to the left operand. If it indicates that it cannot handle the operation, the type assigned to the right operand is handed responsibility. {Note that this operates like the corresponding rule in Python's \emph{dynamic} operator overloading mechanism; see Sec. \ref{related} for a discussion.}
%\item Responsibility over \textbf{constructor calls} (\verb|[t](e1, ..., en)|), where \verb|t| is a \emph{compile-time Python expression} evaluating to an active type, is handed to that type. If \verb|t| evaluates to a \emph{family} of types, like \verb|clx.Cplx|, the active type is first generated via a class method, as discussed below.
%%\item Responsibility over {\bf simple assignment statements}, is handed to the type of the variable on the left (which, as we will see below, is determined by the active base). If this type does not provide special assignment semantics, the base must handle it.%Destructuring assignment is also supported by a somewhat more complex protocol.\todo{clarify}
%\end{itemize}
%%
%%\todo{To be revised from here on down}The core of Ace consists of about 1500 lines of Python code implementing its primary concepts: generic functions, concrete functions, active types, active bases and active targets.  The latter three comprise Ace's extension mechanism. Extensions provide semantics to, and govern the compilation of, Ace functions, rather than logic in Ace's core. %Indeed, the name ``Ace'' might itself be an acronym: it is an ``active compilation environment''.

\section{@$\lambda$: Foundations of Active Typechecking and Translation}\label{theory}
\newcommand{\F}[1]{{\sf #1}~}
\newcommand{\FF}[1]{{\sf #1}}
\newcommand{\Q}{\FF{Arg}}
\renewcommand{\tnil}[1]{[]}
\newcommand{\ltxt}[1]{\ell_{\text{#1}}}
\begin{figure*}[t]
\small
$
\begin{array}{l}
\F{using} \phi_{\texttt{nat}}; \phi_{\texttt{lprod}}\\
\elet{one}{\tvar{s}\langle\tvar{z}\langle\rangle\rangle}{\\
\elet{plus}{(\lambda x.\lambda y.\tvar{natrec}~x~\langle y; \lambda p.\lambda r.\tvar{s}\langle r\rangle\rangle) : \tvar{nat} \rightharpoonup \tvar{nat} \rightharpoonup \tvar{nat}}{\\
\FF{let}~y = \{\ltxt{1}=one, \ltxt{2}=plus~one~one\} : \ttype{lprod}{(\ltxt{1}, \tvar{nat}) :: (\ltxt{2}, \tvar{nat}) :: []}}}~\FF{in}\\
y.\ell_2
\end{array}
$
\caption{\small A program that uses the natural number fragment, $\phi_\texttt{nat}$, defined in Figure \ref{nat-atfrag}, written with the syntactic sugar defined in Figure \ref{desugaring}.}
\label{nat-sugared}
\end{figure*}
\begin{figure*}[t]
\small
\hspace{-5px}
$
\begin{array}{l}
\F{using} \hat\phi_{\texttt{nat}}; \hat\phi_{\texttt{lprod}}\\
\elet{one}{\FF{intro}[\FF{in}[\ltxt{s}](\tunit)](\FF{intro}[\FF{in}[\ltxt{z}](\tunit)]()) : \ttype{nat}{\tunit}}{\\
\elet{plus}{(\lambda x.\lambda y.x\cdot\FF{elim}[()](y; \lambda p.\lambda r.\FF{intro}[\FF{in}[\ltxt{s}]()](r)) :\ttype{parr}{(\ttype{nat}{\tunit}, \ttype{parr}{(\ttype{nat}{\tunit}, \ttype{nat}{\tunit})})}}{\\
\FF{let}~y = \FF{intro}[\ell_1 :: \ell_2 :: []](one; plus\cdot\FF{elim}[()](one)\cdot\FF{elim}[()](one)) : \ttype{lprod}{(\ltxt{1}, \tvar{nat}) :: (\ltxt{2}, \tvar{nat}) :: []}}}~\FF{in}\\
y\cdot\FF{elim}[\ell_2]()
\end{array}
$
\caption{\small An equivalent program written in core @$\lambda$ without syntactic sugar and with type-level terms normalized.}
\label{nat-desugared}
\end{figure*}
\begin{figure*}
\small
$
\begin{array}{lcl}
\phi_{\texttt{nat}} & := & \F{tycon}\fvar{nat}~\F{of}\kunit~\{\delta_{\texttt{nat}}\}\\
&& \F{def}\tvar{nat}:\kTypeBlur = \ttype{nat}{\tunit};\\
&& \F{def}\tvar{z}:\Pi[\{\ltxt{idx}\hookrightarrow\kappa_{\text{nat-intro-idx}},\ltxt{ty}\hookrightarrow\kTypeBlur\}]={(\ltxt{idx}=\FF{in}[\ltxt{z}](\tunit), \ltxt{ty}=\tvar{nat})};\\
&& \F{def}\tvar{s}:\Pi[\{\ltxt{idx}\hookrightarrow\kappa_{\text{nat-intro-idx}},\ltxt{ty}\hookrightarrow\kTypeBlur\}]={(\ltxt{idx}=\FF{in}[\ltxt{s}](\tunit), \ltxt{ty}=\tvar{nat})};\\
&& \F{def}\tvar{natrec}:\kappa_{\text{nat-elim-idx}} =\tunit\\
\kappa_{\text{nat-intro-idx}} & := & \Sigma[\{l_z \hookrightarrow \kunit, l_s \hookrightarrow \kunit\}]\\
\kappa_{\text{nat-elim-idx}} & := & \kunit\\
\end{array}
$
%\vspace{-10pt}
\caption{\small The natural number fragment, including definitions used by the assisted intro and elim desugarings, defined and shown being used above.}
\label{nat-atfrag}
\end{figure*}
In this section, we will describe the foundations of \emph{active typechecking and translation} by constructing a core lambda calculus, @$\lambda$. 
A \emph{program} in @$\lambda$, $\rho$, consists of a series of fragment definitions, $\phi$, for use by an external term, $e$. An example of an @$\lambda$ program, $\rho_{\text{example}}$, that uses {fragment definitions} $\phi_{\texttt{nat}}$ and $\phi_{\texttt{lprod}}$, defining primitive natural numbers and labeled products (i.e. ordered records), is shown in Fig. \ref{nat-sugared}. In Ace,  fragment definitions are packaged separately; we do not include this in the core calculus for simplicity.

In this example, we see the introduction and elimination forms for natural numbers, functions and labeled tuples all being used. However, in the core syntax, shown (from the perspective of fragment clients) in Fig. \ref{client-syntax}, there are only two external introductory forms, $\lambda x.e$ and $\FF{intro}[\sigma](\overline{e})$, and one external elimination form, $e\cdot\FF{elim}[\sigma](\overline{e})$, where $\overline{e}$ is shorthand for zero or more semicolon-separated external terms, called the \emph{arguments} of the introduction or elimination operation. The forms used in Fig. \ref{nat-sugared} are \emph{derived forms}, defined in terms of these core forms in Fig. \ref{derived}. 

Both fragments and external terms can contain \emph{static terms}, $\sigma$, which are evaluated statically to \emph{static values}, $\hat\sigma$. The static language is itself a simply-typed lambda calculus. We call the types of static terms \emph{kinds}, $\kappa$, by convention. Static variables are written in bold font, $\tvar{x}$, to emphasize that they are distinct from variables bound by external terms, written $x$. For example, in the example in Figure \ref{nat-sugared}, $\tvar{nat}$ is a static variable bound by the fragment $\phi_{\texttt{nat}}$ to the natural number type. Types have kind $\kTypeBlur$ and are introduced by naming a \emph{type constructor}, written in small-caps, e.g. $\fvar{nat}$ and $\fvar{lprod}$, and providing an \emph{index}, which is a static term of the \emph{index kind} of the type constructor.  For example, $\fvar{nat}$ is indexed by $\kunit$ (because there is only one natural number type), so $\tvar{nat}$ is defined to be $\ttype{nat}{\tunit}$, and $\fvar{lprod}$ is indexed by $\klist{\kpair{\FF{L}}{\kTypeBlur}}$, classifying static lists pairing \emph{labels} with types. Labels are static values of kind $\FF{L}$, written abstractly using the metavariables containing $\ell$ in our examples. 

Before typechecking the external term in a program, the program is kind checked and its static terms are normalized to static values.  We write static values using the hatted metavariable $\hat \sigma$. Other syntactic metavariables in our system also have hatted forms if they can contain static terms. The desugared, statically normalized version of the example in Figure \ref{nat-sugared}, $\hat\rho_{\text{example}}$, is shown in Figure \ref{nat-normal}. %The syntax of @$\lambda$ from the perspective of a fragment client is given in Fig. \ref{atlam-syntax}. We will see a few additions relevant only to fragment providers when we discuss defining fragments below.

\subsection{External Terms}
 a program consists of first \emph{kind checking} its static terms, then normalizing its static terms, then typechecking the external term and,  simultaneously, \emph{translating} it to a term, $\iota$, in the \emph{typed internal language}. The dynamic behavior of an external term is determined entirely by its translation to this language. Internal types are written $\tau$. The internal language is only exposed to fragment providers, not to clients (i.e. normal developers).


%We will now give a core typed lambda calculus, @$\lambda$, that captures the semantics described in the previous sections. It is intended to make precise how active typechecking and translation works and how our mechanism relates to existing work on bidirectional typechecking, type-level computation and typed compilation while abstracting away from the details of Python's syntax and imposing a stronger type-level semantics that will allow us to state metatheoretic properties of interest. We will assume a fixed base for functions (providing the standard semantics of lambda functions) and target language, which we here call the \emph{internal language}.
 % program written in core @$\lambda$, without syntactic sugar and with all static terms normalized, is shown in Fig. \ref{nat-desugared}. The syntax of core @$\lambda$ is shown in Fig. \ref{grammar} and the syntactic desugarings are shown in Fig. \ref{desugaring}. The definition of $\phi_{\texttt{nat}}$ is shown in Fig. \ref{nat-atfrag}. We will discuss these in the following sections.

%In this section, we will develop an ``actively typed'' version of the simply-typed lambda calculus with simply-kinded type-level computation called $\lamAce$. More specifically, the level of types, $\tau$, will itself form a simply-typed lambda calculus. \emph{Kinds} classify type-level terms in the same way that types conventionally classify expressions. Types become just one kind  of type-level value (which we will write $\kTypeBlur$, though it is also variously written $\star$, \verb|T| and \verb|Type| in various settings). Rather than there being a fixed set of type and operator constructors, we allow the programmer to declare new  constructors, and give their static and dynamic semantics by writing type-level functions. The kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction will allow us to prove strong type safety, decidability and conservativity theorems.
%
%The syntax of Core $\lamAce$ is given in Fig. \ref{grammar}. An example of a program defining type and operator constructors that can be used to construct an active embedding of G\"odel's \textbf{T} into $\lamAce$ is given in Fig. \ref{nat}. We will discuss its semantics and how precisely the embedding, seen being used starting on line 15 to ultimately compute the sum of two and two, works as we go on. Natural numbers can, of course, be isomorphically embedded in existing languages, with a similar usage and asymptotic performance profile (up to function call overhead as an abstract type, for example). We will provide more sophisticated examples where this is less feasible later on (and note that type abstraction is an orthogonal mechanism).
%a type constructor declaration, $\fvar{Nat}$, indexed trivially, together with three operator constructors, also all indexed trivially, that implement the standard introductory forms for natural numbers as well as the recursor operator (as in G\"odel's T \cite{pfpl}). Following the type constructor declaration, we apply $\fvar{Nat}$ with the trivial index, $\tunit$, to form the type $\tvar{nat}$. Finally, we write an external term that uses the operators associated with $\fvar{nat}$ and the built-in constructor $\fvar{Parr}$, governing partial functions, to define an addition function and compute the addition of the natural numbers  two and two. We will introduce a more convenient concrete syntax in later portions of this thesis; for now we will restrict ourselves to the abstract syntax so that this example can directly aid in understanding the semantics.

%\begin{figure}[t]
%\small
%$$\begin{array}{rrcl}	
%%\textbf{programs} & 
%\text{programs} & \rho & ::= & \F{using}\phi~\F{in}e \\
%\text{fragments} & \phi &  ::= & \F{tycon}\fvar{tycon}~\F{of}\kappat{tyidx}~{\{}\F{iana} \tau_1; \F{esyn} \tau_2; \F{rep} \tau_3{\}} %\pipe \phi; \phi %\\
%%\\
%%& \pipe & 
%\pipe \F{def}\tvar{t} : \kappa = \tau \pipe \phi; \phi%\pdef{t}{\kappa}{\tau}{\progsort} \pipe 
%\\ %\pfam{\familyDf}{\progsort}
%%\\&  \pipe & \pdef{t}{\kappa}{\tau}{\progsort}  \pipe e
%%\pipe \pdef{t}{\kappa}{\tau}{\progsort} 
%\text{external terms}& e & ::= & \evar{x} \pipe \F{let} x = e_1~\F{in} e_2 \pipe \lambda x.e \pipe e : \tau\\
%&& \pipe & \FF{intro}[\taut{opidx}](\splat{e}{1}{n})\pipe e\cdot\FF{elim}[\taut{opidx}](\splat{e}{1}{n}) \\
%
%\text{type-level terms} & \tau 	& ::= 	& 	\tvar{t} \pipe \tifeq{\tau_{1}}{\tau_{2}}{\kappa}{\tau_{3}}{\tau_{4}} \pipe \terr  \\
% && \pipe & 
% %\F{let}\tvar{t}{:}\kappa=\tau_1~\F{in} \tau_2 \pipe 
%														\tlam{t}{\kappa}{\tau} \pipe 
%														\tapp{\tau_1}{\tau_2} \\
%&&\pipe&											
%														\tnil{\kappa} \pipe \tcons{\tau_1}{\tau_2} \pipe 
%									                     \tfold{\tau_1}{\tau_2}{\tvar{t}_{hd}}{\tvar{t}_{tl}}{\tvar{t}_{rec}}{\tau_3}
%														\\
% 		&& \pipe	& 	  \ell  \\
%	    && \pipe &  
%		\tunit \pipe 
%														\tpair{\tau_{1}}{\tau_{2}} \pipe 
%														\tfst{\tau} \pipe 
%														\tsnd{\tau} 
%														\\	
%     && \pipe & \tinl{\kappa_1, \kappa_2}{\tau_1} \pipe \tinr{\kappa_1, \kappa_2}{\tau_2} \pipe \tsumcase{\tau}{t}{\tau_1}{t}{\tau_2}\\
%&&\pipe	& 	\ttype{tycon}{\taut{tyidx}} \pipe  \tfamcase{\tau}{tycon}{t}{\tau_1}{\tau_2}\\
%%														\\													%				& & \pipe & \tfamcase{\tau}{Fam}{x}{\tau_1}{\tau_2}\\
%%																								
%%\\ 		 & 	\pipe	&	\tden{\tau_2}{\tau_1} \pipe \terr %\pipe \tden{\ibar}{\tau}^{\checkmark} 
%%\pipe \ttypeof{\tau} \pipe \itransof{\tau} \\% \tdencase{\tau}{x}{t}{\tau_1}{\tau_2}\\
%% %& & \pipe & 
%%%														\tdencase{\tau}{y}{x}{\tau_1}{\tau_2}
%%%														 \\
%%
% && \pipe & \titerm{\iota}
%			\pipe	\titype{\sigma} \pipe \trepof{\tau} \\
%		&& \pipe &  (\Gamma; e)? \pipe \FF{syn}(\tau_1; \tvar{t}_{ty},\tvar{t}_{trans}.\tau_2) \pipe \FF{ana}(\tau_1; \tau_2; \tvar{t}_{trans}.\tau_3)\\
%%\textbf{external terms} 				&	e	&	::=	&	\evar{x} \pipe 
%%%														\efix{x}{\tau}{e} \pipe 
%%														\elam{\evar{x}}{\tau}{e} \pipe 
%%														\eop{tycon}{op}{
%%															\tauidx
%%														}{
%%  												    		\splat{e}{1}{n}
%%														} \\
%%									& 		&		& 	\\
%%
%%
%%												
%%%\text{deabstracted}& \iota & ::= & \mathcal{G}[\iota, \sigma]\\
%%												
%%\\
%%\\
%%							
%%% &  & \pipe & \tvalof{\tau_1}{\tau_2} \pipe \iup{\tau} \\
%%% 												\trepof{\tau} \pipe \dup{\tau}\\
%%\text{translational IL}	& \bar{\iota} & ::= & x \pipe \ifix{x}{\bar \sigma}{\bar \iota} 
%%	%\pipe \ilam{x}{\bar \sigma}{\bar \iota} \pipe \iapp{\bar \iota_1}{\bar \iota_2} 
%%	\pipe \cdots \pipe \itransof{\tau} \\						
%% & \bar{\sigma} & ::= & \darrow{\bar \sigma_1}{\bar \sigma_2} \pipe \cdots \pipe \dup{\tau} \pipe \trepof{\tau} \\
%%%\text{abstracted} & \sabs & ::= & \darrow{\sabs_1}{\sabs_2} \pipe \cdots \pipe \sabsrep{\tau}
%%											\\
%\text{kinds} & \kappa	&	::=	&	\karrow{\kappa_1}{\kappa_2} \pipe \klist{\kappa} \pipe \FF{L} \pipe \kunit \pipe \kpair{\kappa_1}{\kappa_2} \pipe \ksum{\kappa_1}{\kappa_2} \pipe \kTypeBlur \pipe \kITerm \pipe \kIType \pipe \Q
%%											    \klabel \pipe
%%											    \klist{\kappa} \pipe
%%												\kunit \pipe 
%%												\kpair{\kappa_{1}}{\kappa_{2}} \\
%%												&&\pipe&
%%												\ksum{\kappa_1}{\kappa_2} \pipe
%%												\kTypeBlur \pipe \kDen \pipe 
%%												\kIType								
%%%\textbf{ops signature}			& \Theta	&	::=	&	\kOpEmpty \pipe \kOp{\Theta}{op}{\kappai}\\
%%%											 							&		&		&	\\
%\\
%\text{typing contexts} & \Gamma & ::= & \emptyset \pipe \Gamma, x \Rightarrow \tau\\
%\\
%\text{internal terms} & \iota	&	::=	&	\evar{x} \pipe 
%												\ifix{\evar{x}}{\sigma}{\iota} \pipe
%												\ilam{\evar{x}}{\sigma}{\iota} \pipe 
%												\iapp{\iota_{1}}{\iota_{2}} \pipe n \pipe \iota_1 \pm \iota_2 \pipe \FF{if0}(\iota; \iota_1; \iota_2)\\
%		&& \pipe &  \iunit \pipe \ipair{\iota_1}{\iota_2} \pipe \ifst{\iota} \pipe \isnd{\iota} \pipe \iup{\tau} 
%%\\&\pipe&								\cdots \pipe 	
%%												n \pipe \iop{\iota_{1}}{\iota_{2}} \pipe \iIfEq{\iota_{1}}{\iota_{2}}{\dint}{\iota_{3}}{\iota_{4}} \\
%%& \pipe & 
%%												\iunit \pipe
%%												\ipair{\iota_{1}}{\iota_{2}} \pipe 
%%												\ifst{\iota} \pipe
%%												\isnd{\iota} 
%%\\&\pipe&												
%%												 \iinl{\sigma_2}{\iota_1} \pipe \iinr{\sigma_1}{\iota_2} \\&\pipe & \icase{e}{x}{e_1}{x}{e_2} \\
%\\	\text{internal types} & \sigma	&	::=	&    \darrow{\sigma_1}{\sigma_2} \pipe \dint \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} %\pipe \dint \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} \pipe \dsum{\sigma_1}{\sigma_2}  
% \pipe \dup{\tau}
%\end{array}$$
%%\vspace{-10pt}
%\caption{\small Syntax of Core @$\lambda$. Here, $x$ ranges over external and internal language variables, $\tvar{t}$ ranges over type-level variables, $\fvar{tycon}$ ranges over type constructor names, $\ell$ ranges over labels, $n$ ranges over integers and $\pm$ ranges over standard binary operations over integers. The introductory form for arguments, $(\Gamma; e)?$, should only be constructed by the compiler, not by type constructor providers.
%\label{grammar}}
%\end{figure}
\begin{figure*}[t]
\small
\begin{tabular}{r l l l}
 & \textbf{initial} & \textbf{statically normal}\\
\textbf{programs} & $\rho ::=$ & $\hat\rho ::=$\\
 & ~~$\FF{using}~{\phi}~\FF{in}~e$ & ~~$\FF{using}~{\hat\phi}~\FF{in}~\hat e$\\
\textbf{fragment decls.} & $\phi ::=$ & $\hat\phi ::=$\\
tycon decls. & ~~$\FF{tycon}~\fvar{tycon} : \Theta = \theta$ & ~~$\FF{tycon}~\fvar{tycon} : \Theta = \hat\theta$\\
static binding & ~~$\FF{def}~\tvar{x} : \kappa = \sigma$ & \\
 & ~~$\phi; \phi$ & ~~$\hat\phi; \hat\phi$\\
\textbf{static terms} & $\sigma ::=$ & $\hat\sigma ::= $ &\textbf{kinds}~~~  $\kappa ::= $\\
static values & ~~$\hat \sigma$\\
static variables & ~~$\tvar{x}$ & ~ & ~\\
types & ~~$\ttype{tycon}{\sigma}$ & ~~$\ttype{tycon}{\hat\sigma}$ & ~~$\kTypeBlur$\\
static products & ~~$\tunit$ & ~~$\tunit$ & ~~$\kunit$\\
& ~~$(\sigma, \sigma)$ & ~~$(\hat\sigma, \hat\sigma)$ & ~~$\kpair{\kappa}{\kappa}$\\
static sums & ~~$\FF{in}[\ell](\sigma)$ & ~~$\FF{in}[\ell](\hat\sigma)$ & ~~$\Sigma[\{\overline{\ell \hookrightarrow \kappa}\}]$\\
static lists & ~~$[]_\kappa$ & ~~$[]_\kappa$ & ~~$\klist{\kappa}$\\
 & ~~$\sigma :: \sigma$ & ~~$\hat\sigma :: \hat\sigma$ & \\
static labels & ~~$\ell$ & ~~$\ell$ & ~~$\FF{L}$\\
\\
\textbf{external terms} & $e ::= $ & $\hat e ::=$ \\
variables & ~~$x$ & ~~$x$\\
binding & ~~$\FF{let}~x=e~\FF{in}~e$ & ~~$\FF{let}~x=\hat{e}~\FF{in}~\hat{e}$\\
fixpoints & ~~$\FF{fix}~x{:}\sigma~\FF{is}~e$ & ~~$\FF{fix}~x{:}\hat\sigma~\FF{is}~\hat e$\\
ascription & ~~$e : \sigma$ & ~~$\hat{e} : \hat\sigma$\\
%introductory forms & ~~$\nu$ & ~~$\hat\nu$\\
%eliminatory forms & ~~$\eta$ & ~~$\hat\eta$\\
% & $\nu ::= $ & $\hat \nu ::=$\\
analytic lambda & ~~$\lambda x.e$ & ~~$\lambda x.\hat{e}$\\
%synthetic lambda & ~~$\lambda x {:} \sigma.e$ & ~~$\lambda x{:}\hat\sigma.\hat e$\\
analytic intro & ~~$\FF{intro}[\sigma](\overline{e})$ & ~~$\FF{intro}[\hat\sigma](\overline{\hat e})$\\
%synthetic intro & ~~$\fvar{tycon}\cdot\FF{intro}[\sigma](\overline{e})$ & ~~$\fvar{tycon}\cdot\FF{intro}[\hat \sigma](\overline{\hat e})$\\
% & $\eta ::= $ & $\hat\eta ::=$\\
synthetic elim & ~~$e\cdot\FF{elim}[\sigma](\overline{e})$ & ~~$\hat e\cdot\FF{elim}[\hat \sigma](\overline{\hat e})$\\
\end{tabular}
\caption{Syntax from the perspective of fragment clients (i.e. normal developers)}
\end{figure*}
\begin{figure*}[t]
\small
\hspace{-5px}
$
\begin{array}{rclr}
%\FF{intro}(e_1; \ldots; e_n) & := & \FF{intro}[()](e_1; \ldots; e_n)\\
\sigma\langle e_1; \ldots; e_n \rangle & := & \FF{intro}[\tfst{\sigma}](e_1; \ldots; e_n) : \tsnd{\sigma} & \text{assisted intro}\\
%\langle\sigmaidx\rangle(e_1; \ldots; e_n) & := & \FF{intro}[\sigmaidx](e_1; \ldots; e_n)\\
%[\sigmaidx](e_1; \ldots; e_n) & := & \FF{intro}[\sigmaidx](e_1; \ldots; e_n)\\
\{ \ell_1{=}~e_1, \ldots, \ell_n{=}~e_n \} & := & \FF{intro}[\ell_1 :: \ldots :: \ell_n :: []_{\FF{L}}](e_1; \ldots; e_n) & \text{labeled seq.}  \\
(e_1, ..., e_n) & := & \FF{intro}[()](e_1; \ldots; e_n) & \text{unlabeled seq.}\\
%n & := & \FF{intro}[n]() & \text{number literal} \\
%s & := & \FF{intro}[s]() & \text{string literal}\\
%e\cdot\FF{elim}(e_1; \ldots; e_n) & := & e\cdot\FF{elim}[()](e_1; \ldots; e_n)\\
e~e_1 & := & e\cdot\FF{elim}[()](e_1) & \text{application}\\
\sigma~e~\langle e_1; \ldots; e_n \rangle  & := & e\cdot\FF{elim}[\sigma](e_1; \ldots; e_n) & \text{assisted elim}\\
e.\ell & := & e\cdot\FF{elim}[\ell]() & \text{projection}\\
\F{case}e~\{\ell_1\langle x_1 \rangle \Rightarrow e_1~|~\ldots~|~\ell_n\langle x_n \rangle \Rightarrow e_n\} & := & e\cdot\FF{elim}[\ell_1 :: \ldots :: \ell_n :: []_{\FF{L}}]( & \text{case analysis}\\
 & & \quad \lambda x_1.e_1; \ldots; \lambda x_n.e_n)\\
\sigma_1 \rightharpoonup \sigma_2 & := & \ttype{parr}{(\sigma_1, \sigma_2)} & \text{arrow types}
%!TEX encoding = UTF-8 Unicodee.\ell(e_1; ...; e_n) & := & e\cdot\FF{elim}[\ell](e_1; \ldots; e_n)
%(e_1, ..., e_n) & := & intro[()](e_1; ...; e_n)
%n &  := & intro[n]()
%s & := & intro[s]()
%
%e.l & := & elim[l](e)
%e[e_1; ...; e_n] & := & elim[()](e_1, ..., e_n)
%e.l(e_1; ...; e_n) & := & elim[l]
\end{array}
$
\caption{\small Desugaring from conventional concrete syntax to core forms. The number and string literal forms assume type-level numbers, $n$, and strings, $s$ (details not shown).}
\label{desugaring}
\end{figure*}
\newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
\newcommand{\tri}{\cev\iota}
\newcommand{\trt}{\cev\tau}
\begin{figure*}
\newcommand{\tcl}[1]{\multicolumn{2}{l}{#1}}
\small
\begin{tabular}{r l l l l}
 & \tcl{\textbf{initial}} & \textbf{statically normal} \\
\textbf{tycon definitions} & \tcl{$\delta ::=$}  & $\hat\delta ::=$ \\
 & \tcl{~~$\FF{iana}~\sigma; \FF{esyn}~\sigma; \FF{rep}~\sigma$} & ~~$\FF{iana}~\hat\sigma; \FF{esyn}~\hat\sigma; \FF{rep}~\hat\sigma$\\
 \\
\textbf{(static language, cont.)} & \tcl{$\sigma ::= ...$} & $\hat\sigma ::= ...$ & $\kappa ::= ...$\\
type elim & \tcl{~~$\tfamcase{\sigma}{tycon}{x}{\sigma}{\sigma}$} & \\
static product elim & \tcl{~~$\sigma.\FF{prl}$} &\\
 & \tcl{~~$\sigma.\FF{prr}$} &\\
static sum elim & \tcl{~~$\FF{case}~\sigma~\{\overline{l\langle\tvar{x}\rangle\Rightarrow\sigma}\}$} &\\
static list elim & \tcl{~~$\FF{listrec}(\sigma; \sigma; \tvar{h}, \tvar{t}, \tvar{r}.\sigma)$} & ~\\
decidable equality & \tcl{~~$\tifeq{\hat\sigma}{\hat\sigma}{\kappa}{\hat\sigma}{\hat\sigma}$} & ~ & ~\\
type error & \tcl{~~$\FF{tyerr}$} & ~ \\
static functions & \tcl{~~$\tlam{x}{\kappa}{\sigma}$} & ~~$\tlam{x}{\kappa}{\sigma}$ & ~~$\karrow{\kappa}{\kappa}$\\
 & \tcl{~~$\sigma~\sigma$} & ~ & ~\\
term translations & \tcl{~~$\titerm{\tri}$} & ~~$\titerm{\iota}$ & ~~$\kITerm$\\
type translations & \tcl{~~$\titype{\trt}$} & ~~$\titype{\tau}$ & ~~$\kIType$\\
 & \tcl{~~$\trepof{\sigma}$} & ~\\
arguments & \multicolumn{3}{c}{\color{gray}(introduced only by typechecker)} & ~~$\Q$\\
 & \tcl{~~$\FF{ana}(\sigma; \sigma; \tvar{x}.\sigma)$} & \\
 & \tcl{~~$\FF{syn}(\sigma; \tvar{x}, \tvar{t}.\sigma)$} & \\
\\
\textbf{internal language} & $\tri ::= $ & $\trt ::= $ & $\iota ::= $ & $\tau ::=$\\
splice & ~~$\iup{\sigma}$ & ~~$\dup{\sigma}$ & \\
variables & ~~$x$ &  & ~~$x$ & \\
fixpoints & ~~$\FF{fix}~x{:}\trt~\FF{is}~\tri$ & & ~~$\FF{fix}~x{:}\tau~\FF{is}~\iota$\\
functions & ~~$\lambda x{:}\trt.\tri$ & ~~$\trt \rightharpoonup \trt$ & ~~$\lambda x{:}\tau.\iota$ & ~~$\tau \rightharpoonup \tau$\\
 & ~~$\tri~\tri$ & ~ & ~~$\iota~\iota$\\
products & ~~$()$ & ~~$\dunit$ & ~~$()$ & ~~$\dunit$\\
 & ~~$(\tri, \tri)$ & ~~$\dpair{\trt}{\trt}$ & ~~$(\iota, \iota)$ & ~~$\dpair{\tau}{\tau}$\\
 & ~~$\tri.\FF{prl}$ & ~ & ~~$\tri.\FF{prl}$ & ~\\
  & ~~$\tri.\FF{prr}$ & ~ & ~~$\tri.\FF{prr}$ & ~\\
%products & ~~$(\overline{l=\tri})$ & ~~$\Pi[\overline{(l, \trt)}]$ & ~~$(\overline{l = \iota})$ & ~~$\Pi[\overline{(l, \tau)}]$\\
% & ~~$\tri\cdot l$ & ~ & ~~$\iota\cdot l$ & ~\\
sums & ~~$\FF{in}[\ell](\tri)$ & ~~$\Sigma[\{\overline{\ell \hookrightarrow \trt}\}]$ & ~~$\FF{in}[\ell](\iota)$ & ~~$\Sigma[\{\overline{\ell \hookrightarrow \tau}\}]$\\
 & ~~$\FF{case}~ \tri ~\{\overline{\ell(x) \Rightarrow \tri}\}$ & ~ & ~~$\FF{case}~\iota~\{\overline{\ell(x)\Rightarrow \iota}\}$ & ~\\
integers & ~~$\FF{int}[z]$ & ~~$\FF{int}$ & ~~$\FF{int}[z]$ & ~~$\FF{int}$\\
 & ~~$\tri \pm \tri$ & ~ & ~~$\iota \pm \iota$ & ~\\
 & ~~$\FF{if0}(\tri; \tri; \tri; \tri)$ & ~ & ~~$\FF{if0}(\iota; \iota; \iota; \iota)$ & ~
 %& ~~$
\end{tabular}
\caption{Syntax from the perspective of fragment providers also includes an internal language and static language.}
\end{figure*}
\begin{figure*}
\small
\begin{tabular}{r l l}
external typing context & $\Gamma ::= \emptyset \pipe \Gamma, x \Rightarrow \ttype{tycon}{\hat\sigma}$\\
\\
 & \textbf{initial} & \textbf{statically normal} \\
\textbf{static language, cont.} & $\sigma ::= ...$ & $\hat\sigma ::= ...$\\
argument intro & ~~$\FF{arg}(\Gamma; \hat e)$ & ~~$\FF{arg}(\Gamma; \hat e)$
\end{tabular}
\caption{Syntax from the perspective of the semantics / compiler.}
\end{figure*}
\newcommand{\fkctx}{\mathcal{F}}
\newcommand{\ok}[1]{#1~\mathtt{ok}}
\newcommand{\normalize}[3]{#1 \curlyveedownarrow_{#2} #3}
\newcommand{\normalizeX}[2]{\normalize{#1}{\Phi}{#2}}
\newcommand{\serr}[1]{#1~\mathtt{tyerr}}
\begin{figure*}
\small
\begin{tabular}{r l l}
\textbf{context} & \textbf{syntax} & \textbf{well-formedness}\\
fragment kind context & $\fkctx ::= \fkctx_0 \pipe \fkctx, \fvar{tycon}[\kappa]~\{\FF{intro}[\kappa]; \FF{elim}[\kappa]\}$ & $\vdash \fkctx$\\
kinding context & $\Delta ::= \Delta \pipe \Delta, \tvar{x} : \kappa$\\
fragment context & $\Phi ::= \Phi_0 \pipe \Phi, \fvar{tycon}~\{\hat\delta\}$ & $\vdash \Phi \sim \fkctx$\\
external typing context & $\Gamma ::= \emptyset \pipe \Gamma, x \Rightarrow \hat\sigma$ & $\vdash_\Phi \Gamma$\\
internal typing context & $\Omega ::= \emptyset \pipe \Omega, x : \tau$\\
~
\end{tabular}
\\
\\
\begin{tabular}{l l l l l}
\textbf{kinding}& \multicolumn{2}{l}{\textbf{static normalization}} & \multicolumn{2}{l}{\textbf{program compilation}}\\
$\ok{\rho}$ & $\normalize{\rho}{}{\hat\rho}$ & $\serr{\rho}$ & $\hat\rho \leadsto \iota$\\
$\Delta \vdash_\fkctx {\phi}\sim\fkctx$ & $\normalizeX{\phi}{\hat\phi}$ & $\serr{\phi}$ & $\hat\phi \sim \Phi$\\
$\Delta \vdash_\fkctx \delta\sim\{\kappa; \kappa\}$ & $\normalizeX{\delta}{\hat\delta}$ & $\serr{\delta}$ & \multicolumn{2}{l}{\textbf{active typechecking and translation}}\\
$\Delta \vdash_\fkctx \sigma : \kappa$ & $\normalizeX{\sigma}{\hat \sigma}$ & $\serr{\sigma}$ & $\Gamma \vdash_\Phi \hat e \Leftarrow \hat\sigma \leadsto \iota$ & $\Gamma \vdash_\Phi \hat e \Rightarrow \hat\sigma \leadsto \iota$\\
$\Delta \vdash_\fkctx \ok{e}$ & $\normalizeX{e}{\hat e}$ & $\serr{e}$ & $\vdash_\Phi \hat\sigma \leadsto \tau$ & $\vdash_\Phi \Gamma \leadsto \Omega$\\
$\Delta \vdash_\fkctx \ok{\tri}$ & $\normalizeX{\tri}{\iota}$ & $\serr{\tri}$ & \multicolumn{1}{l}{\textbf{internal statics}} & \textbf{internal dynamics}\\
$\Delta \vdash_\fkctx \ok{\trt}$ & $\normalizeX{\trt}{\tau}$ & $\serr{\trt}$ & $\Omega \vdash \iota : \tau$ & $\iota \mapsto \iota$~~~~~~~~$\iota~\mathtt{val}$
\end{tabular}
\caption{Summary of judgements}
\end{figure*}
\begin{figure*}
\small
$
\begin{array}{lcl}
\delta_{\texttt{nat}} & := & \quad \FF{iana}~\tlam{opidx}{\kunit+\kunit}{\tlam{tyidx}{\kunit}{\tlam{args}{\klist{\FF{Arg}}}{
	\tsumcase{\tvar{opidx}\\
&&\quad\quad}{\_}{\tvar{arity0}~\tvar{args}~\titerm{0}\\
&&\quad\quad}{\_}{\tvar{arity1}~\tvar{args}~\tlam{a}{\FF{Arg}}{\FF{ana}(\tvar{a}; \ttype{nat}{\tunit}; \tvar{x}.\titerm{\iup{\tvar{x}}+1})}}
}}}\\
&&\quad \FF{esyn}~\tlam{opidx}{\kunit}{\tlam{tyidx}{\kunit}{\tlam{x}{\kITerm}{\tlam{args}{\klist{\FF{Arg}}}{\tvar{arity2}~\tvar{args}~\tlam{a1}{\FF{Arg}}{\tlam{a2}{\FF{Arg}}{\\
&&\quad\quad \FF{syn}(\tvar{a1}; \tvar{t1}, \tvar{x1}.\\
&&\quad\quad\quad \F{let}\tvar{t2} : \kTypeBlur = \ttype{arrow}{(\ttype{nat}{\tunit}, \ttype{arrow}{(\tvar{t1}, \tvar{t1})})}~\F{in} \\
&&\quad\quad\quad \FF{ana}(\tvar{a2}; \tvar{t2}; \tvar{x2}.(\tvar{t1},\\
&&\quad\quad\quad\quad \titerm{\iapp{(\ifix{f}{\darrow{\dint}{\dup{\trepof{\tvar{t2}}}}}{\ilam{x}{\dint}{\\&&\quad\quad\quad\quad\quad\quad \FF{if0}(x; \iup{\tvar{x1}}; {
		%\\&\quad\quad\quad\quad\quad\quad
		\iapp{\iup{\tvar{x2}}~(x-1)}{(\iapp{f}{(x-1)})}})}}\\
		&&\quad\quad\quad\quad\quad)}{\iup{\tvar{x}}}})))}}}}}}\\
&&\quad \FF{rep}~\tlam{tyidx}{\kunit}{\titype{\dint}}\\
\end{array}
$
\caption{\small The definition of the nat type constronstructor.}
\end{figure*}
%\begin{figure}
%\small
%\begin{flalign}
%& \F{tycon}\fvar{record}~\F{of}\klist{\kpair{\FF{L}}{\kTypeBlur}}~\{\\
%& \quad \FF{iana}~\{\tlam{i}{\kpair{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{\klist{\FF{L}}}}{\tlam{a}{\klist{\Q}}{\\& \quad\quad\tfold{\tvar{zip3}~\tfst{\tvar{i}}~\tsnd{\tvar{i}}~\tvar{a}}
%	 {\titerm{\tunit}}{h}{t}{r}{
%	\\&\quad\quad\quad\tifeq{\tfst{\tvar{first}~\tvar{h}}}{\tvar{second}~\tvar{h}}{\FF{L}}{
%	\\&\quad\quad\quad\quad
%	\F{let}~\tvar{x}:\kITerm~=~\FF{ana}(\tvar{third}~\tvar{h}; \tsnd{\tvar{first}~\tvar{h}})
%~\FF{in}
%\\ & \quad\quad\quad\quad \tfold{\tvar{t}}{\tvar{x}
%}{\_}{\_}{\_}{\titerm{(\iup{\tvar{x}},\iup{\tvar{r}})}}	}{\terr}
%%     \F{let}~\tvar{tt}:\FF{TT}=~\FF{syn}(\tsnd{\tsnd{\tvar{h}}}) \F{in}\\
%%\\     & \quad\quad\quad 
%}
%}}\}\\
%& \quad \FF{isyn}~\{\tlam{i}{\klist{\FF{L}}}{\tlam{a}{\klist{\Q}}{
%\\&\quad\quad \tfold{\tvar{zip2}~\tvar{i}~\tvar{a}}{\tden{\titerm{()}}{\ttype{record}{\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}}}{h}{t}{r}{
%\\&\quad\quad\quad \F{let}~\tvar{htt}:\FF{TT}~=~\FF{syn}(\tsnd{\tvar{h}})
%~\FF{in}
%\\&\quad\quad\quad \F{let}~\tvar{hty}:\kTypeBlur~=~\ttypeof{\tvar{htt}}
%\\&\quad\quad\quad \F{let}~\tvar{ty}:\kTypeBlur~=~\ttype{record}{(\tfst{\tvar{h}}, \tvar{hty})::\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}
%~\FF{in}
%\\&\quad\quad\quad \tfold{\tvar t}{\tden{\itransof{\tvar{htt}}}{\tvar{ty}}}{\_}{\_}{\_}{
%\\&\quad\quad\quad\quad \tden{\titerm{(\iup{\itransof{\tvar{htt}}},\iup{\itransof{\tvar{r}}})}}{\tvar{ty}}}}
%}}\}\\
%& \quad \FF{esyn}~\{\tlam{i}{\FF{L}}{\tlam{a}{\klist{\Q}}{
%	\tvar{arity1}~\tvar{a}~\tlam{ty}{\kTypeBlur}{\tlam{x}{\kITerm}{
%\\&\quad\quad		\tfamcase{\tvar{ty}}{record}{sig}{
%\\&\quad\quad\quad \tfold{\tvar{sig}}{\terr}{h}{t}{r}{
%\\&\quad\quad\quad\quad \tifeq{\tfst{\tvar{h}}}{\tvar{i}}{\FF{L}}{\tfold{\tvar{t}}{\tden{\tvar{x}}{\tsnd{\tvar{h}}}}{\_}{t'}{\_}{
%\\&\quad\quad\quad\quad\quad \tfold{\tvar{t'}}{\tden{\titerm{\ifst{\iup{\tvar{x}}}}}{\tsnd{\tvar{h}}}}{\_}{\_}{\tvar{r'}}{
%\\&\quad\quad\quad\quad\quad\quad \tden{\titerm{\isnd{\iup{{\itransof{\tvar r}}}}}}{\tsnd{\tvar{h}}}}}\\&\quad\quad\quad\quad\hspace{-3px}}{\tvar{r}}
%}\\&\quad\quad\hspace{-3px}}{\terr}
%	}}
%}}\\
%& \quad \FF{rep}~\{\tlam{i}{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{ 	\\& \quad \quad \tfold{\tvar{i}}{\titype{\dunit}}{s}{j}{r}{
% 		\tfold{\tvar{j}}{\trepof{\tsnd{\tvar{s}}}}{\_}{\_}{\_}{\\
%		&\quad\quad\quad
% 		\titype{\dpair{\dup{\trepof{\tsnd{\tvar{s}}}}}{\dup{\tvar{r}}}}
% 		}
% 	}}\}\\
%& \}; \\
%& \F{let}\tvar{R} : \kTypeBlur = \ttype{record}{(\ell_1, \ttype{record}{\tnil{{\FF{L}\times\kTypeBlur}}}) :: \tnil{{\FF{L}\times\kTypeBlur}}}~\F{in}\\
%&\F{let}id = \elam{x}{\tvar{R}}{\{\ell_1=~x\cdot\ell_1\} :: \fvar{record}}\\
%&\F{let}triv = id[\{\ell_1 = \{\}\}]\cdot\ell_1
%%&\F{let}one = \FF{intro}[\ell_\text{succ}]
%%&\F{let}plus = \elam{x}{\tvar{nat}}{\elam{y}{\tvar{nat}}{\FF{elim}[()](x; y; \\
%%& \quad \elam{r}{\tvar{nat}}{intro[\ell_\text{succ}](r) : \tvar{nat}}})}
%\end{flalign}
%\caption{The definition of the record type in $\lamAce$ using the desugarings in Figure \ref{desugaring} and with the addition of simple let bindings for both type and term variables (not shown). Some type-level helper functions also omitted for concision.}
%\label{record-theory}
%\end{figure}


%These correspond to the premises of the \emph{central compilation judgement} $\pkcompiles{\rho}{\iota}$:
%\[
%\inferrule[p-compiles]{
%	\emptyset \vdash_{\fvalCtx_0} \rho\\
%%	\progOK{\emptyset}{\fvalCtx_0}{\rho}\\
%	\pcompiles{\fvalCtx_0}{\rho}{\iota}
%}{\pkcompiles{\rho}{\iota}}
%\]
%This anchors our exposition; we will describe how it is derived (i.e. how to write a compiler for $\lamAce$) in the following sections. 
The key judgements are the \emph{bidirectional active typechecking and translation judgements}, relating an external term, $e$, to a {type}, $\tau$, and an internal term, $\iota$, called its \emph{translation}, under \emph{typing context} $\Gamma$ and \emph{constructor context} $\Phi$.
\[\Gamma \vdash_\fvalCtx e \Rightarrow \tau \leadsto \iota
~~~~~~~\text{and}~~~~~~~
\Gamma \vdash_\fvalCtx e \Leftarrow \tau \leadsto \iota\]
%\[\ecompilesAX{e}{\tau}{\iota}\]

The typing context, $\Gamma$, maps variables to types in essentially the conventional way (\cite{pfpl} contains the necessary background for this section). The constructor context, $\Phi$, tracks user-defined type  constructors introduced in the ``imported'' fragments. %Note that fragments can also export type-level values bound to type-level variables (written in bold font, e.g. $\tvar{nat}$, $\tvar{s}$, $\tvar{z}$, etc.), but substitution will be performed prior to typechecking, so they need not be tracked by $\Phi$. % Weakening and exchange of the constructor context closely related to the issue of conservativity that we will return to. Each constructor is identified by name, so there is no analog to contraction. 
 This form of semantics can be seen as lifting into the language specification the first stage of a type-directed compiler like the TIL compiler for Standard ML \cite{tarditi+:til-OLD} and has some parallels to the Harper-Stone semantics for Standard ML \cite{Harper00atype-theoretic}. There, as in Wyvern, external terms were given meaning by elaboration from the EL to an IL having the same type system. Here the two languages have different type systems, so we call it a translation rather than an elaboration (arranging the judgement form slightly differently to emphasize this distinction, cf. above).

In @$\lambda$, the internal language (IL) provides partial functions (via the generic fixpoint operator of Plotkin's PCF), simple product  types and integers for the sake of our example (and as a nod toward practicality on contemporary machines). In practice, the internal language could be any typed  language with a specification for which type safety and decidability of typechecking have been satisfyingly determined. In Sec. \ref{ace}, we will see how the internal language can itself be made user-definable. The internal type system serves as a ``floor'': guarantees that must hold for terms of any type (e.g. that out-of-bounds access to memory never occurs) must be maintained by the internal type system. User-defined constructors can enforce invariants stronger than those the internal type system maintains at particular types, however. Performance is also ultimately limited by the internal language and downstream compilation stages that we do not here consider (safe compiler extension has been discussed in previous work, e.g. \cite{conf/pldi/TatlockL10}).

The external language has a fixed syntax with six forms: variables, $\FF{let}$-bindings, lambda terms, type ascription and generalized introductory and elimination forms. As we will see, the generalized introductory form is given meaning by the type it is analyzed against (similar to the protocol for TSLs in Wyvern), and the elimination form is given meaning by the type of the external term being eliminated ($e$). This represents an internalization into the language of Gentzen's inversion principle \cite{gentzen}\todo{what to cite for this?}. Fig. \ref{desugaring} shows how to recover more conventional introductory and elimination forms by a purely syntactic desugaring.%\footnote{Note that for even more flexibility, we could also include TSLs, but we choose to avoid that for simplicity (and to show that, strictly speaking, one need not have an extensible syntax to have an extensible type system, and \textit{vice versa}).}

\subsection{Types and Type-Level Computation}\label{types}
@$\lambda$ supports, and makes extensive use of, simply-kinded type-level computation. Specifically, type-level terms, $\tau$, themselves form a typed lambda calculus. The classifiers of type-level terms are called \emph{kinds}, $\kappa$, to distinguish them from  \emph{types}, which are  type-level values of kind $\kTypeBlur$. %In Ace, the type-level language (together with the level of programs) is written in Python, which can be thought of as having a rather limited kind system (with one kind, \verb|dyn|). Here, we are able to more precisely discuss the kinds of values in the type-level language. 
As in Sec. \ref{att}, types are formed by applying a \emph{type constructor} to an \emph{index}. User-defined type constructors are declared in fragment definitions using $\FF{tycon}$. Each constructor in the program must have a unique name, written e.g. \fvar{nat} or \fvar{lprod}. %\footnote{We assume naming conflicts can be avoided by some extrinsic mechanism.} 
A type constructor must also declare an \emph{index kind}, $\kappat{tyidx}$. A type is introduced by applying a type constructor to an index of this kind, written $\ttype{Tycon}{\taut{tyidx}}$.  For example, the type of natural numbers is indexed trivially (i.e. by kind $\kunit$), so it is written $\ttype{Nat}{\tunit}$.

 To permit the embedding of interesting type systems, the type-level language includes several kinds other than $\kTypeBlur$. We lift several functional data structures to the type level: here, only unit ($\kunit$), binary products ($\kpair{\kappa_1}{\kappa_2}$), binary sums ($\ksum{\kappa_1}{\kappa_2}$) and lists ($\klist{\kappa}$), in addition to labels (introduced as $\ell$, possibly with a subscript, having kind \FF{L}).  The type constructor $\fvar{nat}$ is indexed trivially because there is only one natural number type, but $\fvar{lprod}$ would be indexed by a list of pairs of {labels}  and types. The type constructor $\fvar{arrow}$ is included in the initial constructor context, $\fvalCtx_0$, and has index kind $\kpair{\kTypeBlur}{\kTypeBlur}$.
 As with the internal language, in practice, one could include a richer programming language and retain the spirit of the calculus, as long as it does not introduce general recursion at the type level. For example, our desugarings add support for number literals and string literals, which require adding numbers and strings to the type-level language (and providing a means for lifting them from the type-level language to the internal language, as we will discuss).
  
 %We see a record type, abbreviated $\tvar{R}$, constructed on line 26. 

The kind $\kTypeBlur$ also has an elimination form, $\tfamcase{\tau}{Tycon}{x}{\tau_1}{\tau_2}$ allowing the extraction of a type index by case analysis against a contextually-available type constructor. To a first approximation, one might think of type constructors as constructors of a built-in open datatype \cite{conf/ppdp/LohH06}, $\kTypeBlur$, at the type-level. Like open datatypes, there is no notion of exhaustiveness so the default case is required for totality. %We will see where this is used shortly.



%We will write the kind of types as $\kTypeBlur$, though it is also written $\star$ or \verb|Type| in various similarly structured languages (see Sec. \ref{related-work}). 
% Rather than there being a fixed set of type constructors, we allow the programmer to declare new type  constructors, and give the static and dynamic semantics of their associated operators, by writing type-level functions. In the semantics for this calculus, our kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction allow us to prove strong type safety, decidability and conservativity theorems.



% and integers ($\dint$). We also include labels ($\klabel$), written in a slanted font, e.g. $\tlabel{myLabel}$, which are string-like values that only support comparison and play a distinguished role in the expanded syntax, as we will later discuss. Our first example, $\fvar{nat}$, is indexed trivially, i.e. by unit kind, $\kunit$, so there is only one natural number type, $\ttype{nat}{\tunit}$, but we will show examples of type constructors that are indexed in more interesting ways in later portions of this work. For example, $\fvar{LabeledTuple}$ has index kind $\klist{\kpair{\klabel}{\kTypeBlur}}$. 
 
 Type constructors are not first-class; they do not themselves have arrow kind as in some kind systems (e.g.  \cite{watkins2008specifying}; Ch. 22 of \emph{PFPL} describes a related system \cite{pfpl}). The type-level language does, however, include total functions of arrow kind, written $\karrow{\kappa_1}{\kappa_2}$. Type constructor application can be wrapped in a type-level function to emulate a first-class or uncurried version of a type constructor for convenience.% (indeed, such a wrapper could be generated automatically, though we do not do so). 

Two type-level terms of kind $\kTypeBlur$ are equivalent if they apply the same constructor, identified by name, to equivalent indices. Going further, we ensure that deciding type equivalence requires only checking for syntactic equality after normalization by imposing the restriction that equivalence at a type constructor's index kind must be decidable in this way. Our treatment of equivalence in the type-level language is thus quite similar to the treatment of term-level equality using ``equality types'' in a language like Standard ML.
% A kind $\kappa$ is an  \emph{equality kind} if $\kEq{\kappa}$ can be derived (see appendix). 
Conditional branching on the basis of equality at an equality kind can be performed in the type-level language. Equivalence at arrow kind is not decidable by our criteria, so type-level functions cannot appear within type indices. This also prevents general recursion from arising at the type level. Without this restriction, a type-level function taking a type as an argument could ``smuggle in'' a self reference as a type index, extracting it via case analysis (continuing our analogy to open datatypes, this is closely related to the positivity condition for inductive datatypes in total functional languages like Coq).% as maintaining the metatheoretic guarantee that typing respects type equivalence would impose a substantial burden in such a setting.% (a na\"ive approach to this would impose non-trivial extrinsic proof obligations onto extension developers that, unlike in others in this thesis, could threaten type safety).

%Every type constructor also defines type-level functions called $\FF{iana}$ and $\FF{esyn}$, which we will describe below, and a \emph{representation schema}, a type-level function that associates with every type an internal type. We will return to this after introducing operators.
%
%\subsection{Core External Forms and Desugaring}\label{opcons}
%The syntax for external terms (Figure \ref{grammar}) contains variables, $\lambda$ terms, three generalized introductory forms and a single generalized elimination form. The introductory forms are either unascribed, ascribed with a type or ascribed with a type constructor, as in our discussion of Ace. These generalized forms take a single a type-level value as an {index} and $n \geq 0$ arguments, which are other external terms. To better motivate this choice, we can give a purely syntactic desugaring of a Python-like syntax with labels to these forms, shown in Figure \ref{desugaring}. It is instructive to rewrite lines 27-28 of Figure \ref{record-theory} using these desugarings.% In Ace, desugarings can be user-defined 
%
%User-defined operator constructors are declared using \textsf{opcon}.  For reasons that we will discuss, our calculus associates every operator  constructor with a type constructor. The \emph{fully-qualified name} of every operator constructor, e.g. $\fvar{Nat}.\opvar{z}$, must be unique. Operator constructors, like type constructors, declare an index kind, $\kappaidx$. In our first example, all the operator constructors are indexed trivially (by index kind $\kunit$), but other examples use more interesting indices. For example, in SML, the projection operator \verb|#3| can be applied to an $n$-tuple, $e$, iff $n \geq 3$. Note that it thus cannot be a function with a standard arrow type. Notionally, \verb|#| is an operator constructor and \verb|3| is its index. In an active embedding of $n$-tuples into $\lamAce$, this would be written $\eop{Tuple}{prj}{3}{e}$ (we will nearly recover ML's syntax later). $\fvar{LabeledTuple}.\opvar{prj}$ is the operator constructor used to access a field of a labeled tuple, so it has index kind $\klabel$. An operator itself is, notionally, selected by indexing an operator constructor, e.g. $\fvar{Nat}.\opvar{s}\langle \tunit \rangle$, but technically neither operator constructors nor operators are first-class at any level (additional machinery would be needed, e.g. an \textsf{Op} kind, but this is not fundamental to our calculus). 
%Instead, in the external language, an operator constructor is applied by simultaneously providing an index and  $n \geq 0$ \emph{arguments}, written $\eop{Tycon}{op}{\tauidx}{\splat{e}{1}{n}}$\footnote{It may be helpful to distinguish between type/operator constructors and \emph{term formers}. There are term formers at all levels in the calculus. For example, operator constructor application and $\lambda$ are  external term formers, and type constructor application is a type-level term former. We might write these following Harper's conventions for abstract syntax to highlight this distinction \cite{pfpl}: $\mathtt{lam}[\tau](x.e)$, $\mathtt{ocapp}[\fvar{Tycon}, \opvar{op}, \tauidx](\splat{e}{1}{n})$ and $\texttt{tcapp}[\fvar{Tycon}](\tau)$.}. For example, on line 18 of Fig. \ref{nat}, we see the operator constructors $\fvar{Nat}.\opvar{z}$ and $\fvar{Nat}.\opvar{s}$ being applied to compute $two$. %\footnote{Although our focus here is entirely on semantics, a brief note on syntax: in the expanded syntax, the trivial indices and empty argument lists can be omitted, so we could write \texttt{Nat.s(Nat.s(Nat.z))}. With the ability to ``open'' a type's operators into the context, we could shorten this still to \texttt{s(s(z))}. Alternatively, with the ability to define a TSL in a manner similar to that in Sec. \ref{aparsing}, we might instead just write \texttt{2}.}

\subsection{Bidirectional Active Typechecking and Translation}

%
\newcommand{\atjsynX}[3]{\Gamma \vdash_\fvalCtx #1 \Rightarrow #2 \leadsto #3}
\newcommand{\atjanaX}[3]{\Gamma \vdash_\fvalCtx #1 \Leftarrow #2 \leadsto #3}
\newcommand{\atjerrX}[1]{\Gamma \vdash_\fvalCtx #1~ \mathtt{error}}
\begin{figure}[t]
\small
$\fbox{\inferrule{}{\atjsynX{e}{\tau}{\iota}}}$~~~~
$\fbox{\inferrule{}{\atjanaX{e}{\tau}{\iota}}}$~~~~
%$\fbox{\inferrule{}{\atjerrX{e}}}$
\begin{mathpar}
\inferrule[att-flip]{
	\atjsynX{e}{\tau}{\iota}
}{
	\atjanaX{e}{\tau}{\iota}
}

\inferrule[att-var]{
	x \Rightarrow \tau \in \Gamma
}{
	\atjsynX{x}{\tau}{x}
}

\inferrule[att-asc]{
    \tau \Downarrow_\fvalCtx \tau'\\
	\atjanaX{e}{\tau'}{\iota}
}{
	\atjsynX{e : \tau}{\tau'}{\iota}
}

\inferrule[att-let-syn]{
	\atjsynX{e_1}{\tau_1}{\iota_1}\\
	\Gamma, x \Rightarrow \tau_1 \vdash_\fvalCtx e_2 \Rightarrow \tau_2 \leadsto \iota_2\\
	\trepof{\tau_1} \Downarrow_\fvalCtx \titype{\sigma_1}
}{
	\atjsynX{\F{let}x = e_1~\F{in}e_2}{\tau_2}{(\ilam{x}{\sigma_1}{\iota_2})~\iota_1}
}

\inferrule[att-lam-ana]{
	\Gamma, x \Rightarrow \tau_1 \vdash_\fvalCtx e \Leftarrow \tau_2 \leadsto \iota\\
	\trepof{\tau_1} \Downarrow_\fvalCtx \titype{\sigma_1}
}{
	\atjanaX{\lambda x.e}{\ttype{arrow}{(\tau_1, \tau_2)}}{\ilam{x}{\sigma_1}{\iota}}
}

%\inferrule[att-lam]{
%	\tau_1 \Downarrow_\fvalCtx \tau_1'\\
%		\trepof{\tau_1'} \Downarrow_\fvalCtx \titype{\sigma}\\\\
%	\Gamma, x \Rightarrow \tau_1' \vdash_\fvalCtx e \Rightarrow \tau_2 \leadsto \iota
%%	\iota \hookrightarrow_\fvalCtx \iota'\\
%%		\sigma \hookrightarrow_\fvalCtx \sigma'
%%	\ddbar{\fvar{Arrow}}{\fvalCtx}{\trepof{\tau_1'}}{\sbar_1}\\
%	%\delfromtau{$\Xi_0$}{\fvalCtx}{\tau_1'}{\sabs}\\\\
%}{
%	\atjsynX{\elam{x}{\tau_1}{e}}{\ttype{arrow}{(\tau_1', \tau_2)}}{\ilam{x}{\sigma'}{\iota}}
%}
%
\inferrule[att-intro-ana]{
	\vdash_\fvalCtx \FF{iana}(\fvar{tycon})=\taudef\\
	\taudef~\taut{opidx}~\taut{tyidx}~((\Gamma; e_1)? :: \ldots :: (\Gamma; e_n)? :: []) \Downarrow_\fvalCtx \titerm{\iota}\\
%	\trepof{\ttype{tycon}{\tauidx'}} \Downarrow_\fvalCtx \titype{\sigma}\\
		\trepof{\ttype{tycon}{\taut{tyidx}}} \Downarrow_\fvalCtx \titype{\sigma}\\
		\Gamma \vdash_\fvalCtx \iota : \sigma
}{
	\atjanaX{\FF{intro}[\taut{opidx}](e_1; \ldots; e_n)}{\ttype{tycon}{\taut{tyidx}}}{\iota}
}

%\inferrule[att-i-asc-ty]{
%	\atjanaX{I}{\tau}{\iota}
%}{
%	\atjsynX{I : \tau}{\tau}{\iota}
%}
%
%\inferrule[att-i-asc-tycon]{
%	\vdash_\fvalCtx \FF{isyn}(\fvar{tycon})=\taudef\\\\
%	\taudef~\tauidx~((\Gamma; e_1)? {::}{\ldots}{::}(\Gamma; e_n)? {::} []) \Downarrow_\fvalCtx \tden{\titerm{\iota}}{\ttype{tycon}{\tauidx'}}\\
%			\trepof{\tau_1'} \Downarrow_\fvalCtx \titype{\sigma}\\
%	\Gamma \vdash_\fvalCtx \iota : \sigma
%}{
%	\atjsynX{\FF{intro}[\tauidx](e_1; \ldots; e_n)] :: \fvar{tycon}}{\ttype{tycon}{\tauidx'}}{\iota'}
%}
%
\inferrule[att-elim-syn]{
	\atjsynX{e}{\ttype{tycon}{\taut{tyidx}}}{\iota}\\
	\vdash_\fvalCtx \FF{esyn}(\fvar{tycon})=\taudef\\\\
	\taudef~\taut{opidx}~\taut{tyidx}~\titerm{\iota}~((\Gamma; e)? :: (\Gamma; e_1)? :: \ldots :: (\Gamma; e_n)? :: []) \Downarrow_\fvalCtx (\tau, \titerm{\iota'})\\
			\trepof{\tau} \Downarrow_\fvalCtx \titype{\sigma}\\
	\Gamma \vdash_\fvalCtx \iota' : \sigma
}{
	\atjsynX{e\cdot\FF{elim}[\taut{opidx}](e_1; \ldots; e_n)}{\tau}{\iota'}
}
\end{mathpar}
\vspace{-10px}
\caption{\small The bidirectional active typechecking and translation judgements.}
\label{atj}
\end{figure}
\begin{figure}[t]
\small
$\fbox{\inferrule{}{\tau \Downarrow_\fvalCtx \tau'}}$~~~~
\begin{mathpar}
\inferrule[repof]{
	\tau \Downarrow_\fvalCtx \ttype{tycon}{\tauidx}\\
	\vdash_\fvalCtx \FF{rep}(\fvar{tycon}) = \taurep\\
	\taurep~\tauidx \Downarrow_\fvalCtx \titype{\sigma}
}{
	\FF{repof}(\tau) \Downarrow_\fvalCtx \titype{\sigma}
}

\inferrule[syn]{
    \tau \Downarrow_\fvalCtx (\Gamma; e)?\\
	\atjsynX{e}{\tau}{\iota}\\
    [\tau/\tvar{t}_{ty}, \titerm{\iota}/\tvar{t}_{trans}]\tau_2 \Downarrow_\fvalCtx \tau_2'
}{
	\FF{syn}(\tau_1; \tvar{t}_{ty}, \tvar{t}_{trans}.\tau_2) \Downarrow_\fvalCtx \tau_2'
}

\inferrule[ana]{
	\tau_1 \Downarrow_\fvalCtx (\Gamma; e)?\\
	\tau_2 \Downarrow_\fvalCtx \tau_2'\\
	\atjanaX{e}{\tau_2'}{\iota}\\
	[\titerm{\iota}/\tvar{t}_{trans}]\tau_3 \Downarrow_\fvalCtx \tau_3'
}{
	\FF{ana}(\tau_1; \tau_2; \tvar{t}_{trans}.\tau_3) \Downarrow_\fvalCtx \tau_3'
}
\end{mathpar}
\caption{\small Normalization semantics for the type-level language. Missing rules (including error propagation rules and normalization of quoted internal terms and types) are unsurprising and will be given later.}
\label{tleval}
\end{figure}
The rules for bidirectional active typechecking and translation are shown in Fig. \ref{atj}. 

%The expressive and metatheoretic power of the calculus arises from how the rules for the active typing judgement handle these  generalized forms. Rather than fixing the specification of a finite collection of operator constructors and tasking the \emph{compiler} with deciding a typing derivation on its basis, the specification instead delegates to a type-level function associated with a type constructor. In the core calculus, this is one of three functions, called \FF{iana}, \FF{isyn} and \FF{esyn} in the grammar.\footnote{This means that any particular type constructor only supports a single introduction and elimination desugaring. This is a minor inconvenience in some cases that is resolved in Ace by the use of methods.} We see how this is done with the rules for the active typing judgements, given in Figure \ref{atj}.
\begin{enumerate}
\item The rule $\textsc{att-flip}$ is standard in bidirectional type systems (without subtyping): if a term synthesizes a type, it can be analyzed against that type; the translation is unaffected by this.
\item The rule $\textsc{att-var}$ says that variables synthesize the type they have in the typing context and translate to variables in the internal language.
\item The rule $\textsc{att-asc}$ says that a term ascribed by a type synthesizes that type if the term can be  analyzed against that type, after it has been normalized. The normalization judgement for type-level terms is written $\tau \Downarrow_\fvalCtx \tau'$. The kinding rules (not shown here) guarantee that normalization of type-level terms cannot go wrong (we will refine what precisely this means later).
\item The rule $\textsc{att-let-syn}$ first synthesizes a type for the bound value, then adds this binding to the context and synthesizes a type for the term the binding is scoped over. The translation is to a function application, in the conventional manner. 

To do so, however, we must also translate the type itself so that an appropriate type annotation for the function argument can be emitted. Every type has an internal type associated with it called its \emph{representation type}. The type-level operator $\FF{repof}(\tau)$ evaluates to a \emph{quoted internal type}, $\titype{\sigma}$ (of kind $\kIType$), where $\sigma$ is the representation type of $\tau$. Type constructors define the representation type for every possible index by providing a type-level function called the \emph{representation schema}, written after the keyword $\FF{rep}$. The normalization rule showing this is given in Fig. \ref{tleval}. In our example, the representation schema is simple: natural numbers translate to integers. We will see an example where this is less trivial later. Note that quoted internal types support splicing using the $\dup{\tau}$ form. These forms are eliminated during normalization.
\item The rule $\textsc{att-lam}$ says that lambda terms can only be analyzed against arrow types and translate to lambda terms in the internal language.
\item The rule $\textsc{att-intro-ana}$ says that generalized introductory forms can only be analyzed against a type. That type constructor is consulted to extract its \emph{introductory operator definition}, written (e.g. in Figure \ref{nat-atfrag}) after the $\FF{iana}$ keyword as a type-level function. This function is given the provided operator index, the type's type index and a list encapsulating the operator's  \emph{arguments}. An argument is reified as a type-level term of the form $(\Gamma, e)?$ and has kind $\FF{Arg}$. Note that only the compiler should construct such a term (in practice, this could be enforced purely syntactically, so we do not enforce this judgmentally here). The operator definition is responsible for producing a translation, or evaluating to $\terr$ if this is not possible. A translation is simply a quoted internal term, written $\titerm{\iota}$, with kind $\kITerm$. Like quoted internal types, quoted internal terms support an unquote form, written $\iup{\tau}$, which is normalized away (in a capture-avoiding manner, not shown).

Natural numbers have two introductory forms, each indexed trivially. Because there is only one generalized introductory form, we instead index the operator by a simple sum kind with two trivial cases, $\kunit + \kunit$. The first case corresponds to the zero operator and the second to the successor. In the introductory operator definition, we see in the first case that the translation $0$ is produced after checking that no arguments were provided (using a simple helper function, $\tvar{arity0}$, not shown). In the second case, we first extract the single argument (using the helper function $\tvar{arity1}$, not shown). We then analyze it against the type $\ttype{nat}{\tunit}$ using one of the elimination forms for arguments, $\FF{ana}(\tau_1; \tau_2; \tvar{t}_{trans}.\tau_3)$. If it succeeds, we construct the appropriate translation based on the translation of the argument. The normalization rule for successful analysis, $\textsc{ana}$, is shown in Figure \ref{tleval}. 

Note that we do not show here the error propagation rules. We need two additional judgements for this: the judgement $\Gamma \vdash_\fvalCtx e~\mathtt{error}$ says that $e$ has a type error, and the judgement $\tau~\mathtt{err}_\fvalCtx$ says that normalizing $\tau$ produced an $\terr$ term. Appropriate error propagation rules need to be written down.

If analysis of the introductory form succeeds, the translation is checked for \emph{representational consistency}: that the produced translation has the type indicated by the type constructor's representation schema. This is expressed by essentially a standard typechecking judgement for the internal language, written $\Gamma \vdash_\fvalCtx \iota : \sigma$ (not shown). Note that the typing context will need to be translated to a corresponding typing context for the internal language. We will discuss this further below.

Notice that the assisted introductory form in Figure \ref{desugaring} allows us to define type-level variables $\tvar{z}$ and $\tvar{s}$ that simultaneously provide an operator index and type ascription, simulating introductory forms that synthesize a type without requiring support for such in the semantics (we will discuss a more expressive variant of the system in Sec. \ref{ace} where ascriptions can also be type constructors, not just types, permitting a form of synthetic introductory form).

Note also that lambda is technically the introductory form for the $\fvar{arrow}$ type constructor, but because it requires manipulating the context, it must be built in to @$\lambda$. We do not currently support type system fragments that require adding or manipulating existing contexts.

\item The rule $\textsc{att-elim-syn}$ says that elimination forms synthesize a type. This is done again by consulting a type-level function, called the \emph{eliminatory operator definition}, associated with a type constructor, here the type constructor of the type  recursively synthesized for the primary operand, $e$. This definition is invoked with the type index, the operator index, the translation of the primary operand and a list of arguments. Because elimination forms synthesize a type, this definition must produce both a type and a translation. Representational consistency is then checked.

In our example, the elimination form for natural numbers is the recursor (shown in Fig. \ref{natfrag}). Our definition of it first checks that exactly two arguments were provided (not including the primary operand), then synthesizes a type and extracts a translation from the first argument using the form $\FF{syn}(\tau_1; \tvar{t}_{ty}, \tvar{t}_{trans}.\tau_2)$ (Fig. \ref{tleval}). The second argument is analyzed against an appropriate arrow type to support variable binding. If successful, the type $\tvar{t1}$ (the type of the base case) is synthesized and a translation implementing the dynamic semantics of the recursor by a fixpoint computation is produced.

The assisted elimination form shown in Fig. \ref{desugaring} provides a means to avoid providing an operator index explicitly. Here, we simply define $\tvar{natrec}$ as the trivial value to avoid needing to write it explicitly (and for clarity).

Note that the elimination form for the $\fvar{arrow}$ type constructor can be defined in this manner (because it does not require binding variables), so application is simply syntactic sugar, rather than a built-in operator in the external language.
\end{enumerate}

%The rule $\fvar{att-lam}$ must take into account the fact that, because we support type-level computation, the type annotation on the argument may not be in normal form. Thus, we evaluate it to normal form. Note that the kinding rules (not shown) will guarantee that, because $\tau_1$ is of kind $\kTypeBlur$, its normal form, $\tau'_1$, is of the form $\ttype{tycon}{\tauidx}$ for some normal $\tauidx$. To generate an appropriate internal type annotation in the translation, we need to compute the representation type of $\tau_1'$. This involves calling the representation function associated with its type constructor (rule \fvar{repof} in Figure \ref{tleval}). The form $\titerm{\sigma}$ is a \emph{quoted internal type}. Note in the syntax that there is an unquote form, $\dup{\tau}$, which allows us to compose internal types compositionally without needing to expose an elimination form. Indeed, it is an interesting facet of our calculus that we never need to examine syntax trees directly to implement extensions. The evaluation semantics remove quotations, so the normal form of a quoted internal type contains an internal type with no quotations.
%
%Lambda functions are the only introductory form requiring special support in the calculus (because they need to  manipulate the context; see Discussion). The next three rules show how any other abstractions that we define (e.g. records, decimals, etc.) make use of a generalized introductory form. 
%
%The rule  $\fvar{att-i-unasc}$ shows that unascribed introductory forms can only be analyzed against a type. Given such a type, the rule extracts a definition, named $\FF{iana}$, from the type constructor (cf. the \verb|ana_Dict| and \verb|trans_Dict| methods earlier). It calls this function with a pair containing the operator and type index and a list of \emph{reified arguments}, which have kind $\Q$ and introductory form $(\Gamma; e)?$. These are only constructed by the compiler (there would be no corresponding form in the concrete syntax). Their purpose is to allow the definition to programatically invoke synthesis and analysis as needed using the $\FF{ana}$ and $\FF{syn}$ operators. We can see the $\fvar{record}$ type constructor doing so on line 5 of Figure \ref{record-theory} to ensure that the field value provided as an argument has the same type as the corresponding label (accomplished by simultaneously folding over all three pieces of input data). The rule for performing analysis, $\fvar{ana}$, is in Figure \ref{tleval}. If it succeeds, analysis returns a \emph{translation}, which is a quoted internal term with kind $\kITerm$ and introductory form $\titerm{\iota}$. Like quoted internal types, there is an unquote form that is eliminated during evaluation. The operator definition uses this to construct a translation for the record. As before, empty records translate to units and records with a single field are unadorned. In this example, records with two or more fields translate to nested tuples. 
%
%Because we have a representation type associated with the type, we can check that the translation is \emph{representationally consistent} (the final premise). As we will discuss, representational consistency combined with type safety of the internal language implies type safety overall (it arises as a strengthening of the inductive hypothesis needed to prove type safety, and is closely related to work on \emph{type-preserving compilation} in the TIL compiler for Standard ML, \cite{tarditi+:til-OLD}).
%
%The next rule, $\fvar{att-i-asc-ty}$ states that introductory forms ascribed with a type are analyzed against that type.
%
%Introductory forms ascribed with a type constructor can, according to the final introductory rule, $\fvar{att-i-asc-tycon}$, synthesize a type via the definition $\FF{isyn}$. It is passed the operator index (there is no type index, since we only have a type constructor) and a list of arguments, as before. In this case, it must return not just a translation, but also a type. We use the form $\tden{\tau_2}{\tau_1}$ for such a pairing, which has kind $\FF{TT}$. The type and translation can be extracted from it using the appropriately named elimination forms (indeed, as given, it is merely a pair, but we give it special syntax for clarity -- it looks like the conclusion of the typing judgement -- and for other reasons that will become clear in future work). To synthesize a type from a list of labels and arguments, we must be able to synthesize types from the arguments. The $\FF{syn}(\tau)$ operator permits this, per rule $\fvar{syn}$, seen being used in Listing \ref{record-theory}. We check representational consistency of the result, as before.
%
%Finally, we show the rule for elimination forms. It operates by first synthesizing a type for the ``primary'' subterm, then extracting the $\FF{esyn}$ definition from its type constructor. As before, it is called with the arguments and representational consistency is checked. Note that the primary subterm is itself the first argument (though we have already synthesized a type for it, it is more uniform and clear to allow the operator  to do so again, which is done in our example by the $\tvar{arity1}$ helper function).

\subsection{Representational Consistency Implies Type Safety}\label{repcon}
A key invariant that our operator definitions maintain is that well-typed external terms of type $\ttype{Nat}{\tunit}$ always translate to well-typed internal terms of internal type $\dint$, the representation type of $\ttype{Nat}{\tunit}$. Verifying this for the zero case is simple. For the translation produced by the successor case to be of internal type $\dint$ requires that $\iup{\tvar{x}}$ be of internal type $\dint$. Because it is the result of analyzing an argument against $\ttype{Nat}{\tunit}$, and the only other introductory form is the zero case, this holds inductively. We will discuss the recursor later, but it also maintains this invariant inductively.

This invariant would not hold if, for example, we allowed the type and translation: $$({{\ttype{Nat}{\tunit},\titerm{(0, ())}}})$$
\noindent
In this case, there would be two different internal types, $\dint$ and $\dpair{\dint}{\dunit}$,  associated with a single external type, $\ttype{Nat}{\tunit}$. This would make it impossible to reason \emph{compositionally} about the translation of an external term of type $\ttype{Nat}{\tunit}$, so our implementation of the successor would produce ill-typed translations in some cases but not others. Similarly, we wouldn't be able to write functions over all natural numbers because there would not be a well-typed translation to give to such a function. This violates type safety: there are now well-typed external terms, according to the active typechecking and translation judgement, for which evaluating the corresponding translation would ``go wrong''. 

To reason compositionally about the semantics of well-typed external terms when they are given meaning by translation to a typed internal language, the system must maintain the following property: for every  type, $\tau$, there must exist an internal type, $\sigma$, called its \emph{representation type}, such that the translation of every external term of type $\tau$ has internal type $\sigma$. This principle of \emph{representational consistency} arises essentially as a strengthening of the inductive hypothesis necessary to prove that all well-typed external terms translate to well-typed internal terms because $\lambda$ and successor are defined compositionally. It is closely related to the concept of \emph{type-preserving compilation} developed by Morrisett et al. for the TIL compiler for SML \cite{tarditi+:til-OLD}, here lifted into the language. Our judgements check this extension correctness property directly by typechecking each translation produced by a user extension (the translations of, for example, lambda terms will be inductively representationally consistent, so no additional check is needed).%That is, instead of being used as a necessary condition for compiler correctness, we are using it as a sufficient condition for type safety. 
%We emphasize the distinction between translation (which in our calculus endows terms with a dynamic semantics) and compilation (which must preserve the dynamic semantics of terms). %In the presence of extensions, checking for representational consistency becomes subtle if we wish to guarantee conservativity, as we will discuss in the next subsection.

%For the semantics to ensure that representational consistency is maintained by all operator definitions, we require, as briefly mentioned in Sec. \ref{types}, that each type constructor declare a \emph{representation schema} after the keyword \textsf{schema}. This is a type-level function of kind $\karrow{\kappaidx}{\kIType}$, where $\kappaidx$ is the index kind of the type constructor. The kind $\kIType$ has introductory form $\titype{\bar \sigma}$ and no elimination form, and is similar to the kind $\kITerm$ introduced above. There are two forms of \emph{translational internal type}, $\bar \sigma$, that do not correspond to forms in $\sigma$. As with translational internal terms, these are included to allow an internal type to be formed compositionally:
%\begin{enumerate}
%\item $\dup{\tau}$ splices in another translational internal type $\tau$
%\item $\trepof{\tau}$ refers to the representation type of type $\tau$
%\end{enumerate}
%
%When the semantics (i.e. the compiler) needs to determine the representation type of the type $\ttype{Tycon}{\tauidx}$ it simply applies the representation schema of $\fvar{Tycon}$ to ${\tauidx}$. Note that the representation type of a type cannot be extracted directly from within the type-level language, again for reasons that we will discuss below. 
%
%These additional forms are not needed by the representation schema of $\fvar{Nat}$  because it is trivially indexed. In Fig. \ref{tuple}, we will discuss an example of the type constructor $\fvar{Tuple}$, implementing the semantics of $n$-tuples by translation to nested binary products. Here, the representation schema refers to the representations of the tuple's constituent types and is computed by folding over a list, so both of these are used\todo{add this example from appendix of ESOP}. 

Translational internal terms can contain translational internal types. We see this in the definition of the recursor on natural numbers. The type assignment is the arbitrary type, $\tvar{t2}$. We cannot know what the representation type of $\tvar{t2}$ is, so we refer to it abstractly using $\trepof{\tvar{t2}}$. Luckily, the proof of representational consistency of this operator is parametric over the representation type of $\tvar{t2}$. 

\subsection{Metatheory}\label{theory}
%\newcommand{\F}[1]{\textsf{#1}~}
%\newcommand{\FF}[1]{\textsf{#1}}
%\newcommand{\Q}{\FF{Arg}}
%\renewcommand{\tnil}[1]{[]}
%\begin{figure}[t]
%\small
%$$\begin{array}{rcl}	
%%\textbf{programs} & 
%\rho & ::= & \F{tycon}\fvar{tycon}~\F{of}\kappaidx~{\{}TC{\}}; \rho \pipe e\\ %\pfam{\familyDf}{\progsort}
%%\\&  \pipe & \pdef{t}{\kappa}{\tau}{\progsort}  \pipe e
%%\pipe \pdef{t}{\kappa}{\tau}{\progsort} 
% TC & ::= & \F{iana} \{\tau\}; \F{isyn} \{\tau\}; \F{esyn} \{\tau\}; \F{rep} \{\tau\}\\
%\\ 
%e & ::= & \evar{x} \pipe \elam{\evar{x}}{\tau}{e} \pipe I \pipe I : \tau \pipe I :: \fvar{tycon} \pipe E\\
%I & ::= &  \FF{intro}[\tauidx](\splat{e}{1}{n})\\
%E & ::= & e\cdot\FF{elim}[\tauidx](\splat{e}{1}{n})\\
%\\
% \tau 	& ::= 	& 	\tvar{t} \pipe 
%														\tlam{t}{\kappa}{\tau} \pipe 
%														\tapp{\tau_1}{\tau_2}  
%\\&\pipe&											
%														\tnil{\kappa} \pipe \tcons{\tau_1}{\tau_2} \pipe 
%									                     \tfold{\tau_1}{\tau_2}{h}{t}{r}{\tau_3}
%														\\
% 		& \pipe	& 	  \ell \pipe 
%		\tunit \pipe 
%														\tpair{\tau_{1}}{\tau_{2}} \pipe 
%														\tfst{\tau} \pipe 
%														\tsnd{\tau} \pipe \cdots 
%														\\	
%%     & \pipe & \tinl{\kappa_2}{\tau_1} \pipe \tinr{\kappa_1}{\tau_2} \\
%%     &\pipe& \tsumcase{\tau}{t}{\tau_1}{t}{\tau_2} \\
%&\pipe	& 	\ttype{tycon}{\tauidx} \\&\pipe& \tfamcase{\tau}{tycon}{x}{\tau_1}{\tau_2}\\
%&\pipe & 					\tifeq{\tau_{1}}{\tau_{2}}{\kappa}{\tau_{3}}{\tau_{4}} 
%%														\\													%				& & \pipe & \tfamcase{\tau}{Fam}{x}{\tau_1}{\tau_2}\\
%%																								
%\\ 		 & 	\pipe	&	\tden{\tau_2}{\tau_1} \pipe \terr %\pipe \tden{\ibar}{\tau}^{\checkmark} 
%\pipe \ttypeof{\tau} \pipe \itransof{\tau} \\% \tdencase{\tau}{x}{t}{\tau_1}{\tau_2}\\
%% %& & \pipe & 
%%%														\tdencase{\tau}{y}{x}{\tau_1}{\tau_2}
%%%														 \\
%%
% & \pipe & \titerm{\iota}
%			\pipe	\titype{\sigma} \pipe \trepof{\tau} \\
%		& \pipe &  (\Gamma; e)? \pipe \FF{syn}(\tau) \pipe \FF{ana}(\tau; \tau')\\
%%\textbf{external terms} 				&	e	&	::=	&	\evar{x} \pipe 
%%%														\efix{x}{\tau}{e} \pipe 
%%														\elam{\evar{x}}{\tau}{e} \pipe 
%%														\eop{tycon}{op}{
%%															\tauidx
%%														}{
%%  												    		\splat{e}{1}{n}
%%														} \\
%%									& 		&		& 	\\
%%
%%
%%												
%%%\text{deabstracted}& \iota & ::= & \mathcal{G}[\iota, \sigma]\\
%%												
%%\\
%%\\
%%							
%%% &  & \pipe & \tvalof{\tau_1}{\tau_2} \pipe \iup{\tau} \\
%%% 												\trepof{\tau} \pipe \dup{\tau}\\
%%\text{translational IL}	& \bar{\iota} & ::= & x \pipe \ifix{x}{\bar \sigma}{\bar \iota} 
%%	%\pipe \ilam{x}{\bar \sigma}{\bar \iota} \pipe \iapp{\bar \iota_1}{\bar \iota_2} 
%%	\pipe \cdots \pipe \itransof{\tau} \\						
%% & \bar{\sigma} & ::= & \darrow{\bar \sigma_1}{\bar \sigma_2} \pipe \cdots \pipe \dup{\tau} \pipe \trepof{\tau} \\
%%%\text{abstracted} & \sabs & ::= & \darrow{\sabs_1}{\sabs_2} \pipe \cdots \pipe \sabsrep{\tau}
%%											\\
%\kappa	&	::=	&	\karrow{\kappa_1}{\kappa_2} \pipe \klist{\kappa} \pipe \FF{L} \pipe \kunit \pipe \kpair{\kappa_1}{\kappa_2} \pipe \cdots \\
% & \pipe & \kTypeBlur \pipe \FF{TT} \pipe \kITerm \pipe \kIType \pipe \Q
%%											    \klabel \pipe
%%											    \klist{\kappa} \pipe
%%												\kunit \pipe 
%%												\kpair{\kappa_{1}}{\kappa_{2}} \\
%%												&&\pipe&
%%												\ksum{\kappa_1}{\kappa_2} \pipe
%%												\kTypeBlur \pipe \kDen \pipe 
%%												\kIType								
%%%\textbf{ops signature}			& \Theta	&	::=	&	\kOpEmpty \pipe \kOp{\Theta}{op}{\kappai}\\
%%%											 							&		&		&	\\
%\\
%\\
%\iota	&	::=	&	\evar{x} \pipe 
%												\ifix{\evar{x}}{\sigma}{\iota} \pipe
%												\ilam{\evar{x}}{\sigma}{\iota} \pipe 
%												\iapp{\iota_{1}}{\iota_{2}} 
%												\\ & \pipe & \iunit \pipe \ipair{\iota_1}{\iota_2} \pipe \ifst{\iota} \pipe \isnd{\iota} \pipe \cdots \pipe \iup{\tau}
%%\\&\pipe&								\cdots \pipe 	
%%												n \pipe \iop{\iota_{1}}{\iota_{2}} \pipe \iIfEq{\iota_{1}}{\iota_{2}}{\dint}{\iota_{3}}{\iota_{4}} \\
%%& \pipe & 
%%												\iunit \pipe
%%												\ipair{\iota_{1}}{\iota_{2}} \pipe 
%%												\ifst{\iota} \pipe
%%												\isnd{\iota} 
%%\\&\pipe&												
%%												 \iinl{\sigma_2}{\iota_1} \pipe \iinr{\sigma_1}{\iota_2} \\&\pipe & \icase{e}{x}{e_1}{x}{e_2} \\
%\\	\sigma	&	::=	&    \darrow{\sigma_1}{\sigma_2} \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} %\pipe \dint \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} \pipe \dsum{\sigma_1}{\sigma_2}  
%\pipe \cdots \pipe \dup{\tau}
%\end{array}$$
%%\vspace{-10pt}
%\caption{\small Syntax of Core $\lamAce$. Here, $x$ ranges over external and internal language variables, $\tvar{t}$ ranges over type-level variables, $\fvar{tycon}$ ranges over type constructor names and $\ell$ ranges over labels.
%\label{grammar}}
%\end{figure}
%\begin{figure}
%\small
%\begin{flalign}
%& \F{tycon}\fvar{record}~\F{of}\klist{\kpair{\FF{L}}{\kTypeBlur}}~\{\\
%& \quad \FF{iana}~\{\tlam{i}{\kpair{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{\klist{\FF{L}}}}{\tlam{a}{\klist{\Q}}{\\& \quad\quad\tfold{\tvar{zip3}~\tfst{\tvar{i}}~\tsnd{\tvar{i}}~\tvar{a}}
%	 {\titerm{\tunit}}{h}{t}{r}{
%	\\&\quad\quad\quad\tifeq{\tfst{\tvar{first}~\tvar{h}}}{\tvar{second}~\tvar{h}}{\FF{L}}{
%	\\&\quad\quad\quad\quad
%	\F{let}~\tvar{x}:\kITerm~=~\FF{ana}(\tvar{third}~\tvar{h}; \tsnd{\tvar{first}~\tvar{h}})
%~\FF{in}
%\\ & \quad\quad\quad\quad \tfold{\tvar{t}}{\tvar{x}
%}{\_}{\_}{\_}{\titerm{(\iup{\tvar{x}},\iup{\tvar{r}})}}	}{\terr}
%%     \F{let}~\tvar{tt}:\FF{TT}=~\FF{syn}(\tsnd{\tsnd{\tvar{h}}}) \F{in}\\
%%\\     & \quad\quad\quad 
%}
%}}\}\\
%& \quad \FF{isyn}~\{\tlam{i}{\klist{\FF{L}}}{\tlam{a}{\klist{\Q}}{
%\\&\quad\quad \tfold{\tvar{zip2}~\tvar{i}~\tvar{a}}{\tden{\titerm{()}}{\ttype{record}{\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}}}{h}{t}{r}{
%\\&\quad\quad\quad \F{let}~\tvar{htt}:\FF{TT}~=~\FF{syn}(\tsnd{\tvar{h}})
%~\FF{in}
%\\&\quad\quad\quad \F{let}~\tvar{hty}:\kTypeBlur~=~\ttypeof{\tvar{htt}}
%\\&\quad\quad\quad \F{let}~\tvar{ty}:\kTypeBlur~=~\ttype{record}{(\tfst{\tvar{h}}, \tvar{hty})::\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}
%~\FF{in}
%\\&\quad\quad\quad \tfold{\tvar t}{\tden{\itransof{\tvar{htt}}}{\tvar{ty}}}{\_}{\_}{\_}{
%\\&\quad\quad\quad\quad \tden{\titerm{(\iup{\itransof{\tvar{htt}}},\iup{\itransof{\tvar{r}}})}}{\tvar{ty}}}}
%}}\}\\
%& \quad \FF{esyn}~\{\tlam{i}{\FF{L}}{\tlam{a}{\klist{\Q}}{
%	\tvar{arity1}~\tvar{a}~\tlam{ty}{\kTypeBlur}{\tlam{x}{\kITerm}{
%\\&\quad\quad		\tfamcase{\tvar{ty}}{record}{sig}{
%\\&\quad\quad\quad \tfold{\tvar{sig}}{\terr}{h}{t}{r}{
%\\&\quad\quad\quad\quad \tifeq{\tfst{\tvar{h}}}{\tvar{i}}{\FF{L}}{\tfold{\tvar{t}}{\tden{\tvar{x}}{\tsnd{\tvar{h}}}}{\_}{t'}{\_}{
%\\&\quad\quad\quad\quad\quad \tfold{\tvar{t'}}{\tden{\titerm{\ifst{\iup{\tvar{x}}}}}{\tsnd{\tvar{h}}}}{\_}{\_}{\tvar{r'}}{
%\\&\quad\quad\quad\quad\quad\quad \tden{\titerm{\isnd{\iup{{\itransof{\tvar r}}}}}}{\tsnd{\tvar{h}}}}}\\&\quad\quad\quad\quad\hspace{-3px}}{\tvar{r}}
%}\\&\quad\quad\hspace{-3px}}{\terr}
%	}}
%}}\\
%& \quad \FF{rep}~\{\tlam{i}{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{ 	\\& \quad \quad \tfold{\tvar{i}}{\titype{\dunit}}{s}{j}{r}{
% 		\tfold{\tvar{j}}{\trepof{\tsnd{\tvar{s}}}}{\_}{\_}{\_}{\\
%		&\quad\quad\quad
% 		\titype{\dpair{\dup{\trepof{\tsnd{\tvar{s}}}}}{\dup{\tvar{r}}}}
% 		}
% 	}}\}\\
%& \}; \\
%& \F{let}\tvar{R} : \kTypeBlur = \ttype{record}{(\ell_1, \ttype{record}{\tnil{{\FF{L}\times\kTypeBlur}}}) :: \tnil{{\FF{L}\times\kTypeBlur}}}~\F{in}\\
%&\F{let}id = \elam{x}{\tvar{R}}{\{\ell_1=~x\cdot\ell_1\} :: \fvar{record}}\\
%&\F{let}triv = id[\{\ell_1 = \{\}\}]\cdot\ell_1
%%&\F{let}one = \FF{intro}[\ell_\text{succ}]
%%&\F{let}plus = \elam{x}{\tvar{nat}}{\elam{y}{\tvar{nat}}{\FF{elim}[()](x; y; \\
%%& \quad \elam{r}{\tvar{nat}}{intro[\ell_\text{succ}](r) : \tvar{nat}}})}
%\end{flalign}
%\caption{The definition of the record type in $\lamAce$ using the desugarings in Figure \ref{desugaring} and with the addition of simple let bindings for both type and term variables (not shown). Some type-level helper functions also omitted for concision.}
%
%\label{record-theory}
%\end{figure}
%We will now give a core typed lambda calculus, $\lamAce$, that captures the semantics described in the previous sections. It is intended to make precise how active typechecking and translation works and how our mechanism relates to existing work on bidirectional typechecking, type-level computation and typed compilation while abstracting away from the details of Python's syntax and imposing a stronger type-level semantics that will allow us to state metatheoretic properties of interest. We will assume a fixed base for functions (providing the standard semantics of lambda functions) and target language, which we here call the \emph{internal language}.
%
%The syntax of $\lamAce$ is shown in Figure \ref{grammar} and an example \emph{program} that defines a type constructor, $\fvar{record}$, with a semantics similar to that given in Listing \ref{record}\footnote{Technically, this defines labeled tuples, because the order of labels matters.}, using it to write an identity function and compute the value $triv$ (the empty record), is shown in Figure \ref{record-theory}.
%
%%In this section, we will develop an ``actively typed'' version of the simply-typed lambda calculus with simply-kinded type-level computation called $\lamAce$. More specifically, the level of types, $\tau$, will itself form a simply-typed lambda calculus. \emph{Kinds} classify type-level terms in the same way that types conventionally classify expressions. Types become just one kind  of type-level value (which we will write $\kTypeBlur$, though it is also variously written $\star$, \verb|T| and \verb|Type| in various settings). Rather than there being a fixed set of type and operator constructors, we allow the programmer to declare new  constructors, and give their static and dynamic semantics by writing type-level functions. The kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction will allow us to prove strong type safety, decidability and conservativity theorems.
%%
%%The syntax of Core $\lamAce$ is given in Fig. \ref{grammar}. An example of a program defining type and operator constructors that can be used to construct an active embedding of G\"odel's \textbf{T} into $\lamAce$ is given in Fig. \ref{nat}. We will discuss its semantics and how precisely the embedding, seen being used starting on line 15 to ultimately compute the sum of two and two, works as we go on. Natural numbers can, of course, be isomorphically embedded in existing languages, with a similar usage and asymptotic performance profile (up to function call overhead as an abstract type, for example). We will provide more sophisticated examples where this is less feasible later on (and note that type abstraction is an orthogonal mechanism).
%%a type constructor declaration, $\fvar{Nat}$, indexed trivially, together with three operator constructors, also all indexed trivially, that implement the standard introductory forms for natural numbers as well as the recursor operator (as in G\"odel's T \cite{pfpl}). Following the type constructor declaration, we apply $\fvar{Nat}$ with the trivial index, $\tunit$, to form the type $\tvar{nat}$. Finally, we write an external term that uses the operators associated with $\fvar{nat}$ and the built-in constructor $\fvar{Parr}$, governing partial functions, to define an addition function and compute the addition of the natural numbers  two and two. We will introduce a more convenient concrete syntax in later portions of this thesis; for now we will restrict ourselves to the abstract syntax so that this example can directly aid in understanding the semantics.
%
%\subsection{Overview}\label{programs}
%
%A \emph{program}, $\rho$, consists of a series of constructor declarations followed by an external term, $e$.  Compiling a program consists of first \emph{kind checking} it (see below), then typechecking the external term and simultaneously {translating} it to a term, $\iota$, in the {typed internal language}. 
%%These correspond to the premises of the \emph{central compilation judgement} $\pkcompiles{\rho}{\iota}$:
%%\[
%%\inferrule[p-compiles]{
%%	\emptyset \vdash_{\fvalCtx_0} \rho\\
%%%	\progOK{\emptyset}{\fvalCtx_0}{\rho}\\
%%	\pcompiles{\fvalCtx_0}{\rho}{\iota}
%%}{\pkcompiles{\rho}{\iota}}
%%\]
%%This anchors our exposition; we will describe how it is derived (i.e. how to write a compiler for $\lamAce$) in the following sections. 
%The key judgements are the \emph{bidirectional active typing judgements}  (Fig. \ref{att}, which we describe starting in Sec. \ref{opcons}). They relate an external term, $e$, to a {type}, $\tau$, called its \emph{type assignment}, and an internal term, $\iota$, called its \emph{translation}, under \emph{typing context} $\Gamma$ and \emph{constructor context} $\Phi$. The first is synthesis, the second analysis.
%\[\Gamma \vdash_\fvalCtx e \Rightarrow \tau \leadsto \iota
%~~~~~~~
%\Gamma \vdash_\fvalCtx e \Leftarrow \tau \leadsto \iota\]
%%\[\ecompilesAX{e}{\tau}{\iota}\]
%
%The typing context, $\Gamma$, maps variables to types in essentially the conventional way (\cite{pfpl} contains the necessary background for this section). The constructor context, $\Phi$, tracks user-defined type  constructors. % Weakening and exchange of the constructor context closely related to the issue of conservativity that we will return to. Each constructor is identified by name, so there is no analog to contraction.
%
%The dynamic behavior of an external term is determined entirely by its translation to the internal language, which has a conventional operational semantics. This form of semantics can be seen as lifting into the language specification the first stage of a type-directed compiler like the TIL compiler for Standard ML \cite{tarditi+:til-OLD} and has some parallels to the Harper-Stone semantics for Standard ML, where external terms were also given meaning by elaboration from the EL to an IL \cite{Harper00atype-theoretic}. %
%
%In $\lamAce$, the internal language (IL) provides partial functions (via the generic fixpoint operator of Plotkin's PCF) and simple product  types for the sake of our example. In practice, the internal language could be any typed  language with a specification for which type safety and decidability of typechecking have been satisfyingly determined. The internal type system serves as a ``floor'': guarantees that must hold for terms of any type (e.g. that out-of-bounds access to memory never occurs) must be maintained by the internal type system. User-defined constructors can enforce invariants stronger than those the internal type system maintains at particular types, however. Performance is also ultimately limited by the internal language and downstream compilation stages that we do not here consider (safe compiler extension has been discussed in previous work, e.g. \cite{conf/pldi/TatlockL10}).
%
%
%\subsection{Types and Type-Level Computation}\label{types}
%$\lamAce$ supports, and makes extensive use of, simply-kinded type-level computation. Specifically, type-level terms, $\tau$, themselves form a typed lambda calculus. The classifiers of type-level terms are called \emph{kinds}, $\kappa$, to distinguish them from  \emph{types}. Types are  type-level values of kind $\kTypeBlur$. In Ace, the type-level language (together with the level of programs) is written in Python, which can be thought of as having a rather limited kind system (with one kind, \verb|dyn|). Here, we are able to more precisely discuss the kinds of values in the type-level language. 
%
%In most languages, types are formed by applying one of a collection of \emph{type constructors} to zero or more \emph{indices}. In $\lamAce$, the situation is notionally similar. User-defined type constructors can be declared at the top of a program (or lifted to the top, in practice) using \textsf{tycon}. Each constructor in the program must have a unique name, written e.g. \fvar{record}.\footnote{We assume naming conflicts can be avoided by some extrinsic mechanism.} A type constructor must also declare an \emph{index kind}, $\kappaidx$. 
%
% To permit the embedding of interesting type systems, the type-level language includes several kinds other than $\kTypeBlur$. We lift several functional data structures to the type level: here, only unit ($\kunit$), binary products ($\kpair{\kappa_1}{\kappa_2}$) and lists ($\klist{\kappa}$), in addition to labels (introduced as $\ell$, possibly with a subscript, having kind \FF{L}).  Our record type constructor is indexed by a list of pairs of {labels}  and types (a signature, in essence; line 1). The type constructor $\fvar{arrow}$ is included in the initial constructor context, $\fvalCtx_0$ and has index kind $\kpair{\kTypeBlur}{\kTypeBlur}$.
% In practice, one could include a richer functional programming language and retain the spirit of the calculus, as long as it does not introduce general recursion at the type level. 
%  
%A type is introduced by applying a type constructor to an index of this kind, written $\ttype{Tycon}{\tauidx}$.  For example, the type of natural numbers is indexed trivially, so it is written $\ttype{Nat}{\tunit}$. We see a record type, abbreviated $\tvar{R}$, constructed on line 26. 
%
%The kind $\kTypeBlur$ also has an elimination form, $$\tfamcase{\tau}{Tycon}{x}{\tau_1}{\tau_2}$$ allowing the extraction of a type index by case analysis against a contextually-available type constructor. To a first approximation, one might think of type constructors as constructors of a built-in open \cite{conf/ppdp/LohH06}, $\kTypeBlur$, at the type-level. Like open datatypes, there is no notion of exhaustiveness so the default case is required for totality. We will see where this is used shortly.
%
%
%
%%We will write the kind of types as $\kTypeBlur$, though it is also written $\star$ or \verb|Type| in various similarly structured languages (see Sec. \ref{related-work}). 
%% Rather than there being a fixed set of type constructors, we allow the programmer to declare new type  constructors, and give the static and dynamic semantics of their associated operators, by writing type-level functions. In the semantics for this calculus, our kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction allow us to prove strong type safety, decidability and conservativity theorems.
%
%
%
%% and integers ($\dint$). We also include labels ($\klabel$), written in a slanted font, e.g. $\tlabel{myLabel}$, which are string-like values that only support comparison and play a distinguished role in the expanded syntax, as we will later discuss. Our first example, $\fvar{nat}$, is indexed trivially, i.e. by unit kind, $\kunit$, so there is only one natural number type, $\ttype{nat}{\tunit}$, but we will show examples of type constructors that are indexed in more interesting ways in later portions of this work. For example, $\fvar{LabeledTuple}$ has index kind $\klist{\kpair{\klabel}{\kTypeBlur}}$. 
% 
% Type constructors are not first-class; they do not themselves have arrow kind as in some kind systems (e.g.  \cite{watkins2008specifying}; Ch. 22 of \emph{PFPL} describes a related system \cite{pfpl}). The type-level language does, however, include total functions of arrow kind, written $\karrow{\kappa_1}{\kappa_2}$. Type constructor application can be wrapped in a type-level function to emulate a first-class or uncurried version of a type constructor for convenience (indeed, such a wrapper could be generated automatically, though we do not do so). 
%
%Two type-level terms of kind $\kTypeBlur$ are equivalent if they apply the same constructor, identified by name, to equivalent indices. Going further, we ensure that deciding type equivalence requires only checking for syntactic equality after normalization by imposing the restriction that equivalence at a type constructor's index kind must be decidable in this way. Our treatment of equivalence in the type-level language is thus quite similar to the treatment of term-level equality using ``equality types'' in a language like Standard ML.
%% A kind $\kappa$ is an  \emph{equality kind} if $\kEq{\kappa}$ can be derived (see appendix). 
%Conditional branching on the basis of equality at an equality kind can be performed in the type-level language. Equivalence at arrow kind is not decidable by our criteria, so type-level functions cannot appear within type indices. This also prevents general recursion from arising at the type level. Without this restriction, a type-level function taking a type as an argument could ``smuggle in'' a self reference as a type index, extracting it via case analysis (continuing our analogy to open datatypes, this is closely related to the positivity condition for inductive datatypes in total functional languages like Coq).% as maintaining the metatheoretic guarantee that typing respects type equivalence would impose a substantial burden in such a setting.% (a na\"ive approach to this would impose non-trivial extrinsic proof obligations onto extension developers that, unlike in others in this thesis, could threaten type safety).
%
%Every type constructor also defines a \emph{representation}, a type-level function that associates with every type an internal \emph{representation type} (analagous to \verb|trans_type| above). We will return to this after introducing the external forms and operator definitions.
%
%\subsection{Core External Forms and Desugaring}\label{opcons}
%The syntax for external terms (Figure \ref{grammar}) contains variables, $\lambda$ terms, three generalized introductory forms and a single generalized elimination form. The introductory forms are either unascribed, ascribed with a type or ascribed with a type constructor, as in our discussion of Ace. These generalized forms take a single a type-level value as an {index} and $n \geq 0$ arguments, which are other external terms. To better motivate this choice, we can give a purely syntactic desugaring of a Python-like syntax with labels to these forms, shown in Figure \ref{desugaring}. It is instructive to rewrite lines 27-28 of Figure \ref{record-theory} using these desugarings:% In Ace, desugarings can be user-defined 
%\noindent
%\vspace{-5px}
%$$
%\begin{array}{l}
%\FF{let}~id=\lambda x:\tvar{R}.\FF{intro}[\ell_1 :: []](x\cdot\FF{elim}[\ell_1]()) :: \fvar{record}\\
%\FF{let}~triv=id\cdot\FF{elim}[()](\FF{intro}[\ell_1 :: []](\FF{intro}[[]]()))\cdot\FF{elim}[\ell_1]()
%\end{array}
%$$
%\vspace{-5px}
%\begin{figure}[t]
%\small
%\[
%\begin{array}{rcl}
%\{ \ell_1{=}~e_1, \ldots, \ell_n{=}~e_n \} & := & \FF{intro}[\ell_1 :: \ldots :: \ell_n :: []](e_1; \ldots; e_n)\\
%(e_1, ..., e_n) & := & \FF{intro}[()](e_1; \ldots; e_n)\\
%\#n & := & \FF{intro}[n]()\\
%"s" & := & \FF{intro}[s]()\\
%e.\ell & := & e\cdot\FF{elim}[\ell]()\\
%e[e_1; \ldots; e_n] & := & e\cdot\FF{elim}[()](e_1; \ldots; e_n)\\
%e.\ell(e_1; ...; e_n) & := & e\cdot\FF{elim}[\ell](e_1; \ldots; e_n)
%%(e_1, ..., e_n) & := & intro[()](e_1; ...; e_n)
%%n &  := & intro[n]()
%%s & := & intro[s]()
%%
%%e.l & := & elim[l](e)
%%e[e_1; ...; e_n] & := & elim[()](e_1, ..., e_n)
%%e.l(e_1; ...; e_n) & := & elim[l]
%\end{array}
%\]
%\caption{Desugaring from conventional syntax to core forms. We assume that the type-level language has numbers, $n$, and strings, $s$ (not shown for concision).}
%\label{desugaring}
%\end{figure}
%
%%User-defined operator constructors are declared using \textsf{opcon}.  For reasons that we will discuss, our calculus associates every operator  constructor with a type constructor. The \emph{fully-qualified name} of every operator constructor, e.g. $\fvar{Nat}.\opvar{z}$, must be unique. Operator constructors, like type constructors, declare an index kind, $\kappaidx$. In our first example, all the operator constructors are indexed trivially (by index kind $\kunit$), but other examples use more interesting indices. For example, in SML, the projection operator \verb|#3| can be applied to an $n$-tuple, $e$, iff $n \geq 3$. Note that it thus cannot be a function with a standard arrow type. Notionally, \verb|#| is an operator constructor and \verb|3| is its index. In an active embedding of $n$-tuples into $\lamAce$, this would be written $\eop{Tuple}{prj}{3}{e}$ (we will nearly recover ML's syntax later). $\fvar{LabeledTuple}.\opvar{prj}$ is the operator constructor used to access a field of a labeled tuple, so it has index kind $\klabel$. An operator itself is, notionally, selected by indexing an operator constructor, e.g. $\fvar{Nat}.\opvar{s}\langle \tunit \rangle$, but technically neither operator constructors nor operators are first-class at any level (additional machinery would be needed, e.g. an \textsf{Op} kind, but this is not fundamental to our calculus). 
%%Instead, in the external language, an operator constructor is applied by simultaneously providing an index and  $n \geq 0$ \emph{arguments}, written $\eop{Tycon}{op}{\tauidx}{\splat{e}{1}{n}}$\footnote{It may be helpful to distinguish between type/operator constructors and \emph{term formers}. There are term formers at all levels in the calculus. For example, operator constructor application and $\lambda$ are  external term formers, and type constructor application is a type-level term former. We might write these following Harper's conventions for abstract syntax to highlight this distinction \cite{pfpl}: $\mathtt{lam}[\tau](x.e)$, $\mathtt{ocapp}[\fvar{Tycon}, \opvar{op}, \tauidx](\splat{e}{1}{n})$ and $\texttt{tcapp}[\fvar{Tycon}](\tau)$.}. For example, on line 18 of Fig. \ref{nat}, we see the operator constructors $\fvar{Nat}.\opvar{z}$ and $\fvar{Nat}.\opvar{s}$ being applied to compute $two$. %\footnote{Although our focus here is entirely on semantics, a brief note on syntax: in the expanded syntax, the trivial indices and empty argument lists can be omitted, so we could write \texttt{Nat.s(Nat.s(Nat.z))}. With the ability to ``open'' a type's operators into the context, we could shorten this still to \texttt{s(s(z))}. Alternatively, with the ability to define a TSL in a manner similar to that in Sec. \ref{aparsing}, we might instead just write \texttt{2}.}
%
%\subsection{Operator Definitions and Representational Consistency}
%
%%
%\newcommand{\atjsynX}[3]{\Gamma \vdash_\fvalCtx #1 \Rightarrow #2 \leadsto #3}
%\newcommand{\atjanaX}[3]{\Gamma \vdash_\fvalCtx #1 \Leftarrow #2 \leadsto #3}
%\begin{figure}[t]
%\small
%$\fbox{\inferrule{}{\ecompilesAX{e}{\tau}{\iota}}}$
%~~~~$\Gamma ::= \emptyset \pipe \Gamma, x \Rightarrow \tau$
%\begin{mathpar}
%\inferrule[att-var]{
%	x \Rightarrow \tau \in \Gamma
%}{
%	\atjsynX{x}{\tau}{x}
%}
%
%\inferrule[att-syn-to-ana]{
%	\atjsynX{e}{\tau}{\iota}
%}{
%	\atjanaX{e}{\tau}{\iota}
%}
%
%\inferrule[att-lam]{
%	\tau_1 \Downarrow_\fvalCtx \tau_1'\\
%		\trepof{\tau_1'} \Downarrow_\fvalCtx \titype{\sigma}\\\\
%	\Gamma, x \Rightarrow \tau_1' \vdash_\fvalCtx e \Rightarrow \tau_2 \leadsto \iota
%%	\iota \hookrightarrow_\fvalCtx \iota'\\
%%		\sigma \hookrightarrow_\fvalCtx \sigma'
%%	\ddbar{\fvar{Arrow}}{\fvalCtx}{\trepof{\tau_1'}}{\sbar_1}\\
%	%\delfromtau{$\Xi_0$}{\fvalCtx}{\tau_1'}{\sabs}\\\\
%}{
%	\atjsynX{\elam{x}{\tau_1}{e}}{\ttype{arrow}{(\tau_1', \tau_2)}}{\ilam{x}{\sigma'}{\iota}}
%}
%
%\inferrule[att-i-unasc]{
%	\vdash_\fvalCtx \FF{iana}(\fvar{tycon})=\taudef\\
%	\taudef~(\tauidx, \tauidx')~((\Gamma; e_1)? :: \ldots :: (\Gamma; e_n)? :: []) \Downarrow_\fvalCtx \titerm{\iota}\\
%%	\trepof{\ttype{tycon}{\tauidx'}} \Downarrow_\fvalCtx \titype{\sigma}\\
%		\trepof{\ttype{tycon}{\tauidx'}} \Downarrow_\fvalCtx \titype{\sigma}\\
%		\Gamma \vdash_\fvalCtx \iota : \sigma
%}{
%	\atjanaX{\FF{intro}[\tauidx](e_1; \ldots; e_n)}{\ttype{tycon}{\tauidx'}}{\iota}
%}
%
%\inferrule[att-i-asc-ty]{
%	\atjanaX{I}{\tau}{\iota}
%}{
%	\atjsynX{I : \tau}{\tau}{\iota}
%}
%
%\inferrule[att-i-asc-tycon]{
%	\vdash_\fvalCtx \FF{isyn}(\fvar{tycon})=\taudef\\\\
%	\taudef~\tauidx~((\Gamma; e_1)? {::}{\ldots}{::}(\Gamma; e_n)? {::} []) \Downarrow_\fvalCtx \tden{\titerm{\iota}}{\ttype{tycon}{\tauidx'}}\\
%			\trepof{\tau_1'} \Downarrow_\fvalCtx \titype{\sigma}\\
%	\Gamma \vdash_\fvalCtx \iota : \sigma
%}{
%	\atjsynX{\FF{intro}[\tauidx](e_1; \ldots; e_n)] :: \fvar{tycon}}{\tau}{\iota'}
%}
%
%\inferrule[att-elim]{
%	\atjsynX{e}{\ttype{tycon}{-}}{-}\\
%	\vdash_\fvalCtx \FF{esyn}(\fvar{tycon})=\taudef\\
%	\taudef~((\Gamma; e)? :: (\Gamma; e_1)? :: \ldots :: (\Gamma; e_n)? :: []) \Downarrow_\fvalCtx \tden{\tau}{\iota}\\
%			\trepof{\tau} \Downarrow_\fvalCtx \titype{\sigma}\\
%	\Gamma \vdash_\fvalCtx \iota : \sigma
%}{
%	\atjsynX{e\cdot\FF{elim}[\tauidx](e_1; \ldots; e_n)}{\tau}{\iota}
%}
%\end{mathpar}
%\caption{\small The active typing judgement. The normalization judgement for type-level terms ($\Downarrow$) and the representational consistency check will be in an appendix.}
%\label{atj}
%\end{figure}
%\begin{figure}
%\small
%\begin{mathpar}
%\inferrule[repof]{
%	\tau \Downarrow_\fvalCtx \ttype{tycon}{\tauidx}\\
%	\vdash_\fvalCtx \FF{rep}(\fvar{tycon}) = \taurep\\
%	\taurep \tauidx \Downarrow_\fvalCtx \titype{\sigma}
%}{
%	\FF{repof}(\tau) \Downarrow_\fvalCtx \titype{\sigma}
%}
%
%\inferrule[syn]{
%    \tau \Downarrow_\fvalCtx (\Gamma; e)?\\
%	\atjsynX{e}{\tau}{\iota}
%}{
%	\FF{syn}(\tau) \Downarrow_\fvalCtx \tden{\titerm{\iota}}{\tau}
%}
%
%\inferrule[ana]{
%	\tau_1 \Downarrow_\fvalCtx (\Gamma; e)?\\
%	\tau_2 \Downarrow_\fvalCtx \ttype{tycon}{\tauidx}\\\\
%	\atjanaX{e}{\ttype{tycon}{\tauidx}}{\iota}\\
%}{
%	\FF{ana}(\tau_1; \tau_2) \Downarrow_\fvalCtx \titerm{\iota}
%}
%\end{mathpar}
%\caption{\small Evaluation semantics for the type-level language. Missing rules (including error propagation rules, which immediately cause failure of typechecking) are unsurprising and will be given in an appendix.}
%\label{tleval}
%\end{figure}
%The expressive and metatheoretic power of the calculus arises from how the rules for the active typing judgement handle these  generalized forms. Rather than fixing the specification of a finite collection of operator constructors and tasking the \emph{compiler} with deciding a typing derivation on its basis, the specification instead delegates to a type-level function associated with a type constructor. In the core calculus, this is one of three functions, called \FF{iana}, \FF{isyn} and \FF{esyn} in the grammar.\footnote{This means that any particular type constructor only supports a single introduction and elimination desugaring. This is a minor inconvenience in some cases that is resolved in Ace by the use of methods.} We see how this is done with the rules for the active typing judgements, given in Figure \ref{atj}.
%
%The first two rules are standard in bidirectional type systems: variables synthesize types and if a term synthesizes a type, it can be analyzed against a type (cf. Listing \ref{context}). Variables translate to variables, and if we are simply converting from synthesis to analysis, translation is not affected.
%
%The rule $\fvar{att-lam}$ must take into account the fact that, because we support type-level computation, the type annotation on the argument may not be in normal form. Thus, we evaluate it to normal form. Note that the kinding rules (not shown) will guarantee that, because $\tau_1$ is of kind $\kTypeBlur$, its normal form, $\tau'_1$, is of the form $\ttype{tycon}{\tauidx}$ for some normal $\tauidx$. To generate an appropriate internal type annotation in the translation, we need to compute the representation type of $\tau_1'$. This involves calling the representation function associated with its type constructor (rule \fvar{repof} in Figure \ref{tleval}). The form $\titerm{\sigma}$ is a \emph{quoted internal type}. Note in the syntax that there is an unquote form, $\dup{\tau}$, which allows us to compose internal types compositionally without needing to expose an elimination form. Indeed, it is an interesting facet of our calculus that we never need to examine syntax trees directly to implement extensions. The evaluation semantics remove quotations, so the normal form of a quoted internal type contains an internal type with no quotations.
%
%Lambda functions are the only introductory form requiring special support in the calculus (because they need to  manipulate the context; see Discussion). The next three rules show how any other abstractions that we define (e.g. records, decimals, etc.) make use of a generalized introductory form. 
%
%The rule  $\fvar{att-i-unasc}$ shows that unascribed introductory forms can only be analyzed against a type. Given such a type, the rule extracts a definition, named $\FF{iana}$, from the type constructor (cf. the \verb|ana_Dict| and \verb|trans_Dict| methods earlier). It calls this function with a pair containing the operator and type index and a list of \emph{reified arguments}, which have kind $\Q$ and introductory form $(\Gamma; e)?$. These are only constructed by the compiler (there would be no corresponding form in the concrete syntax). Their purpose is to allow the definition to programatically invoke synthesis and analysis as needed using the $\FF{ana}$ and $\FF{syn}$ operators. We can see the $\fvar{record}$ type constructor doing so on line 5 of Figure \ref{record-theory} to ensure that the field value provided as an argument has the same type as the corresponding label (accomplished by simultaneously folding over all three pieces of input data). The rule for performing analysis, $\fvar{ana}$, is in Figure \ref{tleval}. If it succeeds, analysis returns a \emph{translation}, which is a quoted internal term with kind $\kITerm$ and introductory form $\titerm{\iota}$. Like quoted internal types, there is an unquote form that is eliminated during evaluation. The operator definition uses this to construct a translation for the record. As before, empty records translate to units and records with a single field are unadorned. In this example, records with two or more fields translate to nested tuples. 
%
%Because we have a representation type associated with the type, we can check that the translation is \emph{representationally consistent} (the final premise). As we will discuss, representational consistency combined with type safety of the internal language implies type safety overall (it arises as a strengthening of the inductive hypothesis needed to prove type safety, and is closely related to work on \emph{type-preserving compilation} in the TIL compiler for Standard ML, \cite{tarditi+:til-OLD}).
%
%The next rule, $\fvar{att-i-asc-ty}$ states that introductory forms ascribed with a type are analyzed against that type.
%
%Introductory forms ascribed with a type constructor can, according to the final introductory rule, $\fvar{att-i-asc-tycon}$, synthesize a type via the definition $\FF{isyn}$. It is passed the operator index (there is no type index, since we only have a type constructor) and a list of arguments, as before. In this case, it must return not just a translation, but also a type. We use the form $\tden{\tau_2}{\tau_1}$ for such a pairing, which has kind $\FF{TT}$. The type and translation can be extracted from it using the appropriately named elimination forms (indeed, as given, it is merely a pair, but we give it special syntax for clarity -- it looks like the conclusion of the typing judgement -- and for other reasons that will become clear in future work). To synthesize a type from a list of labels and arguments, we must be able to synthesize types from the arguments. The $\FF{syn}(\tau)$ operator permits this, per rule $\fvar{syn}$, seen being used in Listing \ref{record-theory}. We check representational consistency of the result, as before.
%
%Finally, we show the rule for elimination forms. It operates by first synthesizing a type for the ``primary'' subterm, then extracting the $\FF{esyn}$ definition from its type constructor. As before, it is called with the arguments and representational consistency is checked. Note that the primary subterm is itself the first argument (though we have already synthesized a type for it, it is more uniform and clear to allow the operator  to do so again, which is done in our example by the $\tvar{arity1}$ helper function).
%
\subsubsection{Representational Consistency}
The representational consistency lemmas says that active typechecking and translation of an external term, if it succeeds (producing a type, $\hat\sigma$ and a translation $\iota$), always produces a well-typed translation. More specifically, the translation's type is the representation type of $\hat\sigma$.

\begin{theorem}[Representational Consistency]
Given well-kinded contexts and an external term that is both well-kinded and well-typed:
\begin{enumerate}
\item $\vdash \Phi$ 
\item $\vdash_\Phi \Gamma$
\item $\vdash_\Phi e$
\item Either:
\begin{enumerate}
\item[a.] $\Gamma \vdash_\Phi e \Leftarrow \hat{\sigma} \leadsto \iota$
\item[b.] $\Gamma \vdash_\Phi e \Rightarrow \hat{\sigma} \leadsto \iota$
\end{enumerate}
\end{enumerate}
there exists an internal typing context, $\Omega$, and an internal type, $\tau$, such that  $\vdash_\Phi \Gamma \leadsto \Omega$ and $\FF{rep}(\hat\sigma) \Downarrow_\Phi \titype{\tau}$ and $\Omega \vdash \iota : \tau$.
\end{theorem}
\begin{proof}
The proof proceeds by induction on the typing derivation (4a or 4b). Because the representational consistency is explicitly checked in the rules for intro and elim forms, these cases follow directly. The remaining cases are standard and follow inductively, given standard lemmas about valid contexts and a lemma about type safety of the static language. This is a fixed, standard functional language with only a few simple additions (TODO detail this).
\end{proof}

\subsubsection{Type Safety}
Representational safety implies that well-typed external terms produce well-typed internal terms. If the internal language is type safe, then we know that well-typed external terms do not ``go wrong''. The internal language is simply PCF with sums and products, and a base type of integers, so this is a completely standard result \cite{pfpl}.

%\subsubsection{Decidability}
%The metatheoretic properties of interest are: type safety and termination of the type-level language (guaranteed by our kind system, though the details will need to be provided in an appendix; termination is non-trivial), type safety and decidability of the internal language (it i.s a standard variant of PCF, so this is trivial) and representational consistency. Type safety for the language as a whole comes as a corollary of these lemmas. We plan to provide detailed proofs of these, but for now, they should be treated as conjectures and our formulation above as expository. 
%We believe that despite this, the formalization is surprisingly elegant and concise, given its expressive power. It is a useful exercise to implement natural numbers, ala G\"odel's \textbf{T}, using this calculus, assuming that the type-level and internal languages include integers and one can lift them from the former into the latter.
%%
%
%For the synthetic functions, it must decide the type assignment and the translation on the basis of the index and arguments, or decide that this is not possible. An operator constructor with index kind $\kappaidx$ must have a definition of kind \[\kappaidx \rightarrow \klist{\kDen} \rightarrow (\kDen + \kunit)\] 
%As this suggests, the active typing judgement  will provide the operator index and a list of {recursively determined \emph{derivates}}, which have kind $\kDen$, for the arguments and ask the definition to return a value of ``option kind'', $\ksum{\kDen}{\kunit}$, where the trivial case indicates that a derivate cannot be constructed due to an invalid index, an incorrect number of arguments or an argument of an invalid type.\footnote{In practice, we would require operator constructor providers to report information about the precise location of the error (e.g. which argument was of incorrect type) and provide an appropriate error message and other metadata, but we omit this in the semantics.}
%
%
%A \emph{program}, $\rho$, consists of a series of type constructor declarations followed by an external term, $e$. The syntax for external terms contains six forms: variables, $\lambda$ terms, a generalized unascribed introductory form, a generalized introductory form ascribed with a type, a generalized introductory form ascribed with a type constructor and a generalized elimination form. We will discuss how a more natural syntax  like Python's can be added by simple desugaring.\todo{syntax}
%
%
%
%Compiling a program consists of first \emph{kind checking} it (typechecking the type-level terms, which has no analog in Ace because the type-level language is Python; discussed further below), then typechecking the external term and {translating} it to a term, $\iota$, in the {typed internal language}. %These correspond to the premises of the \emph{central compilation judgement} $\pkcompiles{\rho}{\iota}$:
%%\[
%%\inferrule[p-compiles]{%
%%	\emptyset \vdash_{\fvalCtx_0} \rho\\
%%	\progOK{\emptyset}{\fvalCtx_0}{\rho}\\
%%	\pcompiles{\fvalCtx_0}{\rho}{\iota}
%%}{\pkcompiles{\rho}{\iota}}
%%\]
%%his anchors our exposition; we will describe how it is derived (i.e. how to write a compiler for $\lamAce$) in the following sections. 
%
%The key judgement in the calculus is the \emph{active typing judgement}  (Fig. \ref{att}, which we describe starting in Sec. \ref{opcons}). It relates an external term, $e$, to a {type}, $\tau$, called its \emph{type assignment}, and an internal term, $\iota$, called its \emph{translation}, under \emph{typing context} $\Gamma$ and \emph{constructor context} $\Phi$: 
%\[\ecompilesX{e}{\tau}{\iota}\]
%
%The typing context $\Gamma$ maps variables to types in essentially the conventional way (\cite{pfpl} contains the necessary background for this paper). The constructor context tracks user-defined type and operator constructors. % Weakening and exchange of the constructor context closely related to the issue of conservativity that we will return to. Each constructor is identified by name, so there is no analog to contraction.
%There is no separate operational semantics for the external language. Instead, the dynamic behavior of an external term is determined by its translation to the internal language, which has a more conventional operational semantics. This form of semantics can be seen as lifting into the language specification the first stage of a type-directed compiler like the TIL compiler for Standard ML \cite{tarditi+:til-OLD} and has some parallels to the Harper-Stone semantics for Standard ML, where external terms were also given meaning by elaboration from the EL to an IL \cite{Harper00atype-theoretic}. %
%
%In $\lamAce$, the internal language (IL) provides partial functions (via the generic fixpoint operator of Plotkin's PCF), simple product and sum types and a base type of integers (to make our example interesting and as a nod toward speed on contemporary machines). In practice, the internal language could be any typed  language with a specification for which type safety and decidability of typechecking have been satisfyingly determined. The internal type system serves as a ``floor'': guarantees that must hold for \emph{all} well-typed terms, independent of the constructor context (e.g. that out-of-bounds access to memory never occurs), must be maintained by the internal type system. User-defined constructors can (only) enforce invariants stronger than those the internal type system maintains. Performance is also ultimately limited by the internal language and downstream compilation stages that we do not here consider (safe compiler extension has been discussed in previous work, e.g. \cite{conf/pldi/TatlockL10}).
%


%\begin{figure}
%\small
%\begin{flalign}
%\small
% & \family{Tuple}{\klist{\kTypeBlur}}{\\
% & \quad \tops{new}{\kunit}{\_}{a}{\tlam{\_}{\kunit}{\tlam{a}{\klist{\kDen}}{\\
% & \quad\quad
% 	\tfold{\tvar{a}}{\tden{{()}}{\ttype{Tuple}{\tnil{\kTypeBlur}}}}{d}{b}{r}{\\
%			& \quad\quad\quad \tfamcase{\ttypeof{\tvar{r}}}{Ntuple}{i}{\\
%			& \quad\quad\quad\quad \tfold{\tvar{b}}{\tden{\itransof{\tvar{d}}}{\ttype{Ntuple}{\tcons{\ttypeof{\tvar{d}}}{\tvar{i}}}}\\& \quad\quad\quad\quad}{\_}{\_}{\_}{\tden{\\& \quad\quad\quad\quad\quad {(\itransof{\tvar{d}},\itransof{\tvar{r}})}}{\ttype{Ntuple}{\tcons{{\ttypeof{\tvar{d}}}}{\tvar{i}}}}}
%			}{\terr}
%	}
% }}};\\
% & \quad \tops{prj}{\kint}{i}{a}{\lambda \tvar{i}:\kint.\lambda \tvar{a}:\klist{\kDen}. 
% 	\tapp{\tvar{arity1}}{\tapp{\tvar{a}}{\tlam{d}{\kDen}{\\
%	& \quad\quad\tfamcase{\ttypeof{\tvar{d}}}{Ntuple}{nl}{\\
% & \quad\quad \tfold{\tvar{nl}}{\terr}{t1}{j}{\_}{\\
% & \quad\quad \tfold{\tvar{j}}{\tifeq{\tvar{i}}{1}{\dint}{\tden{\itransof{d}}{\tvar{t1}}}{\terr}\\&\quad\quad\quad}{\_}{\_}{\_}{\\
% & \quad\quad\quad (\tlam{p}{\kpair{\kDen}{\dint}}{\tifeq{\tsnd{\tvar{p}}}{\tvar{i}}{\dint}{\tfst{\tvar{p}}}{\terr}})\\
% & \quad\quad\quad (\tapp{\tvar{foldl}}{\tapp{\tvar{nl}}{\tapp{(\tden{\tvar{x}}{\tvar{nt}},0)}{
% 	\tlam{r}{\kpair{\kDen}{\dint}}{\tlam{t}{\kTypeBlur}{\tlam{ts}{\klist{\kTypeBlur}}{\\
%	& \quad\quad\quad\quad \tifeq{\tvar{i}}{\tsnd{\tvar{r}}}{\dint}{\tvar{r}}{\tdencase{\tfst{\tvar{r}}}{rx}{\_}{\\
%	& \quad\quad\quad\quad\quad \tifeq{\tvar{i}}{\tsnd{\tvar{r}}+1}{\dint}{\\
%	& \quad\quad\quad\quad\quad\quad \tifeq{\tvar{ts}}{\tnil{\kTypeBlur}}{\klist{\kTypeBlur}}{(\tden{\tvar{rx}}{\tvar{t}},\tvar{i})\\
%	& \quad\quad\quad\quad\quad\quad}{(\tden{\titerm{\ifst{\iup{\tvar{rx}}}}}{\tvar{t}},\tvar{i})}\\	
%	& \quad\quad\quad\quad\quad}{(\tden{\titerm{\isnd{\iup{\tvar{rx}}}}}{\tvar{t}},\tsnd{\tvar{r}}+1)}}{\terr}}}}}
% }}})
% }}{\terr}}}}}
% }\\ & 
%%& \quad \tops{pr}a}{b}{c}{d}
% }{d}{i}{(\tlam{i}{\klist{\kTypeBlur}}{
% 	\\& \quad \quad \tfold{\tvar{i}}{\titype{\dunit}}{s}{j}{r}{
% 		\tfold{\tvar{j}}{\titype{\trepof{\tvar{s}}}}{\_}{\_}{\_}{\\
%		&\quad\quad\quad
% 		\titype{\dpair{\trepof{\tvar{s}}}{\dup{\tvar{r}}}}
% 		}
% 	}
% })}
%\end{flalign}
%\caption{ABC}
%\label{tuple}
%\end{figure}
%\begin{figure}[t]
%\small
%$\fbox{\inferrule{}{\progOKX{\progsort}}}$
%~~~$\tvarCtx ::= \emptyctx \pipe \tvarCtxX{t}{\kappa}$
%~~~$\fvalCtx ::= \emptyctx \pipe \fvalCtxX{\fvalDf}$
%%$\fCtx ::= \Sigma_0	 \pipe \fvalCtxX$
%\begin{mathpar}
%\inferrule[k-tycon]{
%	\fvar{tycon} \notin \text{dom}(\fvalCtx)\\
%	\kEq{\kappaidx}\\
%	\tKind{\tvarCtx}{\fvalCtx}{\taurep}{\karrow{\kappaidx}{\kIType}}\\
%	\opType{\tvarCtx}{\fvalCtxX{\fvalDf}}{\theta}{\Theta}\\
%	\progOK{\tvarCtx}{\fvalCtxX{\fvalDf}}{\rho}
%}{
%	\progOKX{\pfam{\familyDf}{\rho}}
%}
%
%%\inferrule[def-kinding]{
%%	\tKindX{\tau}{\kappa}\\
%%	\progOK{\tvarCtxX{t}{\kappa}}{\fvalCtx}{\rho}
%%}{
%%	\progOKX{\pdef{t}{\kappa}{\tau}{\rho}}
%%}
%%
%\inferrule[k-e-prog]{
%	\exprOK{\tvarCtx}{\fvalCtx}{e}
%}{
%	\progOKX{e}
%}
%\end{mathpar}
%$\fbox{$\opType{\tvarCtx}{\fvalCtx}{\theta}{\Theta}$}$
%\begin{mathpar}
%\inferrule[k-opcon]{
%	\tKind{\tvarCtx}{\fvalCtx}{\taudef}{\karrow{\kappaidx}{\karrow{\klist{\kDen}}{(\ksum{\kDen}{\kunit})}}}
%}{
%	\opType{\tvarCtx}{\fvalCtx}{{\tops{op}{\kappaidx}{i}{a}{\taudef}}}{
%	\kOpS{op}{\kappaidx}}
%}
%
%\inferrule[k-opcons]{
%	\opType{\tvarCtx}{\fvalCtx}{\theta_1}{\Theta_1}\\
%	\opType{\tvarCtx}{\fvalCtx}{\theta_2}{\Theta_2}\\\\
%	\text{dom}(\theta_1) \cap \text{dom}(\theta_2) = \emptyset
%}{
%	\opType{\tvarCtx}{\fvalCtx}{\theta_1; \theta_2}{\Theta_1, \Theta_2}
%}
%\end{mathpar}
%$\fbox{$\exprOKX{e}$}$
%\begin{mathpar}
%\inferrule[k-e-var]{ }{
%	\exprOK{\tvarCtx}{\fvalCtx}{\evar{x}}
%}
%
%\inferrule[k-e-lam]{
%	\tKindX{\tau}{\kTypeBlur}\\
%	\exprOK{\tvarCtx}{\fvalCtx}{e}
%}{
%	\exprOKX{\elam{\evar{x}}{\tau}{e}}
%}
%
%\inferrule[k-e-op]{
%	\fval{tycon}{-}{-}{\theta} \in \fvalCtx\\
%	\tops{op}{\kappaidx}{i}{a}{-} \in \theta\\
%	\tKindX{\tauidx}{\kappaidx}\\
%	\exprOKX{e_1}\\
%	\cdots\\
%	\exprOKX{e_n}
%}{
%	\exprOKX{\eop{tycon}{op}{\tauidx}{\splat{e}{1}{n}}}
%}
%\end{mathpar}
%$\fbox{$\kEq{\kappa}$}$
%\begin{mathpar}
%\inferrule[t-eq]{ }{
%	\kEq{\kTypeBlur}
%}
%
%\inferrule[i-eq]{ }{
%	\kEq{\kint}
%}
%
%\inferrule[l-eq]{ }{
%	\kEq{\klabel}
%}
%
%\inferrule[list-eq]{
%	\kEq{\kappa}
%}{
%	\kEq{\klist{\kappa}}
%}
%\\
%\inferrule[u-eq]{ }{
%	\kEq{\kunit}
%}
%
%\inferrule[p-eq]{
%	\kEq{\kappa_1}\\
%	\kEq{\kappa_2}
%}{
%	\kEq{\kpair{\kappa_1}{\kappa_2}}
%}
%
%\inferrule[s-eq]{
%	\kEq{\kappa_1}\\
%	\kEq{\kappa_2}
%}{
%	\kEq{\ksum{\kappa_1}{\kappa_2}}
%}
%\end{mathpar}
%\caption{\small Kinding for type and operator constructors and external terms, and equality kinds (see text).  Type-level terms stored in the constructor context are not needed during kinding, indicated using a dash.}
%\label{kindprog}
%\vspace{-10pt}
%\end{figure}


%\section{Interactive Compilation and Invocation}\label{targets}
%\subsection{OpenCL as an Active Library}
%The code in this section uses \verb|clx|, an example library implementing the semantics of the OpenCL programming language and extending it with some additional useful types, which we will discuss shortly. Ace itself has no built-in support for OpenCL.
%
%To briefly review, OpenCL provides a data-parallel SPMD programming model where developers define functions, called {\em kernels}, for execution on \emph{compute devices} like GPUs or multi-core CPUs \cite{opencl11}. Each thread executes the same kernel but has access to a unique index, called its \emph{global ID}. Kernel code is written in a variant of C99 extended with some new primitive types and operators, which we will introduce  as needed in our examples below.
%
%\subsection{Generic Functions}\label{genfn}
%%\begin{codelisting}[t]
%%\lstinputlisting[commentstyle=\color{mauve}]{listing7.py}
%%\caption{[\texttt{listing\ref{metaprogramming}.py}] Metaprogramming with Ace, showing how to construct generic functions from abstract syntax trees.}
%%\label{metaprogramming}
%%\end{codelisting}
%
%%\begin{codelisting}
%%\lstinputlisting[linebackgroundcolor={\btLstHL{5-8}}]{listing3.py}
%%\caption{[\texttt{listing\ref{map}.py}] A generic data-parallel higher-order map function targeting OpenCL.}
%%\label{map}
%%\end{codelisting}
%%\begin{codelisting}[t]
%%\lstinputlisting[commentstyle=\color{mauve}]{listing7.py}
%%\caption{[\texttt{listing\ref{metaprogramming}.py}] Metaprogramming with Ace, showing how to construct generic functions from abstract syntax trees.}
%%\label{metaprogramming}
%%\end{codelisting}
%Lines 3-4 introduce \verb|map|, an Ace function of three arguments that is governed by the \emph{active base} referred to by \verb|clx.base| and targeting the \emph{active target} referred to by \verb|clx.opencl|. The active target determines which language the function will compile to (here, the OpenCL kernel language) and mediates code generation. 
%
%The body of this function, highlighted in grey for emphasis, does not have Python's semantics. Instead, it will be governed by the active base together with the active types used within it. No such types have been provided explicitly, however. Because our type system is extensible, the code inside could be meaningful for many different assignments of types to the arguments. We call functions awaiting types \emph{generic functions}. Once types have been assigned, they are called \emph{concrete functions}.
%
%Generic functions are represented at compile-time as instances of \verb|ace.GenericFn| and consist of an abstract syntax tree, an {active base} and an {active target}. The purpose of the \emph{decorator} on line 3 is to replace the Python function on lines 4-8 with an Ace generic function having the same syntax tree and the provided active base and active target. 
%Decorators in Python are simply syntactic sugar for applying the decorator function directly to the function  being decorated \cite{python}. In other words, line 3 could be replaced by inserting the following  statement on line 9:
%\vspace{-0.28cm}
%\begin{verbbox}
%map = ace.fn(clx.base, clx.opencl)(map)
%\end{verbbox}
%\begin{figure}[h!]
%\centering
%\theverbbox
%\end{figure}
%\vspace{-0.28cm}
%
%The abstract syntax tree for \verb|map| is extracted using the Python standard  library packages  \verb|inspect| (to retrieve its source code) and \verb|ast| (to parse it into a syntax tree). 
%
%%\subsection{Metaprogramming in Ace}
%%Generic functions can be generated directly from ASTs as well, providing Ace with support for  straightforward metaprogramming. Listing \ref{metaprogramming} shows how to generate two more generic functions, \verb|scale| and \verb|negate|. The latter is derived from the former by using a library for manipulating Python syntax trees, \verb|astx|. In particular, the \verb|specialize| function replaces uses of the second argument of \verb|scale| with the literal \verb|-1| (and changes the function's name), leaving a function of one argument.
% 
%\subsection{Concrete Functions and Explicit Compilation}
%To compile a generic function to a particular \emph{concrete function}, a type must be provided for each argument, and typechecking and translation must then succeed. Listing \ref{compscript} shows how to explicitly provide type assignments to \verb|map| using the subscript operator (implemented using Python's operator overloading mechanism). We attempt to do so three times in Listing \ref{compscript}. The first, on line \ref{compscript}.7, fails due to a type error, which we handle so that the script can proceed. The error occurred  because the ordering of the argument types was incorrect. We provide a valid ordering on line \ref{compscript}.9 to generate the concrete function \verb|map_neg_f32|. We then provide a different type assignment to generate the concrete function \verb|map_neg_ci32|.
%Concrete functions are instances of \verb|ace.TypedFn|, consisting of an abstract syntax tree annotated with types and translations along with a reference to the original generic function. %The typing and translation process is mediated by the logic in the active base, types and target that have been provided, as we will describe in more detail below. 
%
%To produce an output file from an Ace ``compilation script'' like \verb|listing|\texttt{\ref{compscript}}\verb|.py|, the command \verb|acec| can be invoked from the shell, as shown in Listing \ref{mapc}. 
%\subsection{Types}
%\begin{codelisting}
%\lstinputlisting{listing4.py}
%\caption{[\texttt{listing\ref{compscript}.py}] The generic \texttt{map} function compiled to map the \texttt{negate} function over two  types of input.}
%\label{compscript}
%\end{codelisting}
%\begin{codelisting}
%\begin{lstlisting}[style=Bash]
%$ `acec listing3.py`
%Hello, compile-time world!
%[ace] TypeError in listing1.py (line 6, col 28): 
%      'GenericFnType(negate)' does not support [].
%[acec] listing3.cl successfully generated.
%\end{lstlisting}
%\caption{Compiling \texttt{listing\ref{compscript}.py} using the \texttt{acec} compiler.}
%\label{mapc}
%\end{codelisting}
%\begin{codelisting}
%\lstinputlisting[style=OpenCL]{listing5.cl}
%\caption{[\texttt{listing\ref{compscript}.cl}] The OpenCL file generated by Listing \ref{mapc}.}
%\label{mapout}
%\end{codelisting}
%%Lines \ref{compscript}.3-\ref{compscript}.5 construct the types assigned to the arguments of \verb|map| on lines \ref{compscript}.7-\ref{compscript}.10. In Ace, types are themselves values that can be manipulated at compile-time. This stands in contrast to other contemporary languages, where user-defined types (e.g. datatypes, classes, structs) are written declaratively at compile-time but cannot be constructed, inspected or passed around programmatically. More specifically, types are instances of a Python class that implements the \verb|ace.ActiveType| interface (see Sec. \ref{atypes}). 
%%As Python values, types can be assigned to variables when convenient (removing the need for  facilities like \verb|typedef| in C or \verb|type| in Haskell). Types, like all compile-time objects derived from Ace base classes, do not have visible state and operate in a referentially transparent manner (by constructor memoization, which we do not detail here).% These types are all implemented in the \verb|clx| library imported on line 1, none are built into Ace itself.
%
%The type named \verb|T1| on line \ref{compscript}.3 corresponds to the OpenCL type \verb|global float*|: a pointer to a 32-bit floating point number stored in the compute device's global memory (one of four address spaces defined by OpenCL \cite{opencl11}). It is constructed by applying \verb|clx.Ptr|, which is an Ace type constructor corresponding to pointer types, to a value representing the  address space, \verb|clx.global_|, and the type being pointed to. That type, \verb|clx.float|, is in turn the Ace type corresponding to \verb|float| in OpenCL (which, unlike C99, is always 32 bits). 
%The \verb|clx| library contains a full implementation of the OpenCL type system (including behaviors, like promotions, inherited from C99).
%Ace is \emph{unopinionated} about issues like memory safety and the wisdom of such promotions. We will discuss how to implement, as libraries, abstractions that are higher-level than raw pointers in Sec. \ref{examples}, but Ace does not prevent users from choosing a low level of abstraction or ``interesting'' semantics if the need arises (e.g. for compatibility with existing libraries; see the discussion in Sec. \ref{discussion}). We also note that we are being more verbose than necessary for the sake of pedagogy. The \verb|clx| library includes more concise shorthand for OpenCL's types: \verb|T1| is equal to \verb|clx.gp(clx.f32)|. %Similarly, the decorators in Listings \ref{map} and \ref{metaprogramming} could have been written \verb|clx.cl_fn|.\todo{move this back there probably}
%
%The type \verb|T2| on line \ref{compscript}.4 is a pointer to a \emph{complex integer} in global memory. It does not correspond direrctly to a type in OpenCL, because OpenCL does not include primitive support for complex numbers. Instead, it uses an active type constructor \verb|clx.Cplx|, which includes the necessary logic for typechecking operations on complex numbers and translating them to OpenCL (Sec. \ref{atypes}). This constructor is parameterized by the numeric type that should be used for the real and imaginary parts, here \verb|clx.int|, which corresponds to 32-bit OpenCL integers. Arithmetic operations with other complex numbers, as well as with plain numeric types (treated as if their imaginary part was zero), are supported. When targeting OpenCL, Ace expressions assigned type \verb|clx.Cplx(clx.int)| are compiled to OpenCL expressions of type \verb|int2|, a  \emph{vector type} of two 32-bit integers (a type that itself is not inherited from C99). This can be observed in several places on lines \ref{mapout}.14-\ref{mapout}.21. This choice is merely an implementation detail that can be kept private to \verb|clx|, however. An Ace value of type \verb|clx.int2| (that is, an actual OpenCL vector) \emph{cannot} be used when a \verb|clx.Cplx(clx.int)| is expected (and attempting to do so will result in a static type error): \verb|clx.Cplx| truly extends the type system, it is not a type alias.
%
%The type \verb|TF| on line \ref{compscript}.5 is extracted from the generic function \verb|negate| constructed in Listing \ref{metaprogramming}. Generic functions, according to Sec. \ref{genfn}, have not yet had a type assigned to them, so it may seem perplexing that we are nevertheless assigning a type to \verb|negate|. Although a conventional arrow type cannot be assigned to \verb|negate|, we can give it a \emph{singleton type}: a type that simply means ``this expression is the \emph{particular} generic function \verb|negate|''. This type could also have been explicitly written as \verb|ace.GenericFnType(listing2.negate)|. During typechecking and translation of \verb|map_neg_f32| and \verb|map_neg_ci32|, the call to \verb|f| on line \ref{map}.6 uses the type of the provided argument to compile the generic function that inhabits the singleton type of \verb|f| (\verb|negate| in both of these cases) to a concrete function. This is why there are two versions of \verb|negate| in the output in Listing \ref{mapout}. In other words, types \emph{propagate} into generic functions -- we didn't need to compile \verb|negate| explicitly. This also explains the error printed on line \ref{mapc}.3-\ref{mapc}.4: when this type was inadvertently assigned to the first argument \verb|input|, the indexing operation on line \ref{map}.6 resulted in an error. A generic function can only be \emph{statically} indexed by a list of types to turn it into a concrete function, not \emph{dynamically} indexed with a value of type \verb|clx.size_t| (the return type of the OpenCL primitive function \verb|get_global_id|).
%
%In effect, this scheme enables higher-order functions even when targeting languages, like OpenCL, that have no support for higher-order functions (OpenCL, unlike C99, does not support function pointers). Interestingly, because they have a singleton type, they are higher-order but not first-class functions. That is, the type system would prevent you from creating a heterogeneous list of generic functions. Concrete functions, on the other hand, can be given both a singleton type and a true function type. For example, \verb|listing2.negate[[clx.int]]| could be given type \verb|ace.Arrow(clx.int, clx.int)|. The base determines how to convert the Ace arrow type to an arrow type in the target language (e.g. a function pointer for C99, or an integer that indexes into a jump table constructed from knowledge of available functions of the appropriate type in OpenCL).
%
%Type assignment to generic functions is similar in some ways to template specialization in C++. In effect, both a template header and type parameters at call sites are being generated automatically by Ace. This simplifies a sophisticated feature of C++ and enables its use with other targets like OpenCL. %Other uses for C++ templates (and the preprocessor) are subsumed by the metaprogramming features discussed above\todo{cite template metaprogramming paper}.
%
%%\subsection{Within-Function Type Resolution}
%%\begin{codelisting}
%%\lstinputlisting{listing6.py}
%%\caption{\texttt{[listing6.py]} A function demonstrating whole-function type inference when multiple values with differing types are assigned to a single identifier, \texttt{y}.}
%%\label{inference}
%%\end{codelisting}
%%On line 5 in the generic \verb|map| function in Listing \ref{map}, the variable \verb|gid| is initialized with the result of calling the OpenCL primitive \verb|get_global_id|.  The type for \verb|gid| is never given explicitly. This is a simple case of Ace's {\em within-function type resolution} strategy (we hesitate to call it \emph{type inference} because it does not, strictly speaking, take a constraint-solving approach). In this case, the type of \verb|gid| will resolve to \verb|size_t| because that is the return type of \verb|get_global_id| (as defined in the OpenCL specification, which the \verb|ace.OpenCL| module follows). The result can be observed on Lines 11 and 24 in Listing \ref{mapout}. 
%%
%%Inference is not restricted within single assignments, as in the \verb|map| example, however. Multiple assignments to the same identifier with values of differing types, or multiple return statements, can be combined if the types in each case are compatible with one another (e.g. by a subtyping relation or an implicit coercion). In Listing \ref{inference}, the \verb|threshold_scale| function assigns different values to \verb|y| in each branch of the conditional. In the first branch, the value \verb|0| is an \verb|int| literal. However, in the second branch of the loop, the type depends on the types of both arguments, \verb|x| and \verb|scale|. We show two choices for these types on Lines 11 and 12. Type inference correctly combines these two types according to OpenCL's C99-derived rules governing numeric types (defined by the user in the \verb|OpenCL| module, as we will describe in Section \ref{att}). We can verify this programmatically on Lines 12 and 13. Note that this example would also work correctly if the assignments to \verb|y| were replaced with \verb|return| statements (in other words, the return value of a function is treated as an assignable for the purpose of type inference).
%
%\subsection{Implicit Compilation and Interactive Execution}\label{compenv}\label{backend}\label{implicit}
%%Professional end-user programmers (e.g. scientists and engineers \cite{professional-end-users}) today generally use dynamically-typed high-level languages like MATLAB, Python, R or Perl for tasks that are not performance-sensitive, such as small-scale data analysis and plotting \cite{nguyen2010survey}. For portions of their analyses where the performance overhead of dynamic type checking and automatic memory management is too high, they will typically call into code written in a statically-typed, low-level language, most commonly C or Fortran, that uses low-level parallel abstractions like pthreads and MPI \cite{4222616,basili2008understanding}. Unfortunately, these low-level languages and abstractions are notoriously difficult to use and automatic verification is intractable in general.\todo{integrate this}
%
%
%A common workflow for \emph{professional end-user programmers} (e.g. scientists and engineers) is to use a simple scripting language for orchestration, small-scale data analysis and visualization and call into a low-level language for performance-critical sections. Python is both designed for this style of use and widely adopted for such tasks \cite{sanner1999python,nguyen2010survey}. Developers can call into native functions using Python's foreign function interface (FFI), for example. A more recent trend is to generate and compile code without leaving Python, using a Python wrapper around a compiler. For example, \verb|weave| works with C and C++, and \verb|pycuda| and \verb|pyopencl| work with CUDA and OpenCL, respectively \cite{klockner2011pycuda}. 
%\begin{codelisting}
%\lstinputlisting{listing8.py}
%\caption{[\texttt{listing\ref{py}.py}] A full OpenCL program using the \texttt{clx} Python bindings, including data transfer to and from a device and direct invocation of a generic function, \texttt{map}.}
%\label{py}
%\end{codelisting}
%The OpenCL language was designed for this workflow, exposing a retargetable compiler and data management  routines as an API, called the \emph{host API} \cite{opencl11}. The \verb|pyopencl| library exposes this API to Python and simplifies interoperation with \verb|numpy|, a popular package for manipulating contiguously-allocated numeric arrays in Python \cite{klockner2011pycuda}.
%
%Ace supports a refinement to this workflow, as an alternative to the \verb|acec| compiler described above, for targets that have wrappers like this available, including \verb|clx.opencl|. Listing \ref{py} shows an example of this workflow where the user chooses a compute device (line \ref{py}.3),  constructs a \verb|numpy| array (line \ref{py}.5), transfers it to the device (line \ref{py}.6), allocates an empty equal-sized buffer for the result of the computation (line \ref{py}.7), launches the {generic} kernel \verb|map| from Listing \ref{map} with these device arrays as well as the function \verb|negate| from Listing \ref{metaprogramming} (line \ref{py}.9) choosing a number of threads equal to the number of elements in the input array (line \ref{py}.10), and transfers the result back into main memory to check that the device computed the expected result (line \ref{py}.12).
%
%For developers experienced with the usual OpenCL or CUDA workflow, the fact that this can be accomplished in a total of 6 statements may be surprising. This simplicity is made possible by Ace's implicit tracking of types throughout the code. First, \verb|numpy| keeps track of the type, shape and order of its arrays. The type of \verb|input|, for example, is \verb|numpy.float64| by default, its shape is \verb|(1024,1024)| and its order is row-major by default. The \verb|pyopencl| library, which the \verb|clx| mechanism is built upon, uses this metadata to automatically call the underlying OpenCL host API function for transferring byte arrays to the device without requiring the user to calculate the size. The \verb|clx| wrapper further retains this metadata in \verb|d_input| and \verb|d_output|, the Python wrappers around the allocated device arrays. The Ace active type of these wrappers is an instance of \verb|clx.NPArray| parameterized by this metadata. This type knows how to typecheck and translate operations, like indexing with a multi-dimensional thread index, automatically.
%
%By extracting the types from the arguments, we can call the generic function \verb|map| without first requiring an explicit type assignment, like we needed when using \verb|acec| above. In other words, dynamic types and other metadata can propagate from Python data structures into an Ace generic function as static type information, in the same manner as it propagated \emph{between} generic functions in the previous section. In both cases, typechecking and translation of \verb|map| happens the first time a particular type assignment is encountered and cached for subsequent use. When called from Python, the generated OpenCL source code is compiled for the device we selected using the OpenCL retargetable compilation infrastructure, and cached for subsequent calls. 
%
%The same program written using the OpenCL C API directly is an order of magnitude longer and significantly more difficult to comprehend. OpenCL does not support higher-order functions nor is there any way to write \verb|map| in a type-generic or shape-generic manner. If we instead use the \verb|pyopencl| library and apply the techniques described in \cite{klockner2011pycuda}, the program is still twice as large and less readable than this code. Both the \verb|map| and \verb|negate| functions must be explicitly specialized with the appropriate types using string manipulation techniques, and custom shapes and orders can be awkward to handle. Higher order functions are still not available, and must also be simulated by string manipulation. That approach also does not permit the use any of the language extensions that Ace enables (beyond the useful type for \verb|numpy| arrays just described; see Sec. \ref{examples} for more interesting possibilities).
%
%%Not shown are several additional conveniences, such as delegated kernel sizing and \verb|In| and \verb|Out| constructs that can reduce the size and improve the clarity of this code further; due to a lack of space, the reader is referred to the language documentation.

\section{Related Work}\label{related}

%\parindent0pt
%
\begin{figure*}
\small
\vspace{-10pt}
\onecolumn
\begin{longtable}{l l@{}l c@{}c c@{}c c@{}c c@{}c c@{}c}
%\toprule%

&&  &&  && {\bfseries Extensible} && {\bfseries Extensible} && {\bfseries Unambiguous} && {\bfseries Alternative}\\

{\bfseries Approach} && {\bfseries Examples} && {\bfseries Library} && {\bfseries Syntax} && {\bfseries Type System} && {\bfseries Composition} && {\bfseries Targets}  \\

\cmidrule(l){1-1} \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7} \cmidrule(l){8-9} \cmidrule(l){10-11} \cmidrule(l){12-13}

\endhead


Active Types && Ace && \CIRCLE && \Circle && \CIRCLE && \CIRCLE && \CIRCLE \\

\myrowcolour%
Desugaring && SugarJ \cite{erdweg2011sugarj} && \Circle && \CIRCLE && \Circle && \Circle && \Circle \\
Type-Specific Literals && Wyvern \cite{Omar:2013:TWP:2489812.2489815} && \Circle && \CIRCLE && \Circle && \CIRCLE && \Circle \\
\myrowcolour%
Rule Injection && Racket \cite{TypedScheme2008}, Xroma \cite{activelibraries}, mbeddr \cite{mbeddr} &&  \cite{TypedScheme2008} && \cite{mbeddr} && \CIRCLE && \Circle && \Circle \\
Attribute Grammars && Silver \cite{VanWyk:2010:SEA}, Xoc \cite{conf/asplos/CoxBCKK08} && \Circle && \CIRCLE && (unsafe)&& \cite{conf/pldi/SchwerdfegerW09} && \Circle\\
\myrowcolour
Static macros /  && Scala \cite{ScalaMacros2013}, MorphJ \cite{MorphJ2011}, && \Circle && \Circle && \Circle && \CIRCLE && \Circle \\
\myrowcolour
Metaprogramming &&  MetaML \cite{Sheard:1999:UMS}, Template Haskell && && && && && \\

Cross-Compilation && LMS/Delite \cite{Rompf:2012:LMS, Delite2011} && \CIRCLE && \Circle && \Circle && \Circle && \CIRCLE \\

\myrowcolour%
%EDSL Frameworks && ? && \CIRCLE && \CIRCLE && \CIRCLE && \Circle && \Circle \\
\end{longtable}
\caption{Comparison to related approaches to language extensibility.}\label{relatedtable}
\twocolumn
\vspace{-10pt}
\end{figure*}
%\subsection{Active Libraries}

%We have implemented the full OpenCL programming language as  primary example showed how to support low-level GPU programming from a high-level language. Other languages have supported similar workflows, e.g. Rust \cite{rustgpu}. These have not been fundamentally extensible and suffer from the ``bootstrapping'' problem described in Sec. \ref{intro}. 

Libraries containing compile-time logic have previously been called {\it active libraries} \cite{activelibraries}. A number of  projects, such as Blitz++, have taken advantage of C++ template metaprogramming to implement domain-specific optimizations \cite{veldhuizen2000blitz++}. Others have chosen custom metalanguages (e.g. Xroma \cite{activelibraries}, mbeddr \cite{mbeddr}). In Ace, we replace these brittle mini-languages with a general-purpose language, Python, and significantly expand the notion of active libraries by consideration of types, base semantics and target languages as values (in this case, objects). We borrow the term ``active'' due to this relationship.

%Generic functions are a novel strategy for {\it function polymorphism} -- defining functions that operate over more than a single type. In Ace, generic functions are implicitly polymorphic and can be called with arguments of {\it any type that supports the operations used by the function}. This is related to structural polymorphism \cite{malayeri2009structural}. Structural types make explicit the structure required of an argument, unlike generic functions, which are only given singleton types because the structure may depend on the semantics of an active type. 
%Structural typing can be compared to the more \emph{ad hoc} approach taken by dynamically-typed languages like Python itself, sometimes called ``duck typing''. It is also comparable to the C++ template system, as discussed previously. 

{\it Operator overloading} \cite{vanWijngaarden:Mailloux:Peck:Koster:Sintzoff:Lindsey:Meertens:Fisker:acta:1975} and {\it metaobject dispatch} \cite{Kiczales91} are run-time protocols that translate operator invocations into function calls. The function is typically selected according to the dynamic type or value of one or more operands. These protocols share the notion of {\it inversion of control} with our strategy. However, our strategy is a {\it compile-time} protocol where the typechecking and translation semantics are implemented for different operators. Note that for syntactic convenience, we used Python's operator overloading and metaobject protocol substantially at the type level.%Our dispatch protocol was designed to be conceptually similar to Python's operator overloading protocol \cite{python}.

There are several other mechanisms that have been described as enabling forms of internal ``extension'' in the literature. These differ from our mechanism in one of the following ways, summarized in Fig. \ref{relatedtable}:
\begin{itemize}
\item Our mechanism is itself implemented as a library, rather than as a language dialect.
\item Our mechanism reuses an existing language's syntax, rather than offering syntax extension support. As we saw, this can still be reasonably natural, and allows the language to benefit from a variety of existing tools. We avoid many facets of the expression problem \cite{wadler1998expression} in this way.
\item Our mechanism permits true type system extensions. New types are not merely aliases, nor must their rules be admissible in some base type system. Term rewriting and macro systems, as well as systems that involve cross-compilation do not handle type system extensions at all. LMS, for example, uses Scala's type system, focusing instead on code generation and optimization \cite{Rompf:2012:LMS}. This is orthogonal to the goals we are seeking to achieve (e.g. the regular expression types we have described would be somewhat difficult to implement as libraries in Scala).
\item Techniques that allow global extensions to the syntax or treat the semantics as a ``bag of rules'' (e.g. SugarJ or rule injection systems like Xroma, Typed Racket (and other typed LISPs) and mbeddr) allow extensions so much control that there can be ambiguities. In Ace, there \emph{can never be more than one extension assigned as the delegate for a term}, so there are no ambiguities. 
\item Our mechanism allows users to control the target language of compilation from within libraries.
\end{itemize}

When the mechanisms available in an existing language prove insufficient, researchers and domain experts often design a new language. Previous work has considered adding statically-typed features to languages like Python in this way. For example, Terra embeds a low-level statically-typed language within Lua \cite{terra}. A number of tools have been developed to assist with this task, including compiler generators, language workbenches and domain-specific language frameworks (cf. \cite{erdweg2013state}). 
Extensible compilers like Xoc \cite{conf/asplos/CoxBCKK08} can be considered a form of language framework as well. It is difficult or impossible for these language-external approaches to achieve interoperability, because languages are not aware of each other's semantics and merging languages is not guaranteed to be unambiguous and sound. A few approaches (e.g. ) have made steps toward addressing the issue of ambiguity (albeit with some cost in expressiveness), but soundness is not guaranteed. In Ace (and in the theory, assuming a suitable packaging system), using different type constructors at the same time cannot cause type safety issues because representational consistency is checked. Representational consistency permits compositional reasoning.

Bidirectional type systems have been used for adding refinement checking to languages like ML and Twelf \cite{Lovas08abidirectional}. Refinement types can add stronger static logic, but do not have control over translation and have not been shown practical in a language as widely adopted as Python. We believe this is a fruitful avenue for future work. The Wyvern programming language uses bidirectional typechecking to control aspects of parsing in a manner similar to how we treat introductory forms, but Wyvern is also a language dialect and does not have an extensible type system \cite{Omar:2013:TWP:2489812.2489815}.

TODO: mention this http://blog.codeclimate.com/blog/2014/05/06/gradual-type-checking-for-ruby/

\section{Discussion}\label{discussion}
This work aimed to show that one can add static typechecking to a language like Python as a library, without undue syntactic overhead. Moreover, the type system is not fixed, but flexibly extensible. We achieve this by using a bidrectional type system and an elaboration semantics targeting a user-defined target language, along with per-function base semantic annotations. We show that the core of this mechanism leads to a surprisingly clean theoretical formalism that combines work on type-level computation, typed compilation and bidirectional type systems to enable type system extensions over a fixed, but flexible, syntax.

% Different dialects can be difficult to combine manually, as we discussed in the introduction. We have implemented the full OpenCL programming language (a variant of C99) as a library (though the details are left for future work), showing that such languages can instead be written as libraries, rather than as dialects.
% 
%\subsection{Compositional Reasoning}\label{safety}
%Every statement and expression in an Ace function is governed by exactly one active type, determined by the dispatch protocol, or by the single active base associated with the Ace function. The representational consistency check ensures that compilation is successful without requiring that types that abstract over other types (e.g. container types) know about their internal implementation details. Together, this permits compositional reasoning about the semantics of Ace functions even in the presence of many extensions. This stands on contrast to many prior approaches to extensibility, where extensions could insert themselves into the semantics in conflicting ways, making the order of imports matter (see Sec. \ref{related}). 


%\begin{enumerate}
%\item We address the ``chicken-and-egg'' problem by designing Ace, an extensible language  embedded within Python \cite{Politz:2013:PFM:2509136.2509536,python} entirely as a library. The top-level of an Ace file is a \emph{compilation script} written in Python and Python serves as Ace's \emph{type-level language}, as we will discuss.  Ace and Python thus share a common syntax and package system, so users of Ace can directly leverage established tools, infrastructure and coding standards. Functions marked by a decorator as under the control of Ace are, unlike Python functions, \emph{statically} typechecked. This makes it possible to rule out many potential issues ahead of execution, and thus  avoid  costly dynamic checks. %The semantics of  define run-time behavior.
%\item  % Instead, Ace has a statically-typed semantics that can be extended by users from within libraries. 
%There is \emph{no particular type system built into Ace}. Instead, a type is any  object implementing the \verb|ace.Type| interface constructed by the compilation script. We call these \emph{active types}. When statically assigning a type to a compound form, e.g. \verb|e.x|, the Ace {compiler} delegates to the type of a subexpression, e.g. to the type of \verb|e|, to determine how to assign a type to the expression as a whole. As we will show, this {type-directed protocol}, \emph{active typing}, permits the expression of a rich variety of type systems as libraries. Unlike syntax-directed protocols, there cannot be ambiguities when separately defined type sytems are combined (types control non-overlapping sets of expressions by construction). Base cases, e.g.  variables and certain statement forms, are handled on a {per-function basis} by a \emph{base semantics}, also a compile-time object implementing an interface, \verb|ace.Base|.
%\end{enumerate}
%% As a result, clients are able to import any combination of extensions with the confidence that link-time ambiguities cannot occur .
%
%The dynamic behavior of an Ace function is determined by translation to a \emph{target language}. We begin by  targeting Python, then discuss targeting OpenCL and CUDA, lower-level languages used to program many-core processors (e.g. GPUs). Active types and bases control translation by the same type-directed protocol that governs type assignment. We call this phase \emph{active translation}. A \emph{target} mediates this process and is also a user-defined compile-time object implementing an interface, \verb|ace.Target|. 
%%One base or type can support multiple targets. 
%When targeting a typed language, care must be taken to ensure that well-typed Ace expressions have well-typed translations, so we integrate a technique for compositionally reasoning about compiler correctness (developed for the TIL compiler for Standard ML \cite{tarditi+:til-OLD}) into the language.
%
%%TODO
%%
%%Static type systems are powerful tools for programming language design and implementation. By tracking the type of a value statically, a typechecker can verify the absence of many kinds of errors over all inputs. This simplifies and increases the performance of the run-time system, as errors need not be detected dynamically using tag checks and other kinds of assertions.
%
%It is legitimate to ask, however, why dynamically-typed languages are so widely-used in multiple domains. Although slow and difficult to reason about, these languages generally excel at satisfying the criteria of \textbf{ease-of-use}. More specifically, Cordy identified the principle of \emph{conciseness} as elimination of
%redundancy and the availability of reasonable defaults \cite{cordy1992hints}. Statically-typed languages, particularly those that professional end-users are exposed to, are often verbose, requiring explicit and often redundant type annotations on each function and variable declaration, separate header files, explicit template headers and  instantiation and other sorts of annotations.  Dynamically-typed languages, on the other hand, avoid most of this overhead by relying on support from the run-time system. Ace was first conceived to explore the question: \emph{does conciseness require run-time mechanisms, or can one develop a statically-typed language with the same usability profile?}
%
%
%Rather than designing a new syntax, or modifying the syntax of an existing language, we chose to utilize, \emph{without modification}, the syntax of an existing language, Python. This choice was not arbitrary, but rather a key means by which Ace satisfies extrinsic design criteria related to familiarity and tool support. Researchers often dismiss the importance of syntax. By repurposing a well-developed syntax, they no longer need to worry about the ``trivial'' task of implementing it.
%
%Most programming languages are {\em monolithic} -- a collection of primitives are given first-class treatment by the language implementation, and users can only creatively combine them to implement  abstractions of their design. Although highly-expressive general-purpose mechanisms have been developed (such as object systems or functional datatypes), these may not suffice when researchers or domain experts wish to evolve aspects of the type system, exert control over the representation of data, introduce specialized run-time mechanisms, if defining an abstraction in terms of existing mechanisms is unnatural or verbose, or if custom error messages are useful. In these situations, it would be desirable to have the ability to modularly extend existing systems with new compile-time logic and be assured that such extensions will never interfere with one another when used in the same program.
%
%Python does not truly prevent extensions from interfering with one another because it lacks, e.g., data hiding mechanisms. One type could change the implementation of another by replacing its methods, for example. But these kinds of conflicts do not occur ``innocently'', as conflicts between two libraries in SugarJ that use similar syntax might.
% 
%
%
%
Ace has several limitations still. Debuggers and other tools that rely not just on Python's syntax but also its semantics cannot be used directly, so if code generation introduces significant complexity that leads to bugs, this can be an issue. We believe that active types can be used to control debugging, and plan to explore this in the future. We have also not yet evaluated the feasibility of implementing more advanced type systems (e.g. linear, dependent or flow-dependent type systems) using Ace. In particular, it is difficult to orthogonally implement type systems that use different contexts for typing, because the base controls the context (though hacks are possible).
%
Not all extensions will be useful. Indeed, some language designers worry that offering too much flexibility to users leads to abuse (this is, for example, widely credited as the reason why Java doesn't support operator overloading). We do not argue with this point. Instead, we argue that the potential for abuse must be balanced with the possibilities made available by a vibrant ecosystem of competing statically-typed abstractions that can be developed and deployed as libraries, and thus more easily evaluated in the wild. With an appropriate community process, this could lead to a convergence toward stable collections of curated, high-quality collections more quickly than is possible today. %We also anticipate that coding guidelines and tools mandating the use of abstractions that are known to have certain desirable properties will replace designer-mandated enforcement.
% 
%This paper is presented as a language design paper. The theoretical foundations of this work lie in type-level computation. We have developed a simplified, type-theoretic formalism of active typechecking and translation, where we prove the safety properties we have outlined here, as well as several additional ones that are only possible to achieve using a typed metalanguage (currently in submission at ESOP 2014). The work described here  extends that mechanism in several directions: Ace supports a rich syntax, makes syntax trees available to extensions, supports a pluggable backend and per-function base semantics. It is implemented in an existing language and supports a wider range of practical extensions. 

The mechanisms described here can be implemented within any language that offers some form of quotation of function ASTs, capturing of function closures, and a reasonably flexible collection of syntactic forms and basic reflection facilities.% It is particularly useful when the language supports interoperability layers with a number of target languages. 

%\acks
%The author is grateful to Jonathan Aldrich, Robert Bocchino and anonymous referees for their useful suggestions. This work was funded by the DOE Computational Science Graduate Fellowship under grant number DE-FG02-97ER25308.
%Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrv}

% The bibliography should be embedded for final submission.

\bibliography{../research}
%\softraggedright
%P. Q. Smith, and X. Y. Jones. ...reference text...

\appendix
\section{Source Code Emission}
Portions of the \verb|Target| class related to source code emission are shown. If compilation occurred externally, the \verb|acec| library asks the targets used by the top-level typed functions to generate one or more files by deciding on a name and file extension on the basis of the name of the compilation script, and emitting code and \emph{snippets}, which are simply strings corresponding to functions, type declarations, imports and other top-level entities in the target language, each inserted at a \emph{location}. Each file being generated is a location, and locations might also be nested, depending on the language (e.g. the import block may be a separate location when generating Java, or the body of the \verb|<head>| tag when generating HTML+CSS+Javascript). Snippets are only added once to a particular location (so imports are not duplicated, for example).
%We leave layering extensions to Python to created typed targets as future work.

\begin{codelisting}
\begin{lstlisting}
class PyTarget(ace.Target):
  class PyAST(object): """Base class for ASTs"""
  target_type = ast
  class Attribute(PyAST, ast.Attribute):
    def emit(self, cg):
      cg.append(self.value.emit(cg), '.', self.attr)
  # ... 
  class AnonModule(object): 
    def __init__(self, name): 
      self.name = name
      self.guid = Guid()
      
    def emit(self, cg):
      import_loc = cg['imports']
      _cache = AnonModule._idx_cache[import_loc]
      if self not in _cache:
        anon_name = '__ace_' + str(len(_cache)+1)
        imp_stmt = ('import ' + self.name + ' as ' + 
          anon_name + cg.newline)
        _cache[self] = (anon_name, imp_stmt)
      else: 
        anon_name, imp_stmt = _cache[self]
      imports.add_snippet(imp_stmt)
      cg.append(anon_name)
      
  def emit(self, script_name, cg):
    file_name = '_' + script_name + '.py'
    if file_name not in generator:
      if 'imports' not in generator[file_name]:
        generator[file_name].push_loc('imports')
      if 'main' not in format_set[file_name]:
        generator[file_name].push_loc('main')
    translation.emit((file_name, 'main'))
\end{lstlisting}
\caption{Portions of the target showing how non-local code emission works.}
\label{cg}
\end{codelisting}

\section{Kinding}
\begin{figure}[t]
\small
%\vspace{-15pt}
$\fbox{\inferrule{}{\tKindX{\tau}{\kappa}}}$
%~~~$\chi ::= \cdot \pipe \checkmark$
\begin{mathpar}
\small\inferrule[k-var]{
}{
  \tKind{\tvarCtxX{t}{\kappa}}{\fvalCtx}{\tvar{t}}{\kappa}
}
~~~~
\inferrule[k-arrow-i]{
  \tKind{\tvarCtxX{t}{\kappa_1}}{\fvalCtx}{\tau}{\kappa_2}
}{
  \tKindX{\tlam{t}{\kappa_1}{\tau}}{\karrow{\kappa_1}{\kappa_2}}
}
~~~~
\inferrule[k-arrow-e]{
  \tKindX{\tau_1}{\karrow{\kappa_1}{\kappa_2}}\\\\
  \tKindX{\tau_2}{\kappa_1}
}{
  \tKindX{\tapp{\tau_1}{\tau_2}}{\kappa_2}
}

\text{\color{gray} (kinding for integers, labels, lists, products and sums also standard)}

%%
%%\inferrule{ }{
%%	\tKindX{\tstr{str}}{\kstr}
%%}(\text{str}^I_\tau)
%%
%%\inferrule{ }{
%%	\tKindX{\tunit}{\kunit}
%%}(\text{1}^I_\tau)
%%
%%\inferrule{
%%	\tKindX{\tau_1}{\kappa_1}\\
%%	\tKindX{\tau_2}{\kappa_2}
%%}{
%%	\tKindX{\tpair{\tau_1}{\tau_2}}{\kpair{\kappa_1}{\kappa_2}}
%%}({\times}^I_\tau)
%%
%%\inferrule{
%%	\tKindX{\tau}{\kpair}{\kappa_1}{\kappa_2}
%%}{
%%	\tKindX{\tfst{\tau}}{\kappa_1}
%%}({\times}^{E1}_\tau)
%%
%%\inferrule{
%%	\tKindX{\tau}{\kpair}{\kappa_1}{\kappa_2}
%%}{
%%	\tKindX{\tsnd{\tau}}{\kappa_2}
%%}({\times}^{E2}_\tau)
%%\inferrule{ }{
%%\inferrule{
%%	\tKindX{\tau_{1}}{\kappa_{1}}\\
%%	\tKindX{\tau_{2}}{\kappa_{2}}
%%}{
%%	\tKindX{\tpair{\tau_{1}}{\tau_{2}}}{\kpair{\kappa_{1}}{\kappa_{2}}}
%%}~(\times_\tau)
%%
%%\inferrule{
%%	\tKindX{\tau}{\kpair{\kappa_{1}}{\kappa_{2}}}
%%}{
%%	\tKindX{\tfst{\tau}}{\kappa_{1}}
%%}~(\text{fst}_\tau)
%%
%%\inferrule{
%%	\tKindX{\tau}{\kpair{\kappa_{1}}{\kappa_{2}}}
%%}{
%%	\tKindX{\tsnd{\tau}}{\kappa_{2}}
%%}~(\text{snd}_\tau)
%%
%% TODO: list
\inferrule[k-eq]{
	\kEq{\kappa}\\
	\tKindX{\tau_1}{\kappa}\\
	\tKindX{\tau_2}{\kappa}\\\\
	\tKindX{\tau_3}{\kappa'}\\
	\tKindX{\tau_4}{\kappa'}
}{
	\tKindX{\tifeq{\tau_1}{\tau_2}{\kappa}{\tau_3}{\tau_4}}{\kappa'}
}

\inferrule[k-ty-i]{
	\fval{tycon}{\kappaidx}{-}{-} \in \fvalCtx\\\\
	\tKind{\tvarCtx}{\fvalCtx}{\tauidx}{\kappaidx}
}{
	\tKindX{\ttype{tycon}{\tauidx}}{\kTypeBlur}
}

\inferrule[k-ty-e]{
	\tKindX{\tau}{\kTypeBlur}\\
	\fval{tycon}{\kappaidx}{-}{-} \in \fvalCtx\\\\
	\tKind{\tvarCtxX{x}{\kappaidx}}{\fvalCtx}{\tau_1}{\kappa}\\
	\tKindX{\tau_2}{\kappa}
}{
	\tKind{\tvarCtx}{\fvalCtx}{\tfamcase{\tau}{tycon}{x}{\tau_1}{\tau_2}}{\kappa}
}
~~~~~~
\inferrule[k-d-i]{
	\tKindX{\tau_1}{\kTypeBlur}\\\\
	\tKindX{\tau_2}{\kITerm}
}{
	\tKindX{\tden{\iota}{\tau}}{\kDen}
}

%\inferrule[k-d-i-checked]{
%	\tKind{\Delta}{\fvalCtx}{\tau}{\kTypeBlur}\\\\
%	\isItermX{\ibar}
%}{
%	\tKind{\Delta}{\fvalCtx}{\tden{\ibar}{\tau}^\checkmark}{\kDen}
%}
%~~~~~~~
\inferrule[k-d-e]{
	\tKindX{\tau}{\kDen}
}{
	\tKindX{\ttypeof{\tau}}{\kTypeBlur}
}
%
%\inferrule[iterm-intro]{
%	\isIterm{\tvarCtx}{\emptyctx}{\fvalCtx}{\iota}
%}{
%	\tKindX{\titerm{\iota}}{\kITerm}
%}

\inferrule[k-reptype-i]{
	\isItype{\tvarCtx}{\fvalCtx}{\sbar}
}{
	\tKindX{\titype{\sigma}}{\kIType}
}
\end{mathpar}
%$\fbox{$\isItermX{\ibar}$}$
%\begin{mathpar}
%\inferrule[k-i-var]{ }{
%	\isItermX{\evar{x}}
%}
%~~~~~~
%\inferrule[k-i-fix]{
%	\isItypeX{\sbar}\\
%	\isItermX{\ibar}
%}{
%	\isItermX{\ifix{\evar{x}}{\sbar}{\ibar}}
%}
%~~~~~~
%\inferrule[k-i-transof]{
%	\tKindX{\tau}{\kDen}
%}{
%	\isItermX{\itransof{\tau}}
%}
%%\inferrule[i-lam-kinding]{
%%	\isItypeX{\sbar}\\\\
%%	\isItermX{\ibar}
%%}{
%%	\isItermX{\ilam{\evar{x}}{\sbar}{\ibar}}
%%}
%
%\text{\color{gray} (omitted rules are trivially recursive, like \textsc{k-i-fix})}
%%\inferrule[iterm-dereify-kinding]{
%%	\tKindX{\tau}{\kITerm}
%%}{
%%	\isItermX{\iup{\tau}}
%%}
%%
%\end{mathpar}
$\fbox{$\isItypeX{\sbar}$}$
\begin{mathpar}
%\inferrule[i-int-kinding]{ }{
%	\isItypeX{\dint}
%}
%
%\inferrule[i-prod-kinding]{
%	\isItypeX{\sigma_1}\\\\
%	\isItypeX{\sigma_2}
%}{
%	\isItypeX{\dpair{\sigma_1}{\sigma_2}}
%}
%
\inferrule[k-s-unquote]{
	\tKindX{\tau}{\kIType}
}{
	\isItypeX{\dup{\tau}}
}

%\inferrule[k-s-repof]{
%	\tKindX{\tau}{\kTypeBlur}
%}{
%	\isItypeX{\trepof{\tau}}
%}
%
\text{\color{gray} (omitted forms are trivially recursive)}
\end{mathpar}
\caption{\small Kinding for type-level terms. The kinding context $\tvarCtx$ maps type variables to kinds.}
\label{tlkind}
\end{figure} 
Some of the kinding rules for type-level terms are shown.
%
%\section{Complex Numbers}
%We have implemented complex numbers (which have a logic similar to decimals, which we added late in the paper cycle). The API is somewhat dated, apologies for
%
%If \verb|c| is a complex number, then \verb|c.ni| and \verb|c.i| are its non-imaginary and imaginary components, respectively. These expressions are of the form \verb|Attribute|, so the typechecker calls \verb|type_Attribute| (line \ref{cplx}.7).
%This method receives the compilation context, \verb|context|, and the abstract syntax tree of the expression, \verb|node| and must return a type assignment for the node, or raise an \verb|ace.TypeError| if there is an error. In this case, a type assignment is possible if the attribute name is either \verb|"ni"| or \verb|"i"|, and an error is raised otherwise (lines \ref{cplx}.8-\ref{cplx}.10). We note that error messages are an important and sometimes overlooked facet of {ease-of-use} \cite{marceau2011measuring}. A common frustration with using general-purpose abstraction mechanisms to encode an abstraction is that they can produce  verbose and cryptic error messages that reflect the implementation details instead of the semantics. Ace supports custom error messages.%Active types permit direct encoding of semantics and customization of error messages.
%
%Complex numbers also support binary arithmetic operations partnered with both other complex numbers and with non-complex numbers, treating them as if their imaginary component is zero. The typechecking rules for this logic is implemented on lines \ref{cplx}.17-\ref{cplx}.29. Arithmetic operations are usually symmetric, so the dispatch protocol checks the types of both subexpressions for support. To ensure that the semantics remain deterministic in the case that both types support the binary operation, Ace asks the left first (via \verb|type_BinOp_left|), asking the right (via \verb|type_BinOp_right|) only if the left indicates an error. In either position, our implementation begins by recursively assigning a type to the other operand in the current context via the \verb|context.type| method (line \ref{cplx}.24). If supported, it applies the C99 rules for arithmetic operations to determine the resulting type (via \verb|c99_binop_t|, not shown). 
%
%Finally, a complex number can be constructed inside an Ace function using Ace's special constructor form: \verb|[clx.Cplx](3,4)| represents $3+4i$, for example. The term within the braces is evaluated at \emph{compile-time}. Because \verb|clx.Cplx| evaluates not to an active type, but to a class, this form is assigned a type by handing control to the class object via the \emph{class method} \verb|type_New|. It operates as expected, extracting the types of the two arguments to construct an appropriate complex number type (lines \ref{cplx}.50-\ref{cplx}.57), raising a type error if the arguments cannot be promoted to a common type according to the rules of C99 or if two arguments were not provided.% (an exercise for the reader: modify this method to also allow a single argument for when the imaginary part is 0). 
%
%As seen in Listing \ref{mapout}, we are implementing complex numbers internally using OpenCL vector types, like \verb|int2|. Let us look first at \verb|trans_New| on lines \ref{cplx}.54-\ref{cplx}.60, where new complex numbers are translated to vector literals by invoking \verb|target.VecLit|. This will ultimately  generate the necessary OpenCL code, as a string, to complete compilation (these strings are not directly manipulated by extensions, however, to avoid problems with, e.g. precedence). For it to be possible to reason compositionally about the correctness of compilation, all complex numbers must translate to terms in the target language that have a consistent target type. The \verb|trans_type| method of the \verb|ace.ActiveType| associates a type in the target language, here a vector type like \verb|int2|, with the active type. Ace supports a mode where this \emph{representational consistency} is dynamically checked during compilation (requiring that the active target know how to assign types to terms in the target language, which can be done  for our OpenCL target as of this writing).
%
%The translation methods for attributes (line \ref{cplx}.12) and binary operations  (line \ref{cplx}.31) proceed in a straightforward manner. The context provides a method, \verb|trans|, for recursively determining the translation of subexpressions as needed. Of note is that the translation methods can assume that typechecking succeeded. For example, the implementation of \verb|trans_Attribute| assumes that if \verb|node.attr| is not \verb|'ni'| then it must have been \verb|'i'| on line \ref{cplx}.14, consistent with the implementation of \verb|type_Attribute| above it. Typechecking and translation are separated into two methods to emphasize that typechecking is not target-dependent, and to allow for more advanced uses, like type refinements and hypothetical typing judgements, that we do not describe here.
%
%\begin{codelisting}
%\lstinputlisting{listing9.py}
%\caption{\texttt{[}in \texttt{examples/clx.py]} The active type family \texttt{Ptr} implements the semantics of OpenCL pointer types.}
%\label{cplx}
%\end{codelisting}
%\section{Expressiveness}\label{examples}
%Thus far, we have focused mainly on the OpenCL target and shown examples of fairly low-level active types: those that implement OpenCL's primitives (e.g. \verb|clx.Ptr|), extend them in simple but convenient ways (e.g. \verb|clx.Cplx|) and those that make interactive execution across language boundaries safe and convenient (e.g. \verb|clx.NPArray|). 
%Ace was first conceived to answer the question: \emph{can we build a statically-typed language with the semantics of a C but the syntax and ease-of-use of a Python?} We submit that the work described above has answered this question in the affirmative. 
%
%But Ace has proven useful for more than low-level tasks like programming a GPU with OpenCL. We now describe several interesting extensions that implement the semantics of primitives drawn from a range of different language paradigms, to justify our claim that these mechanisms are highly expressive. 
%
%\subsection{Growing a Statically-Typed Python Inside an Ace}
%Ace comes with a target, base and type implementing Python itself: \verb|ace.python.python|, \verb|ace.python.base| and \verb|ace.python.dyn|. These can be supplemented by additional active types and used as the foundation for writing actively-typed Python functions. These functions can either be compiled ahead-of-time to an untyped Python file for execution, or be immediately executed with just-in-time compilation, just like the OpenCL examples we have shown. Many of the examples in the next section support this target in addition to the OpenCL target we have focused on thus far.
%
\section{Recursive Labeled Sums}
Listing 12 shows how recursive labeled sums (i.e. functional datatypes) would look.
\begin{codelisting}
\lstinputlisting{datatypes_t.py}
\caption{\texttt{[datatypes\_t.py]} An example using statically-typed functional datatypes.}
\label{datatypest}
\end{codelisting}
%\begin{codelisting}
%\lstinputlisting{datatypes.py}
%\caption{\texttt{[datatypes.py]} The dynamically-typed Python code generated by running \texttt{acec datatypes\_t.py}.}
%\label{datatypes}
%\end{codelisting}
%Listing \ref{datatypest} shows an example of the use of statically-typed functional datatypes (a.k.a. recursive labeled sum types) together with the Python implementation just described. It shows two syntactic conveniences that were not mentioned previously: 1) if no target is provided to \verb|ace.fn|, the base can provide a default target (here, \verb|ace.python.python|); 2) a concrete function can be generated immediately by providing a type assignment after \verb|ace.fn| using braces. Listing \ref{datatypest} generates Listing \ref{datatypes}.
%
%Lines \ref{datatypest}.3-\ref{datatypest}.11 define a function that generates a recursive algebraic datatype representing a tree given a name and another Ace type for the data at the leaves. This type implemented by the active type family \verb|fp.Datatype|. A name for the datatype and the case names and types are provided programmatically on lines \ref{datatypest}.3-\ref{datatypest}.8. To support recursive datatypes, the case names are enclosed within a lambda term that will be passed a reference to the datatype itself. These lines also show two more active type families: units (the type containing just one value), and immutable pairs. Line \ref{datatypest}.13 calls this function with the Ace type for dynamic Python values, \verb|py.dyn|, to generate a type, aliased \verb|DT|. This type is implemented using class inheritance when the target is Python, as seen on lines \ref{datatypes}.1-\ref{datatypes}.13. For C-like targets, a \verb|union| type can be used (not shown).
%
%The generic function \verb|depth_gt_2| demonstrates two features. First, the base has been setup to treat the final expression in a top-level branch as the return value of the function, consistent with typical functional languages. Second, the \verb|case| ``method'' on line \ref{datatypest}.17 creatively reuses the syntax for dictionary literals to express a nested pattern matching construct. The patterns to the left of the colons are not treated as expressions (that is, a type and translation is never recursively assigned to them). Instead, the active type implements the standard algorithm for ensuring that the cases cover all possibilities and are not redundant \cite{pfpl}. If the final case were omitted, for example, the algorithm would statically indicate an error, just as in statically-typed functional languages like ML.
%
%%\subsection{Nested Data Parallelism}
%%Functional constructs have shown promise as the basis for a nested data-parallel parallel programming model. Many powerful parallel algorithms can be specified by composing primitives like \verb|map| and \verb|reduce| on persistent data structures like lists, trees and records. Data dependencies are directly encoded in these specifications, and automatic code generation techniques have shown promise in using this information to automatically execute these specifications on concurrent hardware and networks. The Copperhead programming language, for example, is based in Python as well and allows users to express functional programs over lists using common functional primitives, compiling them to CUDA code \cite{catanzaro2011copperhead}. By combining active types and the metaprogramming facilities described in Sec. 2 to implement optimizations, like fusion, on the typed syntax trees available from concrete functions, these same techniques can be implemented as an Ace library as well.
%
%\subsection{Product Types}
%Product types (types that group heterogeneous values together) like structs, unions, tuples, records and objects can all be implemented as Ace type families  parameterized by a dictionary mapping field names (indices, in the case of tuples) to types. Each family differs slightly in the semantics of the operations available. Structs support mutation, for example, while records do not. Object systems typically introduce additional logic for calling methods, binding special variables like \verb|this|, and accessing information through an inheritance hierarchy. Listing \ref{ooclxfppy} shows an example of each of these. The prototypic object system delegates to a backing record if a field is not available in the foreground. This example also demonstrates cross-compilation to C99, supported for a subset of operations in \verb|clx|. All three abstractions are implemented as simple \verb|struct|s, as can be seen in Listing \ref{ooclxfpc}. This example further demonstrates the use of argument annotations to generate concrete functions (line \ref{ooclxfppy}.9), which is a feature only available when using Python 3.3+. 
%
%\subsection{Distributed Programming}
%Many distributed programming abstractions can be understood as implementing object models with complex forms of dynamic dispatch. For example, in Charm++, dynamic dispatch involves message passing over a dynamically load-balanced network \cite{kale2009charm++}. While Charm++ is a sophisticated system, and we do not anticipate that implementing it as an Ace extension would be easy, it is possible to do so by targeting a system like Adaptive MPI, which exposes the Charm++ runtime system \cite{kale2009charm++}. %Erlang and other message-passing and actor-based systems can also be considered in this manner -- processes can be thought of as objects, channels as methods and messages as calls.
%
%%\subparagraph{Annotation and Extension Inference}
%%In addition to type annotations, OpenCL normally asks for additional annotations in a number of other situations.  Users can annotate functions that meet certain requirements  with the \verb|__kernel| attribute, indicating that they are callable from the host. The \verb|OpenCL| backend can check these and add this annotation automatically. Several types (notably, \verb|double|) and specialized primitive functions require that an OpenCL extension be enabled via a \verb|#pragma|. The OpenCL backend automatically detects many of these cases as well, adding the appropriate \verb|#pragma| declaration. An example of this can be seen in Listing \ref{mapout}, where the use of the \verb|double| type triggers the insertion of the \verb|cl_khr_f64| extension.
%%
%
%A number of recent languages designed for distributed programming on clusters use a partitioned global address space model, e.g. Chapel \cite{chapel}. These languages provide first-class support for accessing data transparently across a massively parallel cluster. Their type systems track information about \emph{data locality} so that the compiler can emit more efficient code. The extension mechanism can also track this information in a manner analogous to how address space information is tracked in the OpenCL example above, and target an existing portable run-time such as GASNet \cite{bonachea2002gasnet}.
%
%\subsubsection{Units of Measure}
%A number of domain-specific type systems can be implemented within Ace as well. For example, prior work has considered tracking units of measure (e.g. grams) statically to validate scientific code \cite{conf/cefp/Kennedy09}. This cannot easily be implemented using many existing abstraction mechanisms because this information should only be maintained statically to avoid excessive run-time overhead associated with tagging/boxing, and the typechecking logic is reasonably complex. The Ace extension mechanism allows this information to be tracked in the type system, but not included during translation, by a mechanism similar to how address space information is tracked with pointers. Because Ace extensions can use Python, the needed logic can be implemented directly.
%
%
%\subsection{OOFPCLX}
%\begin{codelisting}[t]
%\lstinputlisting{ooclxfp.py}
%\caption{\texttt{[ooclxfp.py]} An example combining structs and immutable records using a prototype-based object system, cross-compiled to C99. Uses Python 3 argument annotations.}
%\label{ooclxfppy}
%\end{codelisting}
%\begin{codelisting}[t]
%\lstinputlisting[style=OpenCL]{ooclxfp.c}
%\caption{\texttt{[ooclxfp.c]} The C99 code generated by running \texttt{acec ooclxfp.py}.}
%\label{ooclxfpc}
%\end{codelisting}


\end{document}
