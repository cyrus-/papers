%\documentclass[12pt]{article}
\documentclass[preprint,10pt]{sigplanconf}
\newcommand{\typy}{\texttt{typy}}
\newcommand{\sigmat}[1]{\sigma_{\text{#1}}}

\newcommand{\F}[1]{{\sf #1}~}
\newcommand{\FF}[1]{{\sf #1}}
\newcommand{\Q}{\FF{Arg}}
\usepackage{sfmath}
\usepackage{times}
%\setlength{\parskip}{0pt}
%\setlength{\parsep}{0pt}
%\setlength{\headsep}{10pt}
%\setlength{\topskip}{0pt}
%\setlength{\topmargin}{0pt}
%\setlength{\topsep}{0pt}
%\setlength{\partopsep}{0pt}
\usepackage[colorlinks=true,allcolors=blue,backref,pageanchor=true,plainpages=false, pdfpagelabels, bookmarks,bookmarksnumbered,
pdfborder={0 0 0},  %removes outlines around hyper links in online display
]{hyperref}

\usepackage{subfigure}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
\usepackage{mathpartir}
\usepackage{amsmath}
\usepackage{amssymb} 
%\usepackage{amsthm}
%\newtheorem{corollary}{Corollary}
\usepackage{ stmaryrd }
\usepackage{verbatimbox}
\usepackage{alltt}
\renewcommand{\ttdefault}{txtt}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{light-gray}{gray}{0.5}
\usepackage{listings}
\usepackage{wasysym}
 \usepackage{longtable}
 \usepackage{booktabs}
 \usepackage{pdflscape}
 \usepackage{colortbl}%
 \newcommand{\myrowcolour}{\rowcolor[gray]{0.925}}
 \usepackage{wasysym}
\lstset{
  language=Python,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=4,
  commentstyle=\itshape\color{light-gray},
  basicstyle=\ttfamily\footnotesize,
  morekeywords={lambda, self, assert, cls, @classmethod, with,as},
  deletendkeywords={tuple,id,buffer,map},
  numbers=left,
  numberstyle=\scriptsize\color{light-gray}\textsf,
  numbersep=0.5em,
  xleftmargin=1.5em,
  stringstyle=\color{mauve},
  escapeinside={^}{^},
  aboveskip=4pt,
  belowskip=0pt
}
\lstdefinestyle{Bash}{
    language={}, 
    numbers=left,
    numberstyle=\scriptsize\color{light-gray}\textsf,
    moredelim=**[is][\color{blue}\bf\ttfamily]{`}{`}
}
\lstdefinestyle{OpenCL}{
  language=C++,
  morekeywords={kernel, __kernel, global, __global, size_t, get_global_id, sin, printf, int2}
}

\newcommand{\lip}[1]{\lstinline[language=Python,basicstyle=\ttfamily\footnotesize,morekeywords={with},deletendkeywords={tuple,buffer,map}]{#1}}
\newcommand{\li}[1]{\lip{#1}}

\usepackage{float}
\floatstyle{ruled}
\newfloat{codelisting}{tp}{lop}
\floatname{codelisting}{Listing}
% \setlength{\floatsep}{4pt}
\setlength{\textfloatsep}{10pt plus 1.0pt minus 4.0pt}
% \setlength{\intextsep}{10pt}
% \setlength{\belowcaptionskip}{0pt}
% \usepackage[bottom]{footmisc}
\setlength{\skip\footins}{3px plus 10px minus 5px}
% \setlength{\footnotesep}{0px}
\definecolor{very-light-gray}{gray}{0.9}


\usepackage{url}

\usepackage{todonotes}

\usepackage{placeins}

\usepackage{textpos}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\usepackage{microtype}
%\lefthyphenmin=8
\sloppy
\makeatletter
\renewcommand\section{\@startsection{section}{1}{\z@}%
                      {-6\p@ \@plus 4\p@ \@minus -4\p@}%
                      {2\p@ \@plus 4\p@ \@minus 4\p@}%
                      {\normalfont\large\bfseries\boldmath
                       \rightskip=\z@ \@plus 8em\pretolerance=10000 }}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
                      {-6\p@ \@plus 4\p@ \@minus -4\p@}%
                      {2\p@ \@plus 4\p@ \@minus 4\p@}%
                      {\normalfont\normalsize\bfseries\boldmath
                       \rightskip=\z@ \@plus 8em\pretolerance=10000 }}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
                      {-6\p@ \@plus -4\p@ \@minus -4\p@}%
                      {2\p@ \@plus 4\p@ \@minus 4\p@}%
                      {\normalfont\normalsize\bfseries\boldmath}}
% \renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                      % {-4\p@ \@plus -4\p@ \@minus -4\p@}%
                      % {-0.5em \@plus -0.22em \@minus -0.1em}%
                      % {\normalfont\normalsize\itshape}}
\makeatother


    \renewcommand{\topfraction}{0.9}  % max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8} % max fraction of floats at bottom
\newcommand{\rulename}[1]{{\textsc{\textbf{#1}}}}
\def \TirNameStyle #1{\small\rulename{#1}}
\begin{document}

\conferenceinfo{MODULARITY'15,}{March 16--19, 2015, Fort Collins, CO, USA}
\CopyrightYear{2015}
\crdata{978-1-4503-3249-1/15/03}

\title{Programmable Semantic Fragments}
%\titlebanner{{\tt \textcolor{Red}{{\small Under Review -- distribute within CMU only.}}}}        % These are ignored unless
%\preprintfooter{Distribute within CMU only.}   % 'preprint' option specified.


%\authorinfo{~}{~}{~}
\authorinfo{Cyrus Omar \and Jonathan Aldrich}
          {          Carnegie Mellon University}
          {\{comar, aldrich\}@cs.cmu.edu\vspace{-10px}}   

\maketitle
\begin{abstract}
% \vspace{-2px}
%Evidence suggests that programmers are reluctant to adopt new languages to
%gain access to new abstractions, even when they agree that these abstractions
%could be valuable. This suggests a need for languages that satisfy two key
%growth criteria: compatibility with existing programming ecosystems; and
%internal extensibility, so that new abstractions do not require new
%languages.
%
%We introduce \texttt{typy}, a language compatible with the popular Python ecosystem.
%While Python, like other similar languages, is satisfactory for simple
%scripting, \texttt{typy} is designed for more complex situations where static
%typechecking is beneficial. Unlike most statically-typed languages, \texttt{typy}'s type
%system can be extended from within, by associating compile-time functions with
%type definitions. The compiler selectively invokes these according to a
%type-directed protocol, avoiding interference problems. Despite these design
%constraints, active types in \texttt{typy} can express the static and dynamic semantics
%of a range of functional, object-oriented, parallel and domain-specific
%abstractions, all within libraries.


% Programmers can be convinced to adopt new libraries more easily than they can be convinced to adopt new languages. Unfortunately, new languages are seen as the only viable vehicle for constructs that introduce new static type structure. %Our aim in this paper is to introduce a mechanism that allows abstraction providers to extend the type system of a language from within, in a way that guarantees that any combination of extensions can be used together without worrying about conflicts. 
% Our aim in this paper is to show that there is another way. 

% Language designers package interesting semantic structures together into monolithic programming languages. This imposes barriers to adoption. % This paper introduces 

% This paper introduces a more compositional \emph{fragment-oriented} approach. 
This paper introduces $\typy$, a statically typed programming language embedded by reflection into Python. Rather than defining a monolithic semantics, $\typy$ features a \emph{fragmentary semantics}, i.e. it delegates semantic control over each term, drawn from Python's fixed syntax, to a contextually relevant user-defined \emph{semantic fragment}. 
% For example, \texttt{typy} delegates control over terms of literal form (e.g. dictionary literal form) to the fragment that defines the type that the term is being checked against. 
This fragment programmatically 1)  typechecks the term, and 2) assigns dynamic meaning to the term by computing its translation to Python. 

We argue that this design is \emph{expressive} with examples of fragments that express the static and dynamic semantics of 1) functional records; 2) a variation on JavaScript's prototypal object system; 3) labeled sums (with nested pattern matching \emph{a la} ML); and 4)  typed foreign interfaces to Python and OpenCL. These semantic structures are, or would need to be, defined primitively in other languages.

We further argue that this design is \emph{compositionally well-behaved}. It side\-steps the problems of  grammar composition and the expression problem by allowing different fragments to deterministically share a fixed syntax. Moreover, programs are semantically stable under fragment composition. %(unlike systems where the semantics is a ``bag of inference rules.'') 

% We formalize this mechanism as a small bidirectionally typed lambda calculus and prove that 

%Fragments are \emph{composable}: 

% This paper introduces \texttt{typy}, a library for defining statically typed extensions of Python from composable \emph{fragments}.  
% This paper introduces a programming language that allows library providers to modularly extend its semantic structure without modifying its syntax. The unit of semantic composition is the \emph{type structure}. 
 % The key idea is simple: rather than allowing fragments to extend Python's concrete or abstract syntax, the semantics delegates control over each term drawn from its fixed syntax to a contextually relevant fragment definition. 
%Similarly, the semantics delegates control over projection forms like \texttt{e.label} to the fragment that defined the type assigned to \texttt{e}. 
  %All external types are defined by some user-defined type structures (there are no ``primitive'' external types.)
%(This stands in contrast to  standard macro systems, which compute only term rewritings.)

% We have embedded this language into Python as a library, \texttt{typy}, using its primitive support for reflection and dynamic code generation. 
% We validate our approach with examples of non-trivial features that are built primitively into other languages but that are implemented compositionally using \texttt{typy}, including a statically typed variant of Javascript's prototypic object system and ML-style records and recursive datatypes with pattern matching.  % including functional constructs, an object system, a type system for secure string sanitation and a typed foreign function interface to OpenCL
%, @$\lambda$.  %We call type constructors bundled with statically valued functions of this sort \emph{active type constructors}, and this protocol \emph{active typechecking and translation}.%The key idea is to associate semantic logic with type constructors, rather than syntactic forms. 
%Both \texttt{typy} and @$\lambda$ have a fixed syntax. 
%The semantics of each term is determined by delegating control, during the process of typechecking and translating the term to the underlying \emph{internal language}, to methods associated with a relevant \emph{active type constructor}, defined in a library. This process is stable even as additional type constructors are imported, so ambiguities cannot arise. 
% By flexibly  repurposing a fixed syntax in this type-directed manner, we avoid ambiguities by construction. %In \texttt{typy}, types and functions are defined using Python's syntax, distributed using Python's package system and can call into Python libraries, so \texttt{typy} is uniquely backwards compatible. We show that active type constructors as implemented in \texttt{typy} permit practical library implementations of a variety of type system fragments that would normally need to be built into a language. %We also show how to construct safe, natural and extensible foreign function interfaces using a novel form of ``staged'' type propagation from Python into \texttt{typy} and then through to OpenCL. % can be used to enable type system extensions and demonstrates how type safety can be maintained by lifting a technique from the typed compilation literature into the language.%To show that the method scales, we give a full implementation of the OpenCL programming language as a library. %We briefly examine the type theoretic foundations of this approach and give modular criteria that guarantee that an extension is safe and modular. %\texttt{typy} can be used from the shell or interactively from within Python\todo{take this sentence out?}.
%Researchers developing languages and abstractions for high-performance computing must consider a number of design criteria, including performance, verifiability, portability and ease-of-use. Despite the deficiencies of legacy tools and the availability of seemingly superior options, end-users have been reluctant to adopt new language-based abstractions. We argue that this can be largely attributed to a failure to consider three additional criteria: continuity, extensibility\- and interoperability. This paper introduces \texttt{typy}, a language that aims to satisfy this more comprehensive set of design criteria. To do so, \texttt{typy} introduces several novel compile-time mechanisms, makes principled design choices, and builds upon existing standards in HPC, particularly Python and OpenCL. OpenCL support, rather than being built into the language, is implemented atop an extensibility mechanism that also admits abstractions drawn from other seemingly disparate paradigms. The core innovation underlying this and other features of \texttt{typy} is a novel reification of types as first-class objects at compile-time, representing a refinement to the concept of active libraries that we call \emph{active types}. We validate our overall design by considering a case study of a simulation framework enabling the modular specification and efficient execution of ensembles of neural simulations across a cluster of GPUs.
\end{abstract}

% \category{D.3.2}{Programming Lan\-guages}{Language Classifications}[Extensible Languages]
%\category{D.3.4}{Programming Languages}{Processors}[Compilers]
%\category{F.3.1}{Logics \& Meanings of Programs}{Specifying and Verifying and Reasoning about Programs}[Specification Techniques]
% \keywords
% type systems; extensible languages; metaprogramming
% \vspace{-2px}
\section{Introduction}\label{intro}
% \vspace{-2px}
% Established programming languages like JavaScript and Java often serve as compilation targets for languages with alternative type structure. For example, TypeScript \cite{typescript} and Flow \cite{flow} extend JavaScript with different object systems, and PureScript \cite{purescript} is a functional language in the tradition of ML and Haskell that can be compiled to JavaScript.

% As dynamic languages like JavaScript, Python and Lua grow, they often sprout dialects that introduce richer static type structure. 


% Programmers rarely adopt new programming languages. 
%
% Programmers are more willing to consider constructs defined in libraries than  constructs defined as primitives of a new programming language. 
% This is true even for programmers who have been convinced of the value of many of the features found only in a new language. %The reason for this has fundamentally to do with \emph{compositionality}: there is no principled way to interface between languages.

% Empirical studies of programming language adoption have found that extrinsic criteria  like library availability, tool support and familiarity are as important to practicing programmers as semantic criteria \cite{Meyerovich:2013:EAP:2509136.2509515,chen05}. This tends to favor established languages, which incorporate new ideas slowly and whimsically. Consequently, many interesting and potentially useful semantic constructs have been left languishing in obscurity.% It also makes these features difficult to evaluate at scale% These extrinsic criteria are, of course, quite difficult to satisfy. %Unfortunately, many interesting language constructs seemingly cannot be expressed as libraries within the general-purpose languages that developers use today, limiting their adoption.%This suggests that many interesting constructs that programmers might benefit from  because  adopt because they 
% This has left many interesting constructs that programmers might benefit from languishing in obscure languages, unable even to be evaluated at scale.
% Unfortunately, it is not apparent that there is any other way to advance the state of the art without developing new languages.
% This means that innovative constructs that are housed in new languages are often .  

% Computing is awash in programming languages. The problem with this proliferation of languages is that it is difficult to interface with libraries written in sibling languages. 

% Our aim in this paper is to help rescue these constructs. We develop a mechanism that allows constructs that today are found only as primitives of obscure languages to be expressed as composable libraries for a more widely adopted language, Python. 
As programming languages proliferate, programmers face the daunting problem of \emph{lateral compatibility}, i.e. of interfacing with libraries written in sibling languages. For example, there are useful libraries written in TypeScript \cite{bierman2014understanding}, Flow \cite{Flow} and PureScript \cite{PureScript}, but these libraries are not directly accessible across language boundaries because these languages are all syntactically and semantically incompatible with one another. (The first two define different object systems, and PureScript is a functional language similar to Haskell and ML.) 

One common workaround is to interface indirectly with libraries written in sibling languages through the code generated by a compiler that targets a more established language for which a \emph{foreign interface (FI)} is available. For example, all of the languages above have compilers that target JavaScript and they are all capable of interfacing with JavaScript. Unfortunately, this approach is unnatural (the syntactic and semantic conveniences of the sibling language are unavailable) and unsafe (the type system of the sibling language is not enforced, and the internal representations of the compiler are exposed.) Although the safety problem can be mitigated by inserting dynamic checks \cite{DBLP:journals/toplas/MatthewsF09}, it appears that the language-oriented approach \cite{journals/stp/Ward94} is difficult to reconcile with the best practices of ``programming in the large'' \cite{DeRemer76}. 

% An alternative approach would be to ask the designers of these languages to introduce a safe FFI directly targeting the other languages. This approach clearly doesn't scale -- defining a safe FFI amounts essentially to embedding the foreign language into the calling language (e.g. see MLJ \cite{Benton:1999:IWW:317636.317791}


% When choosing a programming language, developers prize library availability  \cite{Meyerovich:2013:EAP:2509136.2509515,chen05}.  
% In response, new languages tend either to extend a language that has an established  ecosystem of libraries like Java, JavaScript or Python, or to define a safe and low-cost foreign function interface (FFI) to such a language. For example, TypeScript  and Flow  extend JavaScript with different  object systems, and PureScript  is a functional language similar to Haskell that interfaces cleanly with JavaScript.
%  %\emph{Language workbenches} \cite{erdweg2013state} have made it easy to develop sophisticated tooling around such dialects. %As a result, there are now a large number of dialects structured in this manner. 

% These designs satisfyingly address the problem of \emph{backwards compat\-ibility}, but as new languages like these continue to proliferate , developers face the more daunting problem of \emph{lateral compatibility}, i.e. of interfacing with libraries that are written in sibling languages. 
%The problem is that as languages like these continue to proliferate (aided by increasingly convenient {language definition systems} \cite{erdweg2013state}), client programmers must contend not only with the problem of {backwards compatibility} but also with the more difficult problem of \emph{lateral compatibility}, i.e. of interfacing with libraries written in sibling languages. 
% Defining an FFI between every pair of sibling languages is not a scalable solution, but absent a direct FFI, clients can  interface only indirectly with these libraries through the code generated by a compiler  that targets the established languageThe \emph{language-oriented} approach \cite{journals/stp/Ward94} seems, then, to be difficult to reconcile with the best practices of ``programming in the large'' \cite{DeRemer76}. %As more languages emerge (e.g. with the aid of powerful language definition systems), these problems become more acute.\footnote{``Little languages'' \cite{DBLP:journals/cacm/Bentley86c} can, of course, be useful to clients even if they lack interoperability mechanisms, but to support programming in the large, interoperability is crucial.}  %and unsafe (the FFI cannot enforce any non-trivial implementation invariants maintained by the compiler.)  This safety threat can in some cases be mitigated by instrumenting the compiled code with dynamic  contracts \cite{typescript-contracts}, but this is non-trivial for higher-order constructs and decreases performance, even when not crossing language boundaries.% This likely explains much of the resistance that developers have toward ideas housed in ``toy dialects''. %As more languages are used together, more FFIs need to be constructed and traversed.% This can defeat the productivity gains afforded by these dialects.%To avoid this, it is necessary to combine the different languages and their associated tools, but this often requires significant effort and expertise (we will discuss this further in Sec. \ref{related-work}). 



% These problems are severe enough to render even dialects that conservatively introduce only a few new constructs impractical. For example, an empirical study comparing a Java dialect, Habanero-Java (HJ), with a library, \lip{java.util.concurrent}, found that the constructs in HJ were indeed easier to use and provided useful static guarantees \cite{cave2010comparing}. Nevertheless, it concluded that the library-based constructs remained more practical outside the classroom because using HJ, as a distinct dialect of Java with its own type system, would be problematic in settings where some developers had not adopted it. It would also be difficult to use it in combination with other constructs also defined in specialized dialects of Java. %Moreover, its tool support is more limited.

%These languages become, in essence, merely vehicles for communicating ideas, rather than useful tools.

 %This issue was perhaps most succinctly expressed by a participant in a recent study by Basili et al. \cite{basili2008understanding} who stated ``I hate MPI, I hate C++. [But] if I had to choose again, I would probably choose the same.''
%Unfortunately, researchers and domain experts (collectively, \emph{providers}) who design potentially useful new abstractions often find it difficult to implement them in terms of the general-purpose abstraction mechanisms available in existing languages. 
%Using a foreign construct can be  unnatural and, if the languages have  different type systems, can also cause safety problems. 

%Language extension mechanisms have long promised to reduce the need for distinct language dialects by giving users more granular control over the syntax and semantics of the language. 
%Libraries are far more practical. The reason for this is perhaps obvious: importing one library does not make it more difficult to use another library (barring easily avoidable naming conflicts.) 

%The problem of lateral compatibility is intractable because it has a \emph{many-to-many} character, unlike the problem of backwards compatibility, which has a \emph{many-to-one} character. 
% It is little wonder, then, that the communities around established languages have settled on a single backwards-compatible successor -- the Java community has settled on Scala, the JavaScript community on TypeScript and so on.

% Our work seeks to develop a \emph{fragment-oriented approach} to the problem of expressing new semantic structures. 
% in favor of the \emph{language-oriented approach} \cite{journals/stp/Ward94} is, in many cases,  premature. 
%In , we instead pursue a \emph{fragment-oriented approach} to the problem of expressing new semantic structures. 
% Rather than packaging, for example, functional sums and products or new object systems into monolithic language definitions, 

%In this paper, we propose a more compositional \emph{fragment-oriented} approach . 
In this paper, we propose a more compositional \emph{fragment-oriented} approach to the problem of expressing new semantic structures. In particular, we introduce a 
%more compositional \emph{fragment-oriented} approach to the problem of expressing new semantic structures. This approach calls for a 
single ``extensible'' statically typed language, $\typy$, that has no primitive types. Instead, $\typy$ allows library providers to define new \emph{semantic fragments} that library clients can import in any combination. %an ``extensible'' language that  
%two languages - the first, $\typy$, a full-scale language, and the second, $\lambda_\text{frag}$, a minimal calculus - that 
%allows library providers to express  semantic structures that are, or would need to be, built primitively into languages organized in a ``monolithic'' manner, e.g. variations on functional datatypes or object systems. 
For example, a library provider can define a record fragment that expresses the static and dynamic semantics of functional records. This fragment-oriented approach sidesteps the lateral compatibility problem in that clients of a library that requires record values at its public interface can simply import the record fragment themselves.\footnote{We assume throughout that simple naming conflicts are handled by some external coordination mechanism, e.g. a package repository.} % This approach sidesteps the lateral compatibility problem.
Moreover, when interacting with libraries in a foreign language \emph{is} necessary, a fragment system can allow library providers to implement a natural, type safe foreign interface as a library. 
Of course, designing an expressive {fragment system} equipped with useful composition principles presents several challenges.

% We pursue an alternative \emph{fragment-oriented approach}: programmers  express new semantic structures, e.g. functional sums and products or new object systems, as composable \emph{fragments}, which are distributed in libraries. 
%In   an internally extensible language, programmers  can \emph{granularly} import the primitive constructs best suited to each component of their application or library. Providers can thus more easily develop, deploy and evaluate new abstractions in the context of existing codebases, narrowing one of the gaps between research and practice \cite{basili2008understanding}. 
% %Indeed, abstractions could compete on their individual merit, rather than  by the serious adoption barriers mentioned above.

% The issue of the most fundamental importance is that the mechanism must not allow conflicts. Conflicts are a concern at every level. 

First, language designers typically define  concrete forms specific to the primitive structures that they introduce, but if we allow fragment providers to define new concrete forms in a decentralized manner (following, e.g., Sugar* \cite{erdweg2013framework}), then different fragments could define conflicting forms. For example, consider the following family of forms:
\vspace{-2px}\begin{lstlisting}[numbers=none]
{ label^$_1$^: expr^$_1$^, ..., label^$_n$^: expr^$_n$^ }
\end{lstlisting}
One fragment might take these as the introductory forms for functional records, while another fragment might take these as the introductory forms for TypeScript-style objects. These forms might also conflict with those for Python-style dictionaries. Such syntactic conflicts inhibit composition.

We also encounter the classic \emph{expression problem} \cite{wadler1998expression,Reynolds75}: if library providers are able to define new term constructors in a decentralized manner, then it is difficult to define functions that proceed by exhaustive case analysis, e.g. pretty-printers.

At the level of the semantics, we must maintain essential metatheoretic properties, like type safety. Moreover, clients should be able to  assume that importing a new fragment for use in one portion of a program will not change the meaning of other portions of the program, nor allow a program to take on ambiguous meaning. This implies that we cannot simply operationalize the semantics as a ``bag of rules'' that fragment providers freely extend.

The $\typy$ semantic fragment system addresses the problems of concrete and abstract syntax quite simply: fragment providers have no ability to extend $\typy$'s concrete or abstract syntax.  Instead, the system allows fragments to ``share'' syntactic forms by delegating control over each term to some contextually relevant fragment definition according to $\typy$'s \emph{delegation protocol}. For example, $\typy$ delegates control over terms of curly-brace delimited form (see above) to the fragment that defines the type that the term is being checked against. As such, these forms can serve as introductory forms for records, objects and dictionaries alike. The delegation protocol is deterministic, so ambiguities cannot arise. It is also stable under fragment composition, so defining a new fragment cannot change the meaning of a component. 

What does it mean for a fragment to be ``delegated control'' over a term? In our system, the delegated fragment is tasked first with programmatically typechecking the term, following a \emph{bidirectional} protocol, i.e. one that supports local type inference \cite{Pierce:2000:LTI:345099.345100}. (Scala is another notable language that has a bidirectional type system, albeit of different design \cite{OdeZenZen01}.) If the fragment decides that the term is ill-typed, it generates an error message. If typechecking succeeds, the fragment is tasked with assigning dynamic meaning to the term by computing a translation to a fixed \emph{internal language} (IL). The IL is analagous to a first-stage compiler intermediate language (here shifted into the semantics of the language.) Defining the dynamics by translation (following the approach taken in the Harper-Stone definition of Standard ML \cite{Harper00atype-theoretic}) means that type safety of the extensible external language reduces to type safety of the fixed IL.

The general fragment delegation protocol just outlined is the primary conceptual contribution of this paper. To make matters concrete, we must choose a particular syntax and IL for $\typy$. We choose to repurpose Python's syntax without modification and use Python itself as the IL (which explains why we named our language $\typy$.)  
%In Sec. \ref{sec:typy-by-example}, we describe  \texttt{typy}, which repurposes  Python's syntax and uses Python itself as the IL \cite{sanner1999python,python}. 
There are two main reasons for this choice: 1) Python supports both  dynamic reflection and code generation, so we can embed our system itself into Python as a library (no standalone compiler is needed); and 2) Python's syntax, cleverly repurposed, is rich enough to express a wide variety of interesting constructs cleanly (so we inherit various tools ``for free''.) 

%The remainder of the paper is organized as follows: 
Sec. \ref{sec:typy-by-example} introduces $\typy$'s fragment system with simple examples. Sec. \ref{sec:more-examples} then describes more sophisticated case studies. %Programmers do not need to adopt new tools to use \texttt{typy}. 
%In Sec. \ref{sec:formal-semantics}, we formalize this protocol as a  bidirectionally typed translation semantics\todo{name}. This gives us a minimal system where we can rigorously establish the essential metatheory. Theoretically minded readers may wish to read this section before reading Sec. \ref{sec:typy-by-example} and Sec. \ref{sec:more-examples}.
Sec. \ref{sec:related-work} summarizes related work.  Sec. \ref{sec:discussion} concludes with a discussion of present limitations and future work.
% Because the dynamics are given by translation, establishing type safety is straightforward: we simply check that the translation is well-typed. Moreover, this protocol is deterministic and stable under fragment composition, so importing a new fragment will not change the meaning of a program, nor allow a program to take on more than one meaning.


% The remainder of the paper is organized as follows:
% \begin{itemize}
% \item In Sec. \ref{sec:typy-by-example} we introduce $\typy$ by example.
% \item In Sec. \ref{sec:more-examples} we establish the expressiveness of our approach by describing several other examples: ...\todo{list examples}
% \item In Sec. \ref{sec:formal-semantics} we formally define the semantics of fragments with a simplified lambda calculus, ...\todo{name of calculus}
% \item 
% \item 
% \end{itemize}

% \vspace{-6px}
\section{Semantic Fragments in \texttt{typy}}\label{sec:typy-by-example}
\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}
from typy import component
from typy.std import record, string_in, py ^\label{fig:record-example/line:import-start}^ ^\label{fig:record-example/line:import-end}^

@component ^\label{fig:record-example/line:component-start}^
def Listing1():
  Account [: type] = record[ ^\label{line:Account-start}^
    name        : string_in[r'.+'],
    account_num : string_in[r'\d{2}-\d{8}'],
    memo        : py
  ] ^\label{line:Account-end}^

  test_acct [: Account] = {  ^\label{line:test_acct-start}^
    name:        "Harry Q. Bovik",
    account_num: "00-12345678",
    memo:        { }
  }^\label{fig:record-example/line:component-end}\label{line:test_acct-end}^
\end{lstlisting}
\caption{Types and values in $\typy $.}
\label{fig:record-example}
\end{codelisting}

Listing \ref{fig:record-example} gives an example of a well-typed \texttt{typy} program that first imports several fragments, then defines a top-level {component}, \lip{Listing1}, that exports a record type, \lip{Account}, and a value of that type, \lip{test_acct}. 
%Let us examine this program in more detail starting at the top level.

\subsection{Dynamic Embedding} 
\texttt{typy} is dynamically embedded into Python, meaning that Listing \ref{fig:record-example} is simply a standard Python script at the top level. $\typy$ supports Python 2.6+, though in later examples, we use syntactic conveniences not introduced until Python 3.0. Appendix \ref{sec:python2-support} discusses the minor changes necessary to port these examples to Python 2.6+.% The execution of this script begins the \emph{static phase} of compilation, as suggested by line \ref{hello-ct}. The main purpose of this phase is to define the tycons and types that will be used by the \emph{typed functions} in the program, marked in this example by the decorator \lip{@fn}. The static phase ends once  these typed functions have gone through the process of active typechecking and translation. The generated translation is what ultimately determines the run-time behavior of the program. % The evaluation of the translation of a typed function represents the \emph{dynamic phase}. %For example, the string on line \ref{hello-ct} is a statically valued string. Types are also statically valued, as we will discuss shortly. 

%Code inside , is evaluated during the dynamic phase, i.e. these functions can only be called after the active typechecking and translation process we will describe below is complete. %There are two ways to transition from the static phase to the dynamic phase, \emph{standalone compilation}, which we will discuss in Sec. \ref{external-compilation} and Sec. \
% For example, the string on line \ref{account-start}, discussed further below, is \emph{dynamically valued}. %Of particular note, types are statically valued.%No features specific to its primary implementation, CPython, are needed (so \texttt{typy} should support alternative implementations like Jython and PyPy, though as of this writing this claim has not been tested). 

\paragraph{Package Management} On Line \ref{fig:record-example/line:import-start}, we use Python's \lip{import} mechanism to import three fragments from \li{typy.std}, the \verb|typy| standard library. This library receives no special treatment from $\typy$'s semantics -- it comes bundled with $\typy$ merely for convenience (see Sec. \ref{sec:discussion} for a discussion.)

\paragraph{Fragments}  $\typy$ fragments are Python classes that extend \lip{typy.Fragment}. These classes are never instantiated -- instead, $\typy$ interacts with them exclusively through class methods (i.e. methods on the class object.)  Listing \ref{fig:type-validation} shows a portion of the \lip{record} fragment that we will detail in Sec. \ref{sec:types}. 

% Embedding $\typy$ into Python pays off immediately on the first two lines of Listing \ref{fig:record-example}: we use Python's standard import system to distribute $\typy$ itself, and users of $\typy$ can also use these systems to distribute libraries that define constructs governed by $\typy$, including semantic fragments. % fragments are simply defined in the same library and distributed with the language for convenience.

% All existing Python libraries are also available from w, as we will discuss later.% Here, \lip{typy.std} is the ``standard library'' but does not benefit from special support in \lip{typy} itself.% Nothing in \lip{typy.std} benefits from special support from \lip{typy}, the core language package. We will see \lip{typy} being used directly below. %\texttt{typy} can be installed with a single command: \emph{omitted for review}. %\texttt{typy} is agnostic about the particular design choices that we made in our examples.

%The top-level statements in an \texttt{typy} file, like the \lip{print} statement on line 10, are executed to control the compile-time behavior, rather than the run-time behavior, of the program. %That is, Python serves as the \emph{compile-time metalanguage} (and, as we will see shortly, the \emph{type-level language}) of \texttt{typy}. %For readers familiar with C/C++, Python can be thought of as serving a role similar to (but more general than) its preprocessor and template system (as we will see).
%\subsection{Active Typechecking} 

% Python is dynamically typed, but functions governed by \verb|typy| (those decorated with \lip{@fn} in this example) will be statically typed (Section \ref{s:atc}) and then translated to dynamically typed Python code (Section \ref{s:atr}), both under the control of the type constructors of the types used within them. These steps can occur either during a standalone compilation phase (Section \ref{compilation}) or just-in-time upon their first invocation (Section \ref{interactive}). 


\paragraph{Top-Level Components}
On Lines \ref{fig:record-example/line:component-start}-\ref{fig:record-example/line:component-end} of Listing \ref{fig:record-example}, we define a top-level $\typy$ component by decorating a Python function value with \lip{component}, a decorator defined by $\typy$. 
%Python assigns the value produced by calling the decorator with the decorated function value as an argument to the name specified by the function definition (here, \lip{Listing1}.)
This decorator discards the decorated function value  after extracting its abstract syntax tree (AST) and \emph{static environment}, i.e. its closure and globals dictionary, using the reflection mechanisms exposed by the \lip{ast} and \lip{inspect}  packages in Python's standard library.\footnote{The reader may need to refer to  documentation for the \lip{ast} package, available at \url{https://docs.python.org/3.6/library/ast.html}, to fully understand some examples in the remainder of this paper.} It then processes the syntactic forms in the body of the function definition according to an alternative semantics. In particular, \lip{component} repurposes Python's assignment and array slicing forms to allow for type member definitions, described in Sec. \ref{sec:types}, and value member definitions, described in Sec. \ref{sec:typechecking-and-translation}. The return value of the decorator is an instance of \lip{typy.Component} that tracks 1) the identities of type members; and 2) the types and evaluated translations of value members. (Luckily, Python chose the ``generic'' \li{def} keyword -- had it chosen, e.g. \li{fun}, this might be less clean, because components are not functions.) % The translations of top-level value members are evaluated immediately. %(the details are straightforward and not pedagogically useful at the moment, so we elide them.)

%

% \paragraph{Statements}
% $\typy$ then delegates control to the decorating fragment. In particular, this fragment is responsible for computing a $\typy$ type and a translation on the basis of the extracted AST and the static environment. The return value of the decorator (i.e. the top-level value of \lip{Listing1}) is the result of evaluating the generated translation under the static environment.

% Here, \lip{component} defines the semantics of labeled collections of types and values. The details of this fragment are somewhat complex, so we will only outline its semantics as we proceed to consider simpler examples below. (Briefly: $\typy$ uses \emph{singleton kinds} to encode type synonyms \cite{StoneHarper00}. The \lip{component} fragment leverages the same machinery to support type members.)

\subsection{Fragmentary Type Validation}\label{sec:types}
\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}
import ast, typy
class record(typy.Fragment):
  @classmethod
  def init_idx(cls, ctx, idx_ast):
    if isinstance(idx_ast, ast.Slice):
      # Python special cases single slices
      # we don't want that
      idx_ast = ast.ExtSlice(dims=[idx_ast])
    if isinstance(idx_ast, ast.ExtSlice):
      idx_value = dict() # returned below
      for dim in idx_ast.dims:
        if (isinstance(dim, ast.Slice) and 
            dim.step is None and 
            dim.upper is not None and 
            isinstance(dim.lower, ast.Name)):
          lbl = dim.lower.id
          if lbl in idx_value:
            raise typy.TypeFormationError(
              "Duplicate label.", dim)
          ty = ctx.as_type(dim.upper) ^\label{line:as_type}^
          idx_value[lbl] = ty
        else: raise typy.TypeFormationError(
          "Invalid field spec.", dim)
      return idx_value
    else: raise typy.TypeFormationError(
      "Invalid record spec.", idx_ast)
\end{lstlisting}
\caption{Record type validation.}
\label{fig:type-validation}
\end{codelisting}

The type member definition on Lines \ref{line:Account-start}-\ref{line:Account-end} of Listing \ref{fig:record-example} is of the following general form:
\begin{lstlisting}[numbers=none]
name [: kind] = ty_expr
\end{lstlisting}
where \lip{name} is a Python name, \lip{kind} is a $\typy$ \emph{kind} and \lip{ty_expr} is a $\typy$ \emph{type expression}. Kinds classify type expressions, so when $\typy$ encounters a definition like this, it checks that \lip{ty_expr} is of kind \lip{kind}. 

$\typy$ adopts the system of \emph{dependent singleton kinds} first developed for the ML module system \cite{DBLP:conf/lfmtp/Crary09,pfpl}, which elegantly handles the details of type synonynms, type members and parameterized types (we will define a parameterized type in Listing \ref{fig:patterns}.) The only major deviation from this established account of type expressions, which we will not repeat here, is that types in canonical form are  expressed as follows:
\begin{lstlisting}[numbers=none]
fragment[idx]
\end{lstlisting}
where \lip{fragment} is a fragment in the static environment and \lip{idx} is some Python slice form. In other words, every $\typy$ type in canonical form is associated with a fragment -- there are no ``primitive'' types defined by $\typy$ itself. For convenience, programmers can write \lip{fragment} by itself when the index is trivial, i.e. when it is of the form \lip{()}. 

For example, the type expression on Lines \ref{line:Account-start}-\ref{line:Account-end} of Listing \ref{fig:record-example} is a \lip{record} type in canonical form. The index, which is in Python's \emph{extended slice form}, specifies fields named \lip{name}, \lip{account_num} and \lip{memo} and  corresponding field types as shown. We discuss the field types in Sec. \ref{sec:typechecking-and-translation} -- for now, it suffices to see that these are also in canonical form.

To establish that a type in canonical form is valid, i.e. of kind \lip{type}, $\typy$ calls the fragment class method \lip{init_idx}. This method receives  the \emph{context} and the AST of the index and must return a Python value called the type's \emph{index value} if the type is valid, or raise \lip{typy.TypeValidationError} with an error message and a reference to the location of the error within the index otherwise. %The {context} 1) tracks contextual information, e.g. the kinds of type variables; and 2) provides methods that fragments use to interact with the kind system. In particular, the context provides a method \lip{as_type} that 

For example, the \lip{record.init_idx} class method shown in Listing \ref{fig:type-validation} validates record types by checking that 1) the index consists of a  sequence of field specifications of the form \lip{name : ty_expr}, where \lip{name} is a Python name; 2) no names are duplicated; and 3) \lip{ty_expr} is a valid type expression, as determined by calling a method of the context object, \lip{ctx.as_type} (Line \ref{line:as_type}.) This method turns the given Python AST into a type expression, i.e. an instance of (a class that inherits from) \lip{typy.TyExpr}, and checks that it is of kind \lip{type}. The returned index value is a Python dictionary mapping the field names to the corresponding instances of \lip{typy.TyExpr}.
%This method raises a \lip{TypeFormationError} if any of the components are not of the form \lip{label : ty}, or if there are duplicate labels, or if any of the field types are themselves invalid in the context where they appear, as determined by calling the method \lip{as_type} of the provided $\typy$ \emph{context}, \lip{ctx}.  % The \emph{context}, \lip{ctx}, tracks  provided by $\typy$.

% \vspace{-4px}
\subsection{Fragmentary Bidirectional Typing and Translation}\label{sec:typechecking-and-translation}
% \vspace{-3px}
The value member definition on Lines \ref{line:test_acct-start}-\ref{line:test_acct-end} of Listing \ref{fig:record-example} is of the following general form:
\begin{lstlisting}[numbers=none]
name [: ty_expr] = expr
\end{lstlisting}
where \lip{name} is a Python name, \lip{ty_expr} is a type expression and \lip{expr} is an expression. When $\typy$ encounters a definition like this, it 1) checks that \lip{ty_expr} is of kind \li{type}, as described in Sec. \ref{sec:types}; 2) \emph{analyzes} \lip{expr} against \lip{ty_expr}; and 3) generates a \emph{translation} for \lip{expr}, which is another Python AST.%If this succeeds, $\typy$ generates a \emph{translation} for \lip{expr}. The dynamic behavior of \lip{expr} is determined by its translation.

A type annotation is not always necessary:% type annotation is not necessary:
\begin{lstlisting}[numbers=none]
name = expr
\end{lstlisting}
In this case, $\typy$ attempts to \emph{synthesize} a type from \lip{expr} before generating a translation, rather than analyzing it against a known type. 

Type systems that distinguish type analysis (where the type is known) from type synthesis (also known as \emph{local type inference}, where the type must be determined from the expression) are called \emph{bidirectional type systems} \cite{Pierce:2000:LTI:345099.345100,bidi-tutorial}. Our system is based on the system developed by Dunfield and Krishnaswami \cite{conf/icfp/DunfieldK13}. Again, we will not repeat standard details here -- our interest in the remainder of this section will be only in how $\typy$ delegates control  to some contextually relevant fragment according to the $\typy$ \emph{delegation protocol}. %This section describes how the delegation protocol works for different expression forms.
 % -- there is not enough space to repeat the details of standard constructs.

% Taken together, we say that $\typy$ has a \emph{bidirectionally typed translation semantics} where Python serves as the \emph{internal language}.

% \vspace{-3px}
\subsubsection{Literal Forms}\label{sec:literals}
% \vspace{-3px}
\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}
# class record(typy.Fragment): 
  # ... continued from Listing 2 ...
  ^\label{ana_Dict}^@classmethod
  def ana_Dict(cls, ctx, idx, e):
    for lbl, value in zip(e.keys, e.values):
      if isinstance(lbl, ast.Name):
        if lbl.id in idx: 
          ctx.ana(value, idx[lbl.id])
        else: 
          raise typy.TyError("<bad label>", f)
      else: 
        raise typy.TyError("<not a label>", f)
    if len(idx) != len(e.keys): 
      raise typy.TyError("<label missing>", e)

  @classmethod
  def trans_Dict(self, ctx, idx, e): 
    ast_dict = dict((k.id, v) 
      for k, v in zip(e.keys, e.values))
    return ast.Tuple(
      (lbl, ctx.trans(ast_dict[lbl]))
      for lbl in sorted(idx.keys()))
\end{lstlisting}
\caption{Typing and translation of literal forms.}
\label{fig:record-intro}
\end{codelisting}
$\typy$ delegates control over the typechecking and translation of terms of literal form to the fragment defining the type that the expression is being analyzed against. 

For example, the expression on Lines \ref{line:test_acct-start}-\ref{line:test_acct-end} of Listing \ref{fig:record-example} is of dictionary literal form. The type that this expression is being analyzed against is \li{Account}, which is synonymous with a \li{record} type, so $\typy$ first calls the \li{record.ana_Dict} class method, shown in Listing \ref{fig:record-intro}. This method receives the context, the index value computed by \li{record.init_idx} and the AST of the literal term. It must return (trivially) if type analysis is successful or raise \li{typy.TyError} with an error message and a reference to the subterm where the error occurred otherwise. In this case, \li{record.ana_Dict} checks that each key expression is a Python name that appears in the index value, and then asks $\typy$ to analyze the value against the corresponding type by calling \li{ctx.ana}. Finally, it makes sure that all of the components specified in the index value appear in the literal.

The three subexpressions in Listing \ref{fig:record-example} that \li{record.ana_Dict} asks $\typy$ to analyze are also of literal form -- the values of \li{name} and \li{account_num} are string literals and the value of \li{memo} is another dictionary literal. As such, when \li{ctx.ana} is called, $\typy$ follows the same protocol just described, delegating to \li{string_in.ana_Str} to analyze the string literals and to \li{py.ana_Dict} to analyze the dictionary literal. The \li{string_in} fragment implements a regex-based constrained string system, which we described, along with its implementation in $\typy$, in a workshop paper \cite{sanitation-psp14}.\footnote{Certain details of $\typy$ have changed since that paper was published, but the essential idea remains the same.} The \li{py} fragment allows dynamic Python values to appear inside $\typy$ programs, so \li{\{ \}} is analyzed as an empty Python dictionary. Supplemental details about Python interoperability are given in Appendix \ref{appendix:py}.

If typechecking is successful, $\typy$ delegates to the same fragment to generate a translation, i.e. another Python AST. For example, $\typy$ calls the \li{record.trans_Dict} method shown in Listing \ref{fig:record-intro}, which translates records to Python tuples (the field names are needed only statically.) The field ordering is alphabetical. This method asks $\typy$ to determine translations for the field values by calling \li{ctx.trans}, which again follows the delegation protocol ($\typy$ stores the types determined during typechecking as attributes of the AST nodes.)

\vspace{-3px}
\subsubsection{Definition Forms}\label{sec:fns}
\vspace{-3px}
\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}
from typy import component
from typy.std import fn
from listing1 import Listing1

@component
def Listing4():
  @fn ^\label{line:hello-start}^
  def hello(account : Listing1.Account):
    """Computes a string greeting."""
    name = account.name ^\label{line:assign}^
    "Hello, " + name ^\label{line:hello-end}^

  print(hello(Listing1.test_acct)) ^\label{line:print-hello}^
\end{lstlisting}
\caption{Functions, targeted forms and binary forms.}
\label{fig:hello}
\end{codelisting}


Listing \ref{fig:hello} shows an example of another component, \li{Listing4}, that defines a function, \li{hello}, on Lines \ref{line:hello-start}-\ref{line:hello-end} and then applies it to print a greeting on Line \ref{line:print-hello}. This listing imports the component \li{Listing1} defined in Listing \ref{fig:record-example}. %(we assume that Listing \ref{fig:record-example} is in a file called \li{listing1.py}.)

$\typy$ delegates control over the typechecking and translation of \li{def}inition forms that appear inside components, or in other synthetic positions, to the fragment that appears on the form as the first decorator.

Here, the \li{fn} fragment is the first (and only) decorator, so $\typy$ begins by calling the \li{fn.syn_FunctionDef} class method, outlined in Listing \ref{fig:fn}. This method is passed the context and the AST of the function and must initialize the context as desired and return the type that is to be synthesized for the function, or raise \li{typy.TyError} if this is not possible. 

We omit some of the details of this method for concision, but observe on Lines \ref{line:check-1}-\ref{line:check-2} of Listing \ref{fig:fn} that \li{fn} calls \li{ctx.check} on each statement in the function body (other than the docstring, following Python's conventions.) This prompts $\typy$ to follow its delegation protocol for each statement, described in Sec. \ref{sec:statements} below. 

We chose to take the value of the final expression in the function body as its return value, following the usual convention in functional languages (an alternative function fragment could instead use Python-style \li{return} statements.) The synthesized function type is constructed programmatically on Lines \ref{line:rty-first}-\ref{rty-second}. The index value is a pair of the argument types (extracted from the type annotations, not shown) with the synthesized return type.

If typechecking is successful, $\typy$ calls the class method \li{fn.trans_FunctionDef} to generate the translation of the function definition. This method, elided due to its simplicity, recursively asks $\typy$ to generate the translations of the statements in the body of the function definition by calling \li{ctx.trans} and inserts the necessary \li{return} keyword on the final statement.

For definition forms decorated by a type expression rather than a fragment, or those in other analytic positions, $\typy$ treats the function definition as a literal form. We will see examples of these in Sec. \ref{sec:more-examples}. 

\subsubsection{Statement Forms}\label{sec:statements}
\begin{codelisting}
\vspace{-3px}
\begin{lstlisting}
class fn(typy.Fragment):
  ^\label{syn_FunctionDef}^@classmethod
  def syn_FunctionDef(cls, ctx, tree):
    # (elided) process args + their types
    # (elided) process docstring
    ctx.push_bindings({}) # new bindings layer
    # check each statement in remaining body
    for stmt in tree.proper_body: ^\label{line:check-1}^
      ctx.check(stmt) ^\label{line:check-2}^
    # synthesize return type from last stmt
    last_stmt = tree.proper_body[-1]
    if isinstance(last_stmt, ast.Expr): 
      rty = ctx.syn(last_stmt.value)
    else: rty = unit
    ctx.pop_bindings() # pop local bindings
    # generate fn type
    return typy.CanonicalType( ^\label{line:rty-first}^
      fn, (arg_types, rty)) ^\label{rty-second}^

  @classmethod
  def check_Assign(cls, ctx, stmt):
    # (details of _process_assn elided)
    pat, ann, e = _process_assn(stmt)
    if ann is None: 
      ty = ctx.syn(e) ^\label{line:Assign-syn}^
    else:
      ty = ctx.as_type(ann) ^\label{line:check_Assign_1}^
      ctx.ana(e, ty) ^\label{line:check_Assign_2}^
    bindings = ctx.ana_pat(pat, ty) ^\label{line:ana_pat}^
    ctx.add_bindings(bindings) ^\label{line:add_bindings}^

  ^\label{check_Expr}^@classmethod
  def check_Expr(cls, ctx, stmt):
    ctx.syn(stmt.value)
    
  # trans_FunctionDef, trans_Assign & trans_Expr
  # are elided 
\end{lstlisting}
\caption{A portion of the \li{fn} fragment.}
\label{fig:fn}
% @classmethod
% def syn_Call(cls, ctx, idx, e):
%   # ... implements the semantics in 
%   #     Dunfield and Krishnaswami, 2013 ...
\end{codelisting}
Statement forms, unlike expression forms, are not classified by types. Rather, $\typy$ simply checks them for validity when the governing fragment calls \li{ctx.check}. 

For most statement forms, $\typy$ simply delegates control over validation and translation back to the fragment that was delegated control over the definition that the statement appears within. For example, when \li{fn.syn_FunctionDef} calls \li{ctx.check} on the assignment statement on Line \ref{line:assign} of Listing \ref{fig:hello}, $\typy$ delegates control back to the \li{fn} fragment by calling \li{fn.check_Assign}. Similarly, \li{fn.check_Expr} handles expression statements, like the one on Line \ref{line:hello-end} of Listing \ref{fig:hello}. Let us consider these in turn.

\vspace{-3px}\paragraph{Assignment}
The definition of \li{fn.check_Assign} given in Listing \ref{fig:fn} begins by extracting a \emph{pattern} and an optional \emph{type annotation} from the left-hand side of the assignment, and an expression from the right-hand side of the assignment. 

%For the assignment statement in Listing \ref{fig:hello}, the pattern is simply a variable, \li{name}, and there is no type annotation. The expression on the right-hand side is \li{account.name}, which is of \emph{attribute access} form.

No type annotation appears on the assignment in Listing \ref{fig:hello}, so \li{fn.check_Assign} asks $\typy$ to synthesize a type from the expression by calling \li{ctx.syn} (Line \ref{line:Assign-syn} of Listing \ref{fig:fn}.) We will describe how $\typy$ synthesizes a type for the expression \li{account.name} in Sec. \ref{sec:targeted-forms} below.

In cases where an annotation is provided, \li{fn.check_Assign} instead  asks $\typy$ to kind check the ascription to produce a type, then it asks $\typy$ to analyze the expression against that type by calling \li{ctx.ana} (Lines \ref{line:check_Assign_1}-\ref{line:check_Assign_2} of Listing \ref{fig:fn}.)

Finally, \li{fn.check_Assign} checks that the pattern matches values of the type that was synthesized or provided as an annotation by calling \li{ctx.ana_pat}. Patterns of variable form, like \li{name} in Listing \ref{fig:hello}, match values of any type. We will see more sophisticated examples of pattern matching in Sec. \ref{sec:pattern-matching} below. The \li{ctx.add_bindings} method adds the bindings (here, a single binding) to the typing context.

During translation, $\typy$ delegates to \li{fn.trans_Assign}. This method is again omitted because it is straightforward. The only subtlety has to do with shadowing -- \li{fn} follows the functional  convention where different bindings of the same name are distinct, rather than treating them as imperative assignments to a common stack location. This requires generating a fresh name when a name is reused (\li{ctx.add_bindings} does this by default.) As with the semantics of return values, a different function fragment could make a different decision in this regard by managing the context manually.

\vspace{-3px}\paragraph{Expression Statements} The \li{fn.check_Expr} method, shown in Listing \ref{fig:fn}, handles expression statements, e.g. the statement on Line \ref{line:hello-end} of Listing \ref{fig:hello}, by simply asking $\typy$ to synthesize a type for the expression. In Listing \ref{fig:hello}, this expression is of binary operator form -- we will describe how $\typy$ synthesizes a type for expressions of this form in Sec. \ref{sec:binary-forms} below.

\vspace{-3px}\paragraph{Other Statement Forms}
$\typy$ does not delegate to the fragment governing the enclosing definition for statements of definition form that have their own fragment or type decorator. Instead, $\typy$ delegates to the decorating fragment, just as at the top-level of a component definition. The fragment governing the enclosing function determines only how the translation is integrated into its own translation (through a \li{integrate_trans_FunctionDef} method, omitted for concision.)

$\typy$ also does not delegate to the decorating fragment for statements that 1) assign to an attribute, e.g. \li{e1.x = e2} or \li{e1.x += e2};  2) assign to a subscript, e.g. \li{e1[e2] = e3}; or 3) statements with guards, e.g. \li{if}, \li{for} and \li{while}.  These operate as \emph{targeted forms}, described next.

\subsubsection{Targeted Forms}\label{sec:targeted-forms}

\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}[deletendkeywords={slice}]
# class record(typy.Fragment): 
  # ... continued from Listing 3 ...
  @classmethod
  def syn_Attribute(cls, ctx, idx, e):
    if e.attr in idx:
      return idx[e.attr]
    else:
      raise typy.TypeError("<bad label>", e)

  @classmethod
  def trans_Attribute(self, ctx, idx, e): 
    pos = _pos_of(e.attr, sorted(idx.keys()))
    return ast.Subscript(
      value=ctx.trans(e.value),
      slice=ast.Index(ast.Num(n=pos)))
\end{lstlisting}
\caption{Typing and translation of targeted forms.}
\label{fig:record-elim}
\end{codelisting}
\emph{Targeted forms} include 1) the statement forms just mentioned; 2) expression forms having exactly one subexpression, like \lip{-e1} or \lip{e1.attr}; and 3) expression forms where there may be multiple subexpressions but the left-most one is the only one that is syntactically required, like \lip{e1(args)} (there may be no arguments.) When $\typy$ encounters terms of targeted form, it first synthesizes a type for the target subexpression \li{e1}. It then delegates control over typechecking and translation to the fragment defining the type of \li{e1}.

For example, the expression on the right-hand side of the assignment statement on Line \ref{line:assign} of Listing \ref{fig:hello} is \li{account.name}, so $\typy$ first synthesizes a type for \li{account}. Following the standard rule for variables, which are tracked by the context, we have that \li{account} synthesizes type \li{Listing1.Account}. This type is synonymous with a \li{record} type, so $\typy$ first calls the \li{record.syn_Attribute} class method given in Listing \ref{fig:record-elim}. This method looks up the attribute, here \li{name}, in the type's index value and returns the corresponding field type, here \li{string_in[r".+"]}, or raises a type error if it is not found.



To generate the translation for \li{account.name}, $\typy$ calls \li{record.trans_Attribute}, shown in Listing \ref{fig:record-elim}. Because record values translate to tuples, this method translates record field projection to tuple projection, using the position of the attribute within the record type's index value to determine the appropriate slice index.

\subsubsection{Binary Forms}\label{sec:binary-forms}

Python's grammar also defines a number of binary operator forms, e.g. \li{e1 + e2}. One approach for handling these forms would be to privilege the leftmost argument, \li{e1}, and treat these forms as targeted forms. This approach is unsatisfying because binary operators are often commutative. Instead, $\typy$ defines a symmetric protocol to determine which fragment is delegated control over binary forms. First, $\typy$ tries to synthesize a type for both arguments.
If neither argument synthesizes a type, a type error is raised. 

If only one of the two arguments synthesizes a type, then the fragment defining that type is delegated control. For example, the binary operator on Line \ref{line:hello-end} of Listing \ref{fig:hello} consists of a string literal on the left (which does not synthesize a type, per Sec. \ref{sec:literals}) and a variable, \li{name}, of type \li{string_in[r".+"]} on the right, so \li{string_in} is delegated control over this form.

If both arguments synthesize a type and both types are defined by the same fragment, then that fragment is delegated control. If each type is defined by a different fragment, then $\typy$ refers to the \emph{precedence sets} of each fragment to determine which fragment is delegated control. The precedence sets are Python sets listed in the \li{precedence} attribute of the fragment that contain other fragments that the defining fragment claims precedence over (if omitted, the precedence set is assumed empty.) $\typy$ checks that if one fragment claims precedence over another, then the reverse is not the case (i.e. precedence is anti-symmetric, to maintain determinism.) Precedence is not transitive. If a precedent fragment is found, it is delegated control. Otherwise, a type error is raised.% (i.e. that the precedence graph is a tree.)

For example, if we would like to be able to add \li{int}s and \li{float}s and these are defined by separate fragments, then we can put the necessary logic in either fragment and then place the other fragment in its precendence set.

% \subsubsection{Name Expression Forms}\label{sec:name-forms}
% Finally, let us consider how expressions of name form work in $\typy$. For example, we use the names \li{account} and \li{name} in the definition of \li{hello} in Listing \ref{fig:hello}. Like most statement forms, $\typy$ delegates to the fragment decorating the function in the enclosing 

\subsection{Fragmentary Pattern Matching}\label{sec:pattern-matching}
As we saw on Line \ref{line:ana_pat} of Listing \ref{fig:fn}, fragments can request that $\typy$ check that a given \emph{pattern} matches values of a given type by calling \li{ctx.ana_pat}. In the example in Listing \ref{fig:hello}, the pattern was simply a name -- name patterns match values of any type. In this section, we will consider other patterns. For example, the statement below uses a tuple pattern:
\vspace{-2px}\begin{lstlisting}[numbers=none]
(x, y, z) = e
\end{lstlisting}\vspace{-1px}

$\typy$ also supports a more general \li{match} construct, shown on Lines \ref{line:match-start}-\ref{line:match-end} of Listing \ref{fig:patterns}. This construct, which spans several syntactic statements, is treated as a single expression statement by $\typy$. The \emph{scrutinee} is \li{t} and each \emph{clause}  is of the form \li{with pat: stmts} where \li{pat} is a pattern and \li{stmts} is the corresponding \emph{branch}. $\typy$ also supports an analagous expression-level match construct, shown in Appendix \ref{appendix:match-expressions}.

\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}
from typy import component
from typy.std import finsum, tpl, fn
@component
def Listing7():
    # polymorphic recursive finite sum type
    tree(+a) [: type] = finsum[ ^\label{line:tree-start}^
      Empty, 
      Node(tree(+a), tree(+a)),
      Leaf(+a)
    ] ^\label{line:tree-end}^
    # polymorphic recursive function over trees
    @fn
    def map(f : fn[+a, +b], 
            t : tree(+a)) -> tree(+b):
      [t].match ^\label{line:match-start}^
      with Empty: Empty ^\label{line:Empty}^
      with Node(left, right): ^\label{line:Node}^
        Node(map(f, left), map(f, right))
      with Leaf(x):
        Leaf(f(x)) ^\label{line:match-end}^
\end{lstlisting}
\caption{Polymorphism, recursion and pattern matching in $\typy$. An analagous OCaml file is shown in Appendix \ref{sec:ocaml-examples}.}
\label{fig:patterns}
\end{codelisting}

To typecheck a match expression, $\typy$ first synthesizes a type for the scrutinee. Here, the scrutinee, \li{t}, is a variable of type \li{tree(+a)}. This type is an instance of the parameterized recursive type \li{tree} defined on Lines \ref{line:tree-start}-\ref{line:tree-end} (the mechanisms involved in defining recursive and parameterized types are built into $\typy$ in the usual manner.) Type variables prefixed by \li{+}, like \li{+a} and \li{+b}, implicitly quantify over types at the function definition site (like \lstinline[language=ML]{'a} in OCaml \cite{ocaml-manual}.) 

More specifically, \li{tree(+a)} is a \emph{recursive finite sum type} defined by the \li{finsum} fragment imported from \li{typy.std} \cite{pfpl}. This fragment is defined such that values of finite sum type translate to Python tuples, where the first element is a string \emph{tag} giving one of the names in the type index and the remaining elements are the corresponding values. For example, a value \li{Node(e1, e2)} translates to \li{("Node", tr1, tr2)} where \li{tr1} and \li{tr2} are the translations of \li{e1} and \li{e2}. Names and call expressions beginning with a capitalized letter are initially treated as literal forms in $\typy$ (following Haskell \cite{jones2003haskell}.) If the delegated fragment does not define their semantics, they are then treated as targeted forms.

\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}[deletendkeywords={slice}]
import ast, typy
class finsum(typy.Fragment):
  # ... 
  @classmethod
  def ana_pat_Call(cls, ctx, idx, pat):
    if (isinstance(pat.func, ast.Name) and 
        pat.func.id in idx and 
        len(pat.args) == len(idx[pat.func.id])):
      bindings, lbl = {}, pat.func.id
      for p, ty in zip(pat.args, idx[lbl]):
        _combine(bindings, ctx.ana_pat(p, ty))
      return bindings
    else:
      raise typy.TyError("<bad pattern>", pat)

  @classmethod
  def trans_pat_Call(cls, ctx, idx, pat, 
                     scrutinee_tr):
    conditions = [
      ast.Compare(left=_prj(scrutinee_tr, 0), 
        ops=[ast.Eq()],
        comparators=[ast.Str(s=pat.func.id)])
    ]
    binding_translations = {}
    for n, p in enumerate(pat.args):
      arg_scrutinee = _prj(scrutinee_tr, n+1)
      c, b = ctx.trans_pat(p, arg_scrutinee)
      conditions.append(c)
      binding_translations[pat.func.id] = b
    condition = ast.BoolOp(op=ast.And(), 
      values=conditions)
    return (condition, binding_translations)

def _prj(e, n): # helper function
  return ast.Subscript(value=e, 
    slice=ast.Index(ast.Num(n=n)))
\end{lstlisting}
\caption{Typing and translation of patterns.}
\label{fig:finsum}
\end{codelisting}

$\typy$ delegates control over patterns to the fragment that defines the scrutinee type. For example, to check the pattern \li{Node(left, right)} on Line \ref{line:Empty}, $\typy$ calls \li{finsum.ana_pat_Call}, shown in Listing \ref{fig:finsum}. This method must either return a dictionary of \emph{bindings}, i.e. a mapping from variables to types, which $\typy$ adds to the typing context when typechecking the corresponding branch expression, or raise a type error if the pattern does not match values of the scrutinee type. In this case, \li{finsum.ana_pat_Call} first checks to make sure that 1) the name that appears in the pattern appears in the type index (for \li{finsum} types, this is a mapping from names to sequences of types); and 2) that the correct number of pattern arguments have been provided. If so, it asks $\typy$ to check each subpattern against the corresponding type. Here, \li{left} and \li{right} are both checked against \li{tree(+a)}. These happen to be variable patterns, but $\typy$ supports arbitrarily nested patterns. The returned dictionary of bindings is constructed by combining the two dictionaries returned by these calls to \li{ctx.ana_pat}. The \li{_combine} function, not shown, also checks to make sure that the bound variables are distinct.



Match expression statements translate to Python \li{if...elif} statements. For each clause, $\typy$ needs a boolean \emph{condition expression}, which determines whether that branch is taken, and for each binding introduced by that clause, $\typy$ needs a translation. To determine the condition and the binding translations, $\typy$ again delegates to the fragment defining the scrutinee type, here by calling \li{finsum.trans_pat_Call}, given in Listing \ref{fig:finsum}. This class method is passed the context, the type index, the pattern AST and an AST representing the scrutinee (bound to a variable, to avoid duplicating effects.) The generated condition expression first checks the tag. Then, for each, subpattern, it recursively generates its conditions and binding translations by calling \li{ctx.trans_pat(p, arg_scrutinee)}, where \li{arg_scrutinee} makes the new ``local scrutinee'' for the subpattern be the corresponding projection out of the original scrutinee. The returned condition expression is the conjunction of the tag check and the subpattern conditions. For example, for \li{Node(left, right)}, the condition expression is \li{scrutinee[0] == 'Node' and True and True}. The two \li{True}s are because variables always match (these are optimized away.)

The delegated fragment also has responsibility for checking exhaustiveness, via the class method \li{is_exhaustive} (not shown.) It can call \li{ctx.is_exhaustive(pats, ty)} to check the exhaustiveness of another sequence of patterns.

\subsection{Determinism and Stability}
We argue that the $\typy$ delegation protocol is compositionally well-behaved, i.e. it exhibits \emph{determinism} and \emph{stability under fragment composition}. By {determinism}, we mean that under a given context, there is always a single fragment that can be delegated control over any type expression, statement, expression or pattern form, i.e. there can be no ambiguity. By \emph{stability}, we mean that the delegation protocol will not make a different choice simply because a new fragment has been added to the \emph{fragment context} (the set of fragments in the static environment.) A virtue of the design we have presented is that these properties follow essentially immediately. We contrast this with related work in Sec. \ref{sec:related-work}.%In contrast, when the semantics is a ``bag of inference rules'', proofs of these properties often require sophisticated reasoning (if they hold at all.) %Like many other ``pragmatic'' language designs, we are not able to formally prove these metatheorems about $\typy$ (because it incorporates the entirety of Python.)\todo{move to disc?} 


Consider type validation (Sec. \ref{sec:types}): the fragment delegated control over \li{fragment[idx]} is \li{fragment}. The choice is explicit in the term, so determinism and stability follow trivially.

For literal forms (Sec. \ref{sec:literals}), the fragment defining the type provided for analysis is delegated control. To establish determinism and stability, we need only establish that type normalization is deterministic and stable. Our language of type expressions is a standard deterministic lambda calculus \cite{DBLP:conf/lfmtp/Crary09} and normalization interacts with the fragment context only at canonical form, which was just discussed.

For function defintions (Sec. \ref{sec:fns}), the delegated fragment is the first decorator. Most statement forms also delegate to this fragment (Sec. \ref{sec:statements}.) The delegate is again stated explicitly, so the protocol is trivially deterministic and stable.

For targeted terms (Sec. \ref{sec:targeted-forms}), $\typy$ synthesizes a type for the target. For binary terms (Sec. \ref{sec:binary-forms}), $\typy$ also synthesizes types for sub-terms. For determinism and stability to hold, then, we need that type synthesis, implemented by \li{ctx.syn}, is deterministic and stable. This is a straightforward inductive argument, with the base case being variable forms. Variables are tracked by the variable context, which assigns each variable a unique type, so determinism holds. Variables lookup is independent of the fragment context, so stability holds. For binary forms, the only remaining requirement is that the possibilities described in Sec. \ref{sec:binary-forms} are mutually exclusive and do not depend on the fragment context, which is apparent by inspection.

Finally, for patterns (Sec. \ref{sec:pattern-matching}), the fragment defining the type that the scrutinee synthesizes is delegated control. Again, determinism and stability follow from determinism and stability of type synthesis.


\section{More Examples}\label{sec:more-examples}
% In the previous section, we described fragments that express the semantics of standard functional constructs. We also introduced the \li{py} fragment, which embeds Python, a dynamic language, into $\typy$ by treating it like a ``uni-typed'' language \cite{pfpl,scott1980lambda}. Finally, we showed the \li{string_in} fragment, which express a semantics of constrained strings \cite{sanitation-psp14}. 
In this section, we will further demonstrate the flexibility of $\typy$'s fragment system with some more complex examples.

\subsection{Prototypal Object Types}
\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}
from typy import component
from typy.std import proto, decimal, fn, unit
from listing1 import Listing1

@component 
def Listing9():
  Transaction [: type] = proto[
    amount : decimal,
    incr   : fn[Transaction, unit]
    proto  : Listing1.Account
  ]

  @Transaction ^\label{line:test_trans-start}^
  def test_trans():
    amount = 36.50
    def incr(self): self.amount += 1 ^\label{line:mutable-field}^
    proto = Listing1.test_acct ^\label{line:test_trans-end}^

  test_trans.incr() # self passed automatically ^\label{line:auto-self}^
  print(test_trans.name) # prints Harry Q. Bovik^\label{line:proto-inherit}^
\end{lstlisting}
\caption{Prototypal objects in $\typy$.}
\label{fig:proto-example}
\end{codelisting}

JavaScript's object system supports \emph{prototypal inheritance} (based on a similar mechanism in the Self language \cite{Ungar:Smith:oopsla:1987,Lie86}.) We have implemented a statically typed variant of this system as a fragment, \li{typy.std.proto}. 

Listing \ref{fig:proto-example} defines a \emph{prototypal object type}, \li{Transaction}, that specifies fields named \li{amount}, \li{incr} and \li{proto}. We introduce a value of this type using the \li{def} form on Lines \ref{line:test_trans-start}-\ref{line:test_trans-end} (i.e. this form is treated as a literal form, per Sec. \ref{sec:fns}, so $\typy$ calls \li{proto.ana_FunctionDef}.) The inner \li{def} form, on Line \ref{line:mutable-field}, is governed by the \li{fn} fragment because \li{proto.ana_FunctionDef} analyzes it against a \li{fn} type (i.e. it also behaves as a literal.) As such, no type annotations or decorators are needed.

The fields of a prototypic object are mutable, e.g. as shown in the body of \li{incr} on Line \ref{line:mutable-field}. The delegation protocol treats an assignment of this form as a targeted form, per Sec. \ref{sec:statements}.

On Line \ref{line:auto-self}, we call the \li{incr} method. The \li{proto} fragment implicitly passes in the target of the method call as the first argument, as in Python and similar to JavaScript.

When a field is not found in the object itself, e.g. \li{name} on Line \ref{line:proto-inherit}, the \li{proto} fragment delegates to the \li{proto} field. Here, the prototype is the record value \li{Listing1.test_acct}. 

\subsection{Low-Level Foreign-Function Interfaces}\label{sec:ffis}


\begin{codelisting}[t]
\vspace{-3px}
\begin{lstlisting}
from typy import component
from typy.numpy import array, f64
from typy.opencl import buffer, to_device

@component
def Listing10():
  # (device selection code elided)
  # make numpy array + send to device
  x [: array[f64]] = [1, 2, 3, 4] ^\label{line:make-array}^
  d_x = to_device(x) # device buffer ^\label{line:to_device}^

  # define a typed data-parallel OpenCL kernel
  @tycl.kernel ^\label{line:add5-start}^
  def add5(x : buffer[f64]):
    gid = get_global_id(0) # OpenCL primitive
    x[gid] = x[gid] + 5 ^\label{line:add5-end}^

  # spawn one device thread per element and run
  add5(d_x, global_size=d_x.length) ^\label{line:call-add5}^

  y = d_x.from_device() # retrieve from device^\label{line:line:from_device}^
  print(y.to_string()) # prints [6, 7, 8, 9] ^\label{line:from_device-to_string}^
\end{lstlisting}
\caption{\li{numpy} and OpenCL in $\typy$.}
\label{fig:numerics}
\end{codelisting}
Python is widely used in scientific computing \cite{oliphant2007python}. One reason is that Python has support for calling into low-level languages like C (e.g. via SWIG \cite{beazley2003automated}.) Many popular libraries, e.g. \li{numpy} \cite{van2011numpy}, operate essentially as wrappers around low-level routines written in these languages. It is also possible to dynamically compile low-level code generated by Python code as a string. This is particularly useful when working with GPUs and other compute devices, e.g. using the PyCUDA and PyOpenCL libraries \cite{klockner2011pycuda}.

We have designed fragments that allow for statically typed access to these libraries. For example, on Line \ref{line:make-array} of Listing \ref{fig:numerics}, we create a typed \li{numpy} array of 64-bit floating point numbers. The \li{typy.numpy.array} fragment supports the use of list literal syntax to do so. As such, the cost of the type annotation is ``canceled out'' because we don't need to explicitly call \li{numpy.array} as one does in Python. For arrays in analytic position (e.g. as function arguments), this interface to \li{numpy} is therefore of lower syntactic cost.

On Line \ref{line:to_device}, we invoke the \li{to_device} operator to transfer the \li{numpy} array to the compute device's memory (we omit the code needed once per session to select a device.)

On Lines \ref{line:add5-start}-\ref{line:add5-end}, we then define a typed OpenCL kernel \cite{opencl11}. An OpenCL kernel is simply an OpenCL function that is called in a \emph{data parallel} manner, i.e. a large number of threads are spawned, all running the same kernel. Each kernel has access to a unique ID, called the \emph{global ID} in OpenCL. Here, \li{add5} determines its global ID and then adds \li{5} to the corresponding element in the input buffer. Notice that we did not need to specify a return type or a type annotation on \li{gid}, because $\typy$ is bidirectionally typed (unlike OpenCL.) The translation of the definition of \li{add5} uses a Python encoding of OpenCL ASTs. It is equivalent to the following Python code (assuming a variable \li{ctx}, which our library tracks implicitly):
\begin{lstlisting}[numbers=none]
add5 = pyopencl.Program(ctx, """
  __kernel void add5(__global double* x) {
    size_t gid = get_global_id(0);
    x[gid] = x[gid] + 5;
  }""").build()
\end{lstlisting}
The $\typy$ code is again more concise. Moreover, type errors \emph{in the OpenCL kernel} are detected ahead-of-time by $\typy$. This required us to implement the entirety of the OpenCL type system using $\typy$'s fragment system, including the logic of numeric type promotion and various other subtleties inherited from C. This represents the largest case study to date of our methodology. Interestingly, we were also able to extend OpenCL with various higher-level constructs, e.g. pattern matching and sum types, essentially as described in Sec. \ref{sec:typy-by-example}. In fact, in most cases we inherit from the original fragment, overriding only the translation methods. 

Line \ref{line:call-add5} invokes the \li{add5} kernel in a data parallel fashion on the device buffer \li{d_x}. The parameter \li{global_size} determines the number of threads -- here, one thread per array element. Finally, Lines \ref{line:line:from_device}-\ref{line:from_device-to_string} retrieve the result from the device and print out the result.

The details of the various fragments just described, are, of course, somewhat involved. The takeaway lesson, however, is that as the designers of $\typy$, we did not need to anticipate this particular mode of use. 
%Low-level array types and even the type system of a non-trivial foreign language (and extensions thereof) could all be realized using the $\typy$ fragment system as described in Sec. \ref{sec:typy-by-example}. 
A similar approach could be taken, independently, for CUDA, C and even higher-level languages for which a foreign interface is available in Python, like Java. In contrast, monolithic languages like MLj needed to primitively build in a type safe foreign interface \cite{Benton:1999:IWW:317636.317791}. %(though all of these would be a substantial amount of effort!)

\section{Related Work}\label{related}\label{sec:related-work}

%\parindent0pt
%
%\subsection{Active Libraries}

%We have implemented the full OpenCL programming language as  primary example showed how to support low-level GPU programming from a high-level language. Other languages have supported similar workflows, e.g. Rust \cite{rustgpu}. These have not been fundamentally extensible and suffer from the ``bootstrapping'' problem described in Sec. \ref{intro}. 



% Early work proposing the inclusion of compile-time logic in libraries led to the phrase {\it active libraries} \cite{activelibraries}. 
Our recent work on \emph{type-specific languages (TSLs)} in the Wyvern language used a bidirectionally typed protocol to delegate control over the parsing of literal forms to functions associated with type definitions \cite{TSLs}. This inspired our treatment of literal forms in $\typy$. Unlike Wyvern, $\typy$'s literal forms are parsed according to Python's fixed syntax. Unlike $\typy$, Wyvern has a monolithic semantics. Both mechanisms could exist in the same language, but presently do not. %TSLs give library providers more syntactic control. On the other hand, by repurposing Python's syntax, we benefit from an established ecosystem of tools (e.g. parsers, syntax highlighters, editor modes, etc.)  % The Wyvern formalism guarantees hygiene, using an approach that is also likely applicable in the setting of this paper.

Like $\typy$, Typed Racket is a statically typed language embedded into a dynamic language \cite{TypedScheme2008,culpepper2007advanced}. Typed Racket is not modularly extensible.% -- defining new semantic structures requires directly modifying the language.


% Qi is a typed Lisp dialect where library providers can introduce new typing rules declaratively \cite{tarver2008functional}. The type system is a ``bag of inference rules.'' The problem with this approach is that it permits non-determinism (i.e. more than one rule may apply to a term.) The fragment delegation protocol of $\typy$, on the other hand, is deterministic and stable under fragment composition. Our design is also easier to reason about because fragments have a narrow locus of control, i.e. determining which fragment has control over any particular term is straightforward.

Language-external mechanisms for creating and combining language dialects, e.g. extensible compilers like Xoc \cite{conf/asplos/CoxBCKK08}, JastAdd \cite{Ekman:2007:JEJ:1297027.1297029}, Polyglot \cite{Nystrom-Clarkson-Myers03}, JaCo \cite{zenger2001implementing}, Silver \cite{VanWyk:2010:SEA} and various ``language workbenches'' \cite{erdweg2013state}, do not guarantee determinism. In particular, these systems presume that new language constructs define new textual forms. These forms can conflict with one another when combined, i.e. syntactic determinism is not conserved. Copper, the syntax definition system in Silver, defines a modular analysis that guarantees syntactic determinism, but this requires verbose marking tokens and grammar names \cite{conf/pldi/SchwerdfegerW09}. In contrast, $\typy$ allows  different fragments to share common forms without qualification.

Putting syntactic determinism aside, many such systems also do not guarantee semantic determinism. This is because these systems allow extension providers to exert non-local control, e.g. by allow extension providers to define new inference rules that apply throughout the program, or by allowing extension providers to define new whole-program passes. This also incurs cognitive cost: programmers have no definitive way to identify which extension is in control of a given term. In contrast, $\typy$'s delegation protocol explicitly delegates control to a single fragment, in a stable manner.

Systems based on extensible attribute grammars (if used idiomatically), e.g. Silver \cite{VanWyk:2010:SEA}, and algebraic methods, e.g. object algebras \cite{oliveira2012extensibility}, give extension providers control over only those extensions to the abstract syntax that they have defined. However, even if somehow we needed only to extend the abstract syntax (leaving the concrete syntax alone), this is problematic: it becomes impossible to define functionality that operates by exhaustive case analysis (e.g. a pretty printer.) This is particularly problematic when a new such function is invented -- this is known as the \emph{expression problem} \cite{wadler1998expression,Reynolds75}. In contrast, $\typy$ operates over a fixed abstract syntax.

TeJaS is a typed variant of JavaScript that is implemented as a collection of mutually recursive ML modules, each defining a particular feature \cite{DBLP:conf/dls/LernerPGK13}. This means that modules cannot be distributed separately. A new module can redefine constructs defined elsewhere, so stability is not guaranteed.

% The problem is twofold: 1) systems like these allow extension/dialect providers to introduce new concrete and abstract syntactic forms. Our insight is that we can sidestep the difficulties inherent in doing so by instead repurposing a fixed syntax. %Another benefit of our approach is that it allows fragment providers only a narrow locus of control, i.e. determining which fragment has control over any particular term is straightforward.

%The Copper syntax definition system, used by Silver \cite{VanWyk:2010:SEA}, 

Proof assistants, e.g. TinkerType \cite{LevinPierce99}, PLT Redex \cite{Felleisen-Findler-Flatt09}, Agda \cite{norell2007towards} and Coq \cite{Coq:manual} can be used to inductively specify and mechanize the metatheory of languages. These tools generally require a complete specification (this  has been identified as a key challenge \cite{aydemir05tphols}.) Techniques for composing specifications and proofs exist \cite{conf/popl/DelawareOS13,Delaware11,conf/plpv/SchwaabS13}, relying on various algebraic methods to encode ``open'' term encodings (e.g. Mendler-style $f$-algebras \cite{conf/popl/DelawareOS13}), but these techniques  require additional proofs at ``combine-time''. %For example, Delaware et al. develop a technique based on software product lines that requires specifying assumptions at feature boundaries and providing proofs of these for each composition \cite{Delaware11}. In later work based on Mendler-style $f$-algebras, this problem of \emph{feature interactions} persists for many lemmas, including those related to tycon invariants (e.g. canonical forms) \cite{conf/popl/DelawareOS13}. 
Several authors, e.g. Chlipala  \cite{Chlipala10}, have suggested  proof automation as a heuristic solution to the problem of combine-time proof obligations. 
The $\typy$ fragment system does not work with inductive semantic specifications -- instead, fragment providers directly implement their intended semantics in Python (see Sec. \ref{sec:discussion}.) 

Refinement type systems \cite{Freeman91}, pluggable type systems \cite{Brac04a,Andreae:2006:FIP:1167473.1167479,brown2016build,DBLP:journals/toplas/MarkstrumMEMAN10} and gradual type system \cite{Siek06a,siek2007gradual} define additional static checks for programs written against an existing semantics. Some of these systems support fragmentary definitions of new analyses \cite{DBLP:journals/toplas/MarkstrumMEMAN10,brown2016build}. $\typy$ is different in that its semantics (static and dynamic) is itself programmable. In other words, $\typy$ is not a gradual type system for Python like \texttt{mypy} \cite{mypy}, but rather a distinct language that 1) repurposes Python's syntax; and 2) is defined by typed translation to Python. %We have implemented an interface to Python using our fragment system (i.e. the \li{py} fragment.) Calling into $\typy$ from Python, however, is only possible from within a $\typy$ component. Luckily, $\typy$ is itself embedded into Python as a library, rather than a separate compiler, so this is straightforward. 
Defining a fragmentary refinement system that sits atop our fragmentary semantics is an interesting avenue for future work. This might allow us to use \texttt{mypy}'s annotations as refinements of the \li{py} type.  %Python's reflection mechanisms are expressive enough that we can implement $\typy$ itself as a library, but nothing necessitates that we do so.

\emph{Lightweight modular staging (LMS)} is a Scala library that supports staged translation of well-typed Scala terms to other targets \cite{Rompf:2012:LMS}. In contrast, $\typy$'s type system is itself  programmable. No specific type structure is built in to $\typy$. As described in Sec. \ref{sec:ffis}, fragment providers can target (and even extend) different languages via a foreign interface. %We plan to further explore this approach in future work.

Macros implement local term rewritings \cite{Hart63a,ScalaMacros2013}. Our fragment system is similar in that translation methods  programmatically work with and generate ASTs, but the translation target is a different language -- Python -- from the source language -- $\typy$. In fact, macros can be implemented using our fragment system, by defining a singleton type for the macro for which the call operation constructs the rewriting and then asks the context to typecheck and translate it.


{\it Operator overloading} \cite{vanWijngaarden:Mailloux:Peck:Koster:Sintzoff:Lindsey:Meertens:Fisker:acta:1975} and {\it metaobject dispatch} \cite{Kiczales91} interpret operator invocations as method calls. The method is typically selected according to either the type or the dynamic tag of one or more operands. These protocols are similar to our delegation protocol for targeted expressions. However, our strategy is a {\it compile-time} protocol and gives direct control over typing and translation. An object system with  operator overloading could be implemented in $\typy$. %Note that we used Python's operator overloading and metaobject protocol for convenience in the static language. %Our dispatch protocol was designed to be conceptually similar to Python's operator overloading protocol \cite{python}.

%Generic functions are a novel strategy for {\it function polymorphism} -- defining functions that operate over more than a single type. In \texttt{typy}, generic functions are implicitly polymorphic and can be called with arguments of {\it any type that supports the operations used by the function}. This is related to structural polymorphism \cite{malayeri2009structural}. Structural types make explicit the structure required of an argument, unlike generic functions, which are only given singleton types because the structure may depend on the semantics of an active type. 
%Structural typing can be compared to the more \emph{ad hoc} approach taken by dynamically-typed languages like Python itself, sometimes called ``duck typing''. It is also comparable to the C++ template system, as discussed previously. 



% Bidirectional type systems are increasingly being used in practical settings, e.g. in Scala and C\#. They are useful in producing good error messages (which our mechanism shows can be customized) and help avoid the need for redundant type annotations while avoiding decidability limitations associated with whole-program type inference. Bidirectional techniques have also been used for adding refinement types to languages like ML and Twelf \cite{Lovas08abidirectional}. Refinement types can add stronger static checking over a fixed type system (analagous to formal verification), but cannot be used to add new operations directly to the language. Our use of a dynamic semantics for the static language relates to the notion of \emph{type-level computation}, being explored in a number of languages (e.g. Haskell) for reasons other than extensibility. %Combining the approaches may be a fruitful avenue for future work. 

%\begin{figure*}[t]
%\small
%\vspace{-10pt}
%{\onecolumn
%\begin{longtable}{l l@{}l c@{}c c@{}c c@{}c c@{}c}
%%\toprule%
%
%&&  &&  {\bfseries Itself} && {\bfseries Extensible} && {\bfseries Extensible} && {\bfseries Unambiguous}\\
%
%{\bfseries Approach} && {\bfseries Examples} && {\bfseries Library} && {\bfseries Syntax} && {\bfseries Type System} && {\bfseries Composition}  \\
%
%\cmidrule(l){1-1} \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7} \cmidrule(l){8-9} \cmidrule(l){10-11} 
%
%\endhead
%
%
%Active Types && && \CIRCLE && \Circle && \CIRCLE && \CIRCLE  \\
%\myrowcolour%
%Type-Specific Literals && \cite{Omar:2013:TWP:2489812.2489815} && \Circle && \CIRCLE && \Circle && \CIRCLE  \\
%Desugaring && \cite{erdweg2011sugarj} && \Circle && \CIRCLE && \Circle && \Circle\\
%\myrowcolour%
%Typed LISP && Qi, \cite{TypedScheme2008} &&  \CIRCLE && \Circle && \CIRCLE && \Circle \\
%Attribute Grammars && \cite{VanWyk:2010:SEA}, \cite{conf/asplos/CoxBCKK08} && \Circle && \CIRCLE && \CIRCLE && \Circle\\
%%\myrowcolour
%%Static macros /  && \cite{ScalaMacros2013}, \cite{MorphJ2011}, && \Circle && \Circle && \Circle && \CIRCLE && \Circle \\
%%\myrowcolour
%%Metaprogramming &&  \cite{Sheard:1999:UMS} && && && && && \\
%%
%%Cross-Compilation && \cite{Rompf:2012:LMS, Delite2011} && \CIRCLE && \Circle && \Circle && \Circle && \CIRCLE \\
%%
%\myrowcolour%
%%EDSL Frameworks && ? && \CIRCLE && \CIRCLE && \CIRCLE && \Circle && \Circle \\
%\end{longtable}}
%\caption{Comparison to related approaches to language extensibility.}\label{relatedtable}
%\end{figure*}
%
%TODO: mention this http://blog.codeclimate.com/blog/2014/05/06/gradual-type-checking-for-ruby/

\section{Discussion}\label{sec:discussion}
In summary, $\typy$ is a bidirectionally typed programming language with no primitive types. Instead, it is organized around a novel \emph{semantic fragment system} that allows library providers to implement the kinding semantics for new types, the static and dynamic semantics of their associated operations and the pattern matching semantics of their associated patterns programmatically. Library clients can import these fragments in any combination because fragments are contextually delegated control over terms in a deterministic and stable manner. Unlike other language extension systems, the syntax of the language is fixed, which we take to be a feature of our system because it eliminates a number of difficult problems related to composition. 

We were able to implement $\typy$ itself as a Python library, using Python's standard reflection and code generation facilities. Using $\typy$, we have been able to implement a variety of semantic structures that are, or would need to be, built primitively into other languages.

%We achieve this by combining a bidrectional type system and an elaboration semantics and delegating to static functions associated with user-defined type constructors, rather than operating in the usual syntax-directed manner. 
%Our examples were not trivial, showing a prototypic object system, a powerful string tracking system and immutable records coexisting as libraries within a language that initially had no static typechecking at all. 

% Although we only touched on the details here, we have conducted a substantial case study using \texttt{typy}: an implementation of the entirety of the OpenCL type system (a variant of C99), as well as several extensions to it, as a library. Our operations translate to Python's lower-level FFI with OpenCL but guarantee type safety at the interface between languages. A neurobiological circuit simulation framework has been built atop this library (a detailed case study is in preparation). %Based on our examples in this paper and this admittedly anecdotal experience report, we submit that active type constructors deserve broad consideration by the research community as a foundational technique for structuring typed programming languages. We are currently refactoring these projects against the (new) bidirectional semantics described here. The language core of  has already been implemented.

% Because $\typy$ fragments are implemented in Python, it is difficult to rigorously prove metatheoretic properties about them (though formal specifications of a large portion of Python do exist \cite{politz2013python}.) 

Our design does have its limitations. Python is a complex dynamic language, so we are not able to rigorously prove determinism and stability. Our argument is simply that these properties are essentially immediate consequences of our proposed design. The same complexity makes it difficult for fragment providers to reason about correctness (relative to, e.g., an inductive specification.) 
In the future, we hope to develop a dialect of $\typy$ using a reduced subset of Python (e.g. RPython \cite{ancona2007rpython} or $\lambda_\pi$ \cite{Politz:2013:PFM:2509136.2509536}) or a simpler language still for which a complete formal definition is available. Another approach would be to design a fragment system where the fragment definition language is dependently typed. This would make it possible to prove interesting correctness properties about fragments. By imposing stronger abstraction barriers between fragments, we conjecture that it should be possible for the language to guarantee that a broad class of such properties are conserved by fragment composition, without the need for ``combine-time'' proofs. 

It is not presently possible to define fragments using $\typy$ itself, but this is an interesting future direction. It would also be interesting to automate the generation of fragment definitions from inductive specifications, e.g. building on the techniques developed by the Veritas project \cite{grewe2015type}. 
 %A related line of future work is to allow new fragments to be written using $\typy$ itself, rather than Python. %Finally, we have made substantial progress on a ``minimal'' variant of our fragment system based on the typed lambda calculus. %Due to space limitations, it is impossible to give a satisfactory account of this calculus in this paper. 

%Like many other ``pragmatic'' language designs, we are not able to formally prove these metatheorems about $\typy$ (because it incorporates the entirety of Python.)\todo{move to disc?} In contrast, when the semantics is a ``bag of inference rules'', proofs of these properties often require sophisticated reasoning.

% Python is a complex dynamic language, so it is difficult to formally verify that fragments are correct and do not explicitly  interfere with one another (though a formal specification of a substantial subset of Python does exist \cite{Politz:2013:PFM:2509136.2509536}.) 



The fragment system that we have developed here could be adapted to use a different surface syntax and internal language without major difficulty. If the internal language itself has non-trivial type structure, e.g. JVM bytecode, then fragments must define a type translation method (to complement the type validation method.) Moreover, term translations must be validated against the corresponding type translations. This correctness condition has been studied in the design of the TIL compiler for Standard ML \cite{tarditi+:til-OLD}. 

Another aspect of translation validation that we did not consider here is \emph{hygiene}, i.e. that the translations do not make inappropriate assumptions about the surrounding bindings, or inadvertently shadow bindings in an unexpected manner \cite{Kohlbecker86a,DBLP:conf/popl/Adams15}. A proper hygiene mechanism would require that we use an internal language with a more disciplined binding structure. For now, the context simply provides a method for generating unique identifiers.

By repurposing Python's syntax, $\typy$ benefits from many established Python tools. However, debuggers and other tools that rely not just on Python's syntax but also its semantics do not work directly on $\typy$ programs. We leave the problem of integrating fragments with tools like these as future work. %In future work, we hope to integrate a debugger into $\typy$ and explore the possibilities of fragment-specific debugging facilities. 

%Type systems that require tracking specialized contextual information (e.g. ML-style exceptions \cite{pfpl,harper1997programming}) are possible using our framework, but a corresponding attribute of the context object needs to be initialized by the fragment the first time that fragment is delegated control. This is somewhat awkward, so we plan to explore a mechanism that allows imported fragments to initialize the context ahead of time. 

$\typy$ imposes a bidirectional structure on all fragments. This structure is known to be highly flexible \cite{conf/icfp/DunfieldK13}, and even advanced dependently typed languages like Agda are fundamentally bidirectional \cite{norell2007towards}. That said, we have not explored the practicality of implementing advanced type systems, e.g. dependent or linear type systems, using our fragment system.  It would not be straightforward to implement type systems that are not bidirectional, e.g. those that rely on non-local type inference or flow typing \cite{Flow,guha2011typing}. 

% We have experimented with various extensions of $\typy$ that we did not have space to detail here. In particular, we have added a notion of fragmentary implicit coercion to $\typy$ (which generalizes subtyping \cite{DBLP:journals/iandc/Breazu-TannenCGS91}.) We have also experimented with a system where literal forms can be annotated with \emph{incomplete types}, i.e. types where the type index contains an ellipsis. For example, rather than stating all of the element types in the tuple below, the \li{tpl} fragment synthesizes the complete index:
% \begin{lstlisting}[numbers=none]
% (x, y, z) [: tpl[...]]
% \end{lstlisting}

Finally, we must acknowledge that not all fragments will be tasteful. This concern must be balanced against the possibilities of a vibrant ecosystem of competing fragments. % Because fragments are developed and deployed as libraries, they can more easily be evaluated ``in the wild.'' 
We plan to curate a substantial standard library of high-quality fragments. This will help avoid the problem of different programmers reimplementing the same structures. With an appropriate community process, our position is that a fragmentary language like $\typy$ will hasten the research, development and adoption of good ideas, particularly those that are found only in obscure languages today.


% There remain several promising avenues for future work, many of which we mentioned throughout this paper. From a practical perspective, extensible implicit coercions (i.e. subtyping) using a mechanism similar to our ``handle sets'' mechanism for binary expressions would be useful. An extensible mechanism supporting type index polymorphism would also be of substantial utility and theoretical interest.  We believe that active types can be used to control debugging and other tools, and plan to explore this in the future. We have also not yet evaluated the feasibility of implementing more advanced type systems (e.g. linear, dependent or flow-dependent type systems) and those that require a more ``global'' view (e.g. security-oriented types) using our framework. 

% From a theoretical perspective, the next step is to introduce a static semantics for the internal language and the static language, so that we can help avoid issues of extension correctness and guarantee extension safety (by borrowing techniques from the typed compilation literature). An even more interesting goal would be to guarantee that extensions are mutually conservative: that one cannot weaken any of the guarantees of the other. We believe that by enforcing strict abstraction barriers between extensions, we can approach this goal (and a manuscript is in preparation). Bootstrapping the \texttt{typy} compiler would be an interesting direction to explore, to avoid the awkwardness of relying on a loose dynamically typed language to implement static type systems.
% Different dialects can be difficult to combine manually, as we discussed in the introduction. We have implemented the full OpenCL programming language (a variant of C99) as a library (though the details are left for future work), showing that such languages can instead be written as libraries, rather than as dialects.
% 
%\subsection{Compositional Reasoning}\label{safety}
%Every statement and expression in an \texttt{typy} function is governed by exactly one active type, determined by the dispatch protocol, or by the single active base associated with the \texttt{typy} function. The representational consistency check ensures that compilation is successful without requiring that types that abstract over other types (e.g. container types) know about their internal implementation details. Together, this permits compositional reasoning about the semantics of \texttt{typy} functions even in the presence of many extensions. This stands on contrast to many prior approaches to extensibility, where extensions could insert themselves into the semantics in conflicting ways, making the order of imports matter (see Sec. \ref{related}). 


%\begin{enumerate}
%\item We address the ``chicken-and-egg'' problem by designing \texttt{typy}, an extensible language  embedded within Python \cite{Politz:2013:PFM:2509136.2509536,python} entirely as a library. The top-level of an \texttt{typy} file is a \emph{compilation script} written in Python and Python serves as \texttt{typy}'s \emph{type-level language}, as we will discuss.  \texttt{typy} and Python thus share a common syntax and package system, so users of \texttt{typy} can directly leverage established tools, infrastructure and coding standards. Functions marked by a decorator as under the control of \texttt{typy} are, unlike Python functions, \emph{statically} typechecked. This makes it possible to rule out many potential issues ahead of execution, and thus  avoid  costly dynamic checks. %The semantics of  define run-time behavior.
%\item  % Instead, \texttt{typy} has a statically-typed semantics that can be extended by users from within libraries. 
%There is \emph{no particular type system built into \texttt{typy}}. Instead, a type is any  object implementing the \lip{ace.Type} interface constructed by the compilation script. We call these \emph{active types}. When statically assigning a type to a compound form, e.g. \lip{e.x}, the \texttt{typy} {compiler} delegates to the type of a subexpression, e.g. to the type of \lip{e}, to determine how to assign a type to the expression as a whole. As we will show, this {type-directed protocol}, \emph{active typing}, permits the expression of a rich variety of type systems as libraries. Unlike syntax-directed protocols, there cannot be ambiguities when separately defined type sytems are combined (types control non-overlapping sets of expressions by construction). Base cases, e.g.  variables and certain statement forms, are handled on a {per-function basis} by a \emph{base semantics}, also a compile-time object implementing an interface, \lip{ace.Base}.
%\end{enumerate}
%% As a result, clients are able to import any combination of extensions with the confidence that link-time ambiguities cannot occur .
%
%The dynamic behavior of an \texttt{typy} function is determined by translation to a \emph{target language}. We begin by  targeting Python, then discuss targeting OpenCL and CUDA, lower-level languages used to program many-core processors (e.g. GPUs). Active types and bases control translation by the same type-directed protocoThere remain several promising avenues for future work, many of which we mentioned throughout this paper. From a practical perspective, extensible implicit coercions (i.e. subtyping) using a mechanism similar to our ``handle sets'' mechanism for binary expressions would be useful. An extensible mechanism supporting type index polymorphism would also be of substantial utility and theoretical interest.  We believe that active types can be used to control debugging and other tools, and plan to explore this in the future. We have also not yet evaluated the feasibility of implementing more advanced type systems (e.g. linear, dependent or flow-dependent type systems) and those that require a more ``global'' view (e.g. security-oriented types) using our framework. 

% From a theoretical perspective, the next step is to introduce a static semantics for the internal language and the static language, so that we can help avoid issues of extension correctness and guarantee extension safety (by borrowing techniques from the typed compilation literature). An even more interesting goal would be to guarantee that extensions are mutually conservative: that one cannot weaken any of the guarantees of the other. We believe that by enforcing strict abstraction barriers between extensions, we can approach this goal (and a manuscript is in preparation). Bootstrapping the \texttt{typy} compiler would be an interesting direction to explore, to avoid the awkwardness of relying on a loose dynamically typed language to implement static type systems.l that governs type assignment. We call this phase \emph{active translation}. A \emph{target} mediates this process and is also a user-defined compile-time object implementing an interface, \lip{ace.Target}. 
%%One base or type can support multiple targets. 
%When targeting a typed language, care must be taken to ensure that well-typed \texttt{typy} expressions have well-typed translations, so we integrate a technique for compositionally reasoning about compiler correctness (developed for the TIL compiler for Standard ML \cite{tarditi+:til-OLD}) into the language.
%
%%TODO
%%
%%Static type systems are powerful tools for programming language design and implementation. By tracking the type of a value statically, a typechecker can verify the absence of many kinds of errors over all inputs. This simplifies and increases the performance of the run-time system, as errors need not be detected dynamically using tag checks and other kinds of assertions.
%
%It is legitimate to ask, however, why dynamically-typed languages are so widely-used in multiple domains. Although slow and difficult to reason about, these languages generally excel at satisfying the criteria of \textbf{ease-of-use}. More specifically, Cordy identified the principle of \emph{conciseness} as elimination of
%redundancy and the availability of reasonable defaults \cite{cordy1992hints}. Statically-typed languages, particularly those that professional end-users are exposed to, are often verbose, requiring explicit and often redundant type annotations on each function and variable declaration, separate header files, explicit template headers and  instantiation and other sorts of annotations.  Dynamically-typed languages, on the other hand, avoid most of this overhead by relying on support from the run-time system. \texttt{typy} was first conceived to explore the question: \emph{does conciseness require run-time mechanisms, or can one develop a statically-typed language with the same usability profile?}
%
%
%Rather than designing a new syntax, or modifying the syntax of an existing language, we chose to utilize, \emph{without modification}, the syntax of an existing language, Python. This choice was not arbitrary, but rather a key means by which \texttt{typy} satisfies extrinsic design criteria related to familiarity and tool support. Researchers often dismiss the importance of syntax. By repurposing a well-developed syntax, they no longer need to worry about the ``trivial'' task of implementing it.
%
%Most programming languages are {\em monolithic} -- a collection of primitives are given first-class treatment by the language implementation, and users can only creatively combine them to implement  abstractions of their design. Although highly-expressive general-purpose mechanisms have been developed (such as object systems or functional datatypes), these may not suffice when researchers or domain experts wish to evolve aspects of the type system, exert control over the representation of data, introduce specialized run-time mechanisms, if defining an abstraction in terms of existing mechanisms is unnatural or verbose, or if custom error messages are useful. In these situations, it would be desirable to have the ability to modularly extend existing systems with new compile-time logic and be assured that such extensions will never interfere with one another when used in the same program.
%
%Python does not truly prevent extensions from interfering with one another because it lacks, e.g., data hiding mechanisms. One type could change the implementation of another by replacing its methods, for example. But these kinds of conflicts do not occur ``innocently'', as conflicts between two libraries in SugarJ that use similar syntax might.
% 
%
%
 %We also anticipate that coding guidelines and tools mandating the use of abstractions that are known to have certain desirable properties will replace designer-mandated enforcement.
% 
%This paper is presented as a language design paper. The theoretical foundations of this work lie in type-level computation. We have developed a simplified, type-theoretic formalism of active typechecking and translation, where we prove the safety properties we have outlined here, as well as several additional ones that are only possible to achieve using a typed metalanguage (currently in submission at ESOP 2014). The work described here  extends that mechanism in several directions: \texttt{typy} supports a rich syntax, makes syntax trees available to extensions, supports a pluggable backend and per-function base semantics. It is implemented in an existing language and supports a wider range of practical extensions. 

%\acks
%The author is grateful to Jonathan Aldrich, Robert Bocchino and anonymous referees for their useful suggestions. This work was funded by the DOE Computational Science Graduate Fellowship under grant number DE-FG02-97ER25308.
%Acknowledgments, if needed.

\vspace{-3px}\paragraph{Implementation}
At the time of submission, we have implemented the fragment system and the fragments described in this paper, though many of these fragments were developed against previous versions of $\typy$, which had a somewhat different API. We plan to update these and release \verb|typy| and its standard library as a free open source project before GPCE. %The \lip{typy} website will tentatively be found at \url{http://www.cs.cmu.edu/~comar/typy/} (we have not made a website at the time of submission.)

% \section{Conclusion}%This appears to be a widespread problem: programmers and development teams often cannot use  the constructs they might prefer because they are only available in specialized dialects they cannot  adopt \cite{Meyerovich:2012:SDR:2414721.2414724,Meyerovich:2013:EAP:2509136.2509515}.\todo{finish this} %

% Chicken-and-egg problem


% First, it is widely believed that new we must be careful not to allow syntactic conflicts between separately developed constructs.

% Alas, the candidate mechanisms available today, discussed in Sec. \ref{related}, suffer from two problems. 

% First, they allow syntactic and semantic conflicts between separately developed constructs. These conflicts, if detected at all, are detected far too late: when a client happens to use the conflicting constructs within the same program. The client must patch the problem on their own. The exceptions are mechanisms that support the definition of modularly composable  syntactic sugar, as in Copper \cite{conf/pldi/SchwerdfegerW09} and our work on Wyvern \cite{TSLs,sac15}, but these operate atop a fixed semantics. 

%Second, existing mechanisms are often themselves  tied to an obscure language dialect or compiler infrastructure, and so face the classic ``chicken-and-egg'' problem: they face the same barriers to adoption as other languages. %Clients are not generally capable of understanding and fixing the issue productively.% Too many constraints can leave it difficult to express real-world abstractions.%It is difficult to guarantee that extensions are sound in isolation and more so to guarantee that they can be {safely} combined. %This requires further research before it would be appropriate to widely rely on them (which we will discuss as we go on).

% \paragraph{Terminology}
% A type system is typically defined in fragments organized around (and often identified by) a \emph{type constructor} (e.g. a fragment defining binary product types, organized around the type constructor $\fvar{prod}$). The syntax of each fragment can be characterized by its contributions to the \emph{concrete syntax} (e.g. expression forms \lip{(e, e)}, \lip{e.l} and \lip{e.r} and type form \lip{T * T}) and the \emph{abstract syntax}, where terms are written uniformly by applying a \emph{term constructor} to an \emph{index} and zero or more \emph{arguments}, e.g. $\FF{pair}[\FF{triv}](e_1; e_2)$ or $\FF{prj}[\FF{left}](e)$. Term indices are evaluated, if necessary, in an earlier \emph{phase} than term arguments and trivial indices are often omitted for clarity of presentation. The indices of expression terms,  like these examples,  are \emph{static terms}, $\sigma$. \emph{Types}, $\tau$, can be seen as static terms constructed by static term constructor $\FF{ty}$, indexed by a type constructor and having another (possibly trivial) static term as an argument, called (by convention) the \emph{type index}, e.g. $\FF{ty}[\fvar{prod}](\FF{spair}(\tau_1; \tau_2))$, often  abbreviated $\fvar{prod}\texttt{[}\tau_1; \tau_2\texttt{]}$ (%or stylized $\tau_1 \times \tau_2$. %Static terms are also called \emph{type-level terms}.  
%  often, the static terms are not made explicit but rather assumed to be in the ambient metatheory).%Types classify expressions according to the rules comprising the fragment's type system. A type safe language is one where evaluation of a well-typed expression cannot lead to undefined behavior or result in a value of a different type, greatly simplifying static reasoning about program behavior and eliminating the need for many run-time checks. 

% %In a monolithic language design, decisions about which fragments are to be combined to constitute a complete  language are made centrally, so the designer can ensure that the concrete syntax is unambiguous and that other metatheoretic concerns, like guarantees of type safety, are maintained. To decentralize control over these decisions, so that fragments can be defined as libraries and combined arbitrarily, these guarantees must be shown to hold for all possible combinations of fragments deemed individually valid. This can quickly lead to difficulties.

%   Each fragment also defines rules that contribute to the \emph{static semantics} (specifying how types are assigned to expressions at ``compile-time'') and the \emph{dynamic semantics} (specifying the behavior of well-typed expressions at ``run-time'') \cite{pfpl}. The dynamic semantics of full-scale languages are generally implemented (or even directly specified, as in the Harper-Stone semantics for Standard ML \cite{Harper00atype-theoretic}) by elaboration to a simpler \emph{internal language}.% In some languages, static terms themselves may have their own static and dynamic semantics (the \emph{static statics} and the \emph{static dynamics}).  

% A better 
% This suggests a need for a mechanism that gives library providers the ability to modularly extend the semantics of a general-purpose language from within. In order to avoid the ``chicken-and-egg'' problem, we bootstrap this mechanism itself as a library inside a widely adopted language, Python.


% Our aim is to develop a mechanism that addresses these concerns and implement this mechanism itself as a library, \texttt{typy}, for a widely adopted language, Python, using its existing metaprogramming constructs. %Programmers can construct types and, using these, define typed functions, from within a Python script. Typed functions, unlike regular Python functions, go through a process of static typechecking and translation. %This process can occur either ahead-of-time or just-in-time. 




% \clearpage



% In this paper, we develop 

% This design constrains us: we must stay within the confines of Python's existing syntax. In fact, this constraint is freeing: questions about syntactic ambiguity never arise, the syntax is familiar and tools that require an exhaustive syntax specification (e.g. syntax highlighters and documentation generators)  work without modification. In other words, this design largely sidesteps the \emph{expression problem} \cite{wadler1998expression}.%Taken together, this design addresses many more of the the extrinsic concerns about adoption than a dialect-oriented approach. %Indeed, enabling the decentralized introduction of new term constructors is the standard example used when describing the more general problem of enabling the extension of a sum type with new cases as well as new  functions over its cases  \cite{Reynolds75}, prompting Wadler to dub it the \emph{expression problem} \cite{wadler1998expression}. 

% We show that you do not need an extensible syntax to support an extensible statically-typed  semantics. The trick is 1) to organize the semantics around a protocol that delegates control over typechecking and translation of each term to logic associated with some contextually-relevant \emph{type constructor} (\emph{tycon}, for brevity); and 2) to give the ability to define new tycons and the logic associated with them to library providers. We call this protocol \textbf{active typechecking and translation} (AT+T).

% For example, using \lip{typy}, we will show how a library provider can introduce record types, like those that are built in to languages like ML and Haskell, by defining a tycon called \lip{record}. A different library provider, inspired to give a static semantics to a Javascript-style prototypic object system, can define the active tycon \lip{proto}. We will discuss several other examples throughout the paper.

% Types are constructed by applying a tycon to a \emph{type index} using the subscript operator: \lip{tycon[idx]}. For example, we can construct a record type by applying \lip{record} to a mapping from row labels, represented as strings, to types:
% \begin{lstlisting}[numbers=none]
% R = record[
%   'label1' : dyn,    # we discuss dyn and string later
%   'label2' : string
% ]
% \end{lstlisting}
% Because the language of types, type indices and tycons is Python itself, we are able to use \emph{array slices} to emulate conventional syntax for row specifications, and assignment to define a type synonym, \lip{R}.% We assume that the types \lip{dyn} and \lip{string}, which we will return to shortly, were imported previously.

% The following prototypic object type has an analagous field structure (and for simplicity, the default prototype):
% \begin{lstlisting}[numbers=none]
% P = proto[
%   'label1' : dyn,   
%   'label2' : string 
% ]
% \end{lstlisting}

% The first question that one should ask when shown a new type is ``how do I introduce a value of this type?'' Values of \lip{typy} types are introduced inside \emph{typed functions}, which are those placed under the control of \lip{typy} by the placement of a decorator. Putting the details of typed functions aside for a moment, the form in Python's syntax most similar to the conventional introductory form for records in ML/Haskell is the dictionary literal form, so we would like to be able to introduce values of type \lip{R} inside a typed function using syntax like this:
% \begin{lstlisting}[numbers=none]
% {'label1': val1, 'label2': val2} 
% \end{lstlisting}
% But this is also the form that one would hope to use to introduce a value of the prototypic object type \lip{P} above. Moreover, we still need to be able to construct Python dictionaries, of type \lip{dyn}. 

% Inspired directly by our recent work on \emph{type-specific languages} in Wyvern \cite{TSLs}, we resolve this sort of ambiguity using \emph{local type inference} \cite{Pierce:2000:LTI:345099.345100}. When AT+T encounters an expression  of a literal form, it delegates semantic control over it to the tycon of the type that it is being analyzed against. For example, when the expression above appears as an argument to a function \lip{f}, then the argument type of \lip{f} determines its meaning:
% \begin{lstlisting}[numbers=none]
% f({'label1': val1, 'label2': val2})
% \end{lstlisting}
%  In positions where there is no expected type, like at the top level of a typed function, the programmer must provide an explicit type ascription. We co-opt Python's array slice syntax for type ascriptions:
% \begin{lstlisting}[numbers=none]
% myRecord = {'label1': val1, 'label2': val2} [: R]
% \end{lstlisting}
% Ascriptions can be placed at binding sites as well:
% \begin{lstlisting}[numbers=none]
% myObj [: P] = {'label1': val1, 'label2': val2}
% \end{lstlisting}

% The second question one should ask when presented with a new type is ``what can I do with a value of this type?'' For records, the fundamental operation is row projection. Similarly, for prototypic objects, it is field projection. In both cases, the most natural form in Python's syntax for projecting out a row/field labeled \lip{label1} is \lip{e.label1}, where \lip{e} is the record/object, so we must again come up with some way to resolve the ambiguity. Once again, our solution is to take a {tycon-directed} approach: AT+T delegates static control to  the tycon of the type of \lip{e}. %Symmetric binary forms, e.g. addition, have only a slightly more involved delegation protocol.

% We will see exactly how  tycons like \lip{record} and \lip{proto} statically control typechecking and translation (to standard Python code) of expressions that they are delegated control over in Sec. \ref{usage}. For now, it suffices to state that the code involved is essentially identical to what one would write if asked to implement a compiler for a dialect that built in the construct in question directly. The novelty of this paper comes not in the constructs themselves, but in how AT+T decentralizes control over the semantics. The locus of control is no longer the language definition but instead individual type constructors, defined in libraries.

% % A statically-valued function 
% % %, identified based on the name Python's grammar gives to the form, here \lip{syn_Attribute},  %So if \lip{e} is of record type, then the \lip{record} tycon  is delegated control, and if \lip{e} is of prototypic object type then \lip{proto} is delegated control, in either case by calling \lip{syn_Attribute} with a reference to the syntax tree of the term as well as a \emph{typing context}, which we will describe later. 
% % must  either synthesize a type for the term as a whole or indicate a type error. If it synthesizes a type, the tycon must then generate a \emph{translation} for the term as a whole, which determines its dynamic behavior. So, for the term \lip{e.label1}, if \lip{e} is of record type, then the \lip{record} tycon is delegated control. It will check that \lip{label1} is one of the row labels specified in the index of the type of \lip{e}. Records can be implemented by translation to tuples (the row labels are only needed statically), so the translation it then generates will project out the appropriate position from the translation recursively generated for \lip{e}. 

% % We will detail how tycons are implemented as Python classes in Sec. \ref{usage}.  The innovation is in how this protocol decentralizes control over a fixed set of forms, allowing us to define a range of semantic constructs in libraries. The language itself needs to specify only the \emph{delegation protocol} that determines how a tycon is delegated control over each syntactic form. 


% % More specifically, what this means is that the statically valued function  because that the tycon defines must either synthesize a type for the term as a whole or indicate a type error. If a type is synthesized, the tycon must also generate a \emph{translation} for the term as a whole. 

% % We call a type constructor equipped with a set of such statically valued functions an \emph{active type constructor}.

% % More specifically, the semantics are a \emph{bidirectionally-typed type-directed translation semantics}. 

% % allow the associated operations, the semantics of terms are no longer determined directly based on the identity of their term constructors (the usual ``syntax-directed'' approach). Instead, we delegate to a statically defined function associated with a relevant type constructor. The term constructor's identity only serves to determine how to determine this delegate. For example, consider the concrete term \lip{x.r} (where \lip{x} is a variable and \lip{r} is a static label), which, according to Python's grammar, maps to the abstract term \lip{Attribute("r"; Name("x"))} (i.e. the term constructor $\FF{Attribute}$, \emph{indexed} by a static representation of the label and one \emph{argument}, $e$). There are many possible meanings for this term. If $e$ is a pair, this might be a way to project out its right component. If $e$ were a functional record,  this might be the way to project out a field labeled \verb|r|. An object system might introduce a more complex semantics involving dynamic dispatch. The type of dynamically classified Python values would follow Python's own complex field lookup protocol. Were we to naively ``open up'' the syntax and semantics in an attempt to allow all of these features into our language, these semantics would conflict. 

% % Instead, our semantics will delegate responsibility for assigning a type and a translation to this term to a static function associated with the type constructor of the type recursively synthesized by $e$, the \emph{target} of the operation. Thus, tuples, records, objects, dynamic values and variants thereof can  share this syntax. 

% % An expression like \lip{e.label1} has a clear ``target'' subexpression that can be examined recursively to determine the tycon to delegate control to. For other forms, it is perhaps less clear. For example, the curly-brace-delimited form in the following expression could be a value of record type, or prototypic object type or be a dynamically typed Python dictionary, in which case it would have type \lip{dyn}:
% % \begin{lstlisting}[numbers=none]
% % f({label1: "val1", label2: "val2"})
% % \end{lstlisting}


% % For example, if \lip{f} is a function that takes an argument of a  record type, like the one defined above, then \lip{record} is delegated control over the curly-brace-delimited form. This can be understood as a variant of the mechanism that Wyvern introduced for resolving ambiguous syntax extensions \cite{TSLs}, here applied in a setting where there is a fixed syntax but an extensible semantics.% When a literal form appears at the top-level of a function, where there is no expected type, the programmer can provide a type annotation on an expression directly, using the form \lip{e [: T]}, where \lip{e} is an expression and \lip{T} is a type (or a tycon applied to an incomplete index, which we discuss in Sec. \todo{inc indices}).

% % Symmetric binary terms (e.g. addition) use a third protocol, which we will discuss. 

% Notably, the semantic flexibility of AT+T does not come at the risk of ambiguity  because there is always exactly one tycon delegated  control over each expression and this delegate remains stable no matter which other tycons have been defined. This stands in contrast to systems that treat the type system as a ``bag of rules'', e.g. .%This is a generalization of a previous mechanism that we have developed, which deals only with extending the elaboration, but not the typechecking, of the bodies of delimited literals \cite{omar-ecoop14}.

% %The approach we describe here is general with respect to the fixed syntax of the language it is introduced within. To implement it, we must only 1) define the delegation protocol that determines how a type constructor is delegated responsibility over each form, basing it on a bidirectional type system that distinguishes locations where an expression must \emph{synthesize} a type (e.g. to the right of bindings) from those where it can be \emph{analyzed} against a known type (this has also been called ``local type inference'' \cite{Pierce:2000:LTI:345099.345100}); 2) expose a fixed static language, which is used to implement active type constructors, construct types by applying type constructors to static indices and  import libraries; and 3) expose a fixed internal target language to the static language (fragment providers  implement the dynamic semantics of their fragments by translation to this language).

% \paragraph{Organization} The remainder of the paper is organized as follows:
% \begin{itemize}
% \item We provide more details on AT+T as realized in \verb|typy| by tracing through a complete program in Sec. \ref{usage}, covering \verb|typy|'s notion of {phase separation}, function types, the delegation protocol, the compilation model, interoperability with Python  and various conveniences, including \emph{incomplete ascriptions}.
% \item We outline more advanced examples of type system fragments that can be implemented as libraries using \verb|typy| in Sec. \ref{examples}: functional datatypes, as in ML and Haskell, and a typechecked foreign function interface to a low level language, OpenCL. 
% \item We give a minimal specification of AT+T using a typed lambda calculus, @$\lambda$, in Sec. 4. The theoretically minded reader may skip ahead to this section first.
% \item We compare active typing to various related work in Sec. 5.
% \item We conclude after discussing some limitations and directions for future work in Sec. 6. We also discuss the features necessary to implement active typing within other languages.
% \end{itemize}



% % Our specific aims in this paper are to introduce this approach, which we call \emph{active typechecking and translation}, and demonstrate that they are expressive, practically realizable within existing languages and theoretically well-founded. Our target audience is typed language fragment providers (researchers or domain experts) and language designers interested in extensibility mechanisms. % We do not aim to evaluate the claim that our approach will ultimately be of use to fragment clients (i.e. normal developers) in writing and reasoning about a variety of real-world programs (this first requires adoption by our target audience). 
% % We describe two artifacts in pursuit of these aims:
% % \begin{enumerate}
% % \item We build our mechanism as a library inside Python, \verb|typy|,  using its quasiquotation and reflection facilities.\footnote{The name was chosen as an initialism for ``actively  typed language'' and because Python marks function decorators, which we use extensively for this purpose, using the @ symbol.} Python is also an interesting starting point  because it  can be thought of as beginning with a  static type system providing just one trivially indexed type constructor, \texttt{dyn} \cite{pfpl}. Thus, our mechanism represents a modularly extensible gradual type system. Taken together, \texttt{typy} is almost completely backwards compatible with the Python ecosystem with remarkably little effort: we inherit a large number of  existing tools for working with Python files, Python's substantial packaging infrastructure and access to all existing Python libraries. We introduce \texttt{typy}  in Sec. \ref{usage}. We continue in Sec. \ref{examples} by briefly describing  some more sophisticated examples, including statically typed functional datatypes with nested pattern matching and a complete and statically safe FFI to OpenCL for GPU programming.
% % \item In Sec. \ref{theory}, we describe active type constructors in their essential form, developing a minimal calculus called @$\lambda$. The syntax provides only variables, variable binding constructs, type ascription and two generic term constructors. Perhaps surprisingly, we show how a conventional syntax like Python's can be recovered by a purely syntactic desugaring to these term constructors. The semantics for @$\lambda$ take the form of a \emph{bidirectionally typed elaboration semantics} where the available type constructors are tracked by a \emph{tycon context}. We use a simple lambda calculus as both our static and internal language to emphasize the fundamental character of the approach. For more theoretically minded readers or those with no familiarity with Python, this section can be read first.
% % \end{enumerate}
% %\texttt{typy} can be used from the shell, producing source files that can be further compiled or executed by external means. \texttt{typy} functions targeting a language with Python bindings (e.g. Python itself, as well as the others just described) can also be compiled and invoked interactively from Python. 
% %\texttt{typy} supports a form of staged \emph{ad hoc} polymorphism (based on singleton types) wherein the ``dynamic type'' of a Python value can determine a static type and trigger active typechecking and translation ``just-in-time'' when Python code calls a generic \texttt{typy} function. We show how this can significantly streamline interactive workflows (e.g. in scientific computing) that use Python for orchestration and exploratory tasks (e.g. plotting) but call into a statically-typed  language via a foreign-function interface (FFI) for performance- and safety-critical portions. We provide a complete implementation of the OpenCL type system (a language for low-level data parallel programming on GPUs), show several higher-level extensions to it, and discuss bindings that bridge between the type systems of \texttt{typy}, OpenCL and Python's standard numerics package, \lip{numpy}, all as a library (contribution 3, Sec. \ref{targets}).%We will discuss in detail as our primary realistic case study. 

% %The remainder of the paper is organized as follows: in Sec. \ref{usage}, we describe the basic structure and usage of \texttt{typy} with an example that uses several interesting \texttt{typy} type libraries. In Sec. \ref{att}, we detail how active typing and translation delegate control, at compile-time, to active types and bases, and discuss how this represents a form of extensibility that side-steps the expression problem. In Sec. \ref{theory}, we reduce this mechanism to  a core lambda calculus, $\lam\texttt{typy}$,  clarify the relationship to   type-level computation and show how type safety can be maintained. In Sec. \ref{targets}, we discuss generic functions, FFIs and staging via a complete implementation of the OpenCL type system (and extensions to it) as a library.  

% %In Sec.  \ref{related}, we describe how active type constructors relate to several threads of related work. We conclude in Sec. \ref{discussion} by summarizing the key features needed by a host language to practically support active type constructors, and delineating present limitations and promising directions for future work. %The appendix has details omitted for concision as well as several more examples, including functional datatypes, flexible complex numbers and a previously little-explored type system for tracking string invariants using regular expressions statically (which we show being used in Listing \ref{example}, amongst  other examples). 

% \section{\texttt{typy}}\label{usage}


% %
% Listing \ref{example} shows an example of a well-typed \texttt{typy} program. We will refer to it throughout this section.

% \paragraph{Phase Separation} The top level of a \texttt{typy} program is simply a Python script (versions 2.6+ and 3.0+ are supported). The execution of this script begins the \emph{static phase} of compilation, as suggested by line \ref{hello-ct}. The main purpose of this phase is to define the tycons and types that will be used by the \emph{typed functions} in the program, marked in this example by the decorator \lip{@fn}. The static phase ends once  these typed functions have gone through the process of active typechecking and translation. The generated translation is what ultimately determines the run-time behavior of the program. % The evaluation of the translation of a typed function represents the \emph{dynamic phase}. %For example, the string on line \ref{hello-ct} is a statically valued string. Types are also statically valued, as we will discuss shortly. 

% %Code inside , is evaluated during the dynamic phase, i.e. these functions can only be called after the active typechecking and translation process we will describe below is complete. %There are two ways to transition from the static phase to the dynamic phase, \emph{standalone compilation}, which we will discuss in Sec. \ref{external-compilation} and Sec. \
% % For example, the string on line \ref{account-start}, discussed further below, is \emph{dynamically valued}. %Of particular note, types are statically valued.%No features specific to its primary implementation, CPython, are needed (so \texttt{typy} should support alternative implementations like Jython and PyPy, though as of this writing this claim has not been tested). 

% \paragraph{Package Management} Using Python as \verb|typy|'s static language pays off immediately on the first line: \texttt{typy} uses Python's import mechanism directly, so Python's package management tools (e.g. \lip{pip}) and package repostories (e.g. \lip{PyPI}) are directly available for distributing \texttt{typy} libraries, including those defining type system fragments. In this case, all of the constructs we will use are part of the \verb|typy| standard library, \texttt{typy.std}, though \texttt{typy}'s semantics do not fundamentally privilege it. All existing Python libraries can be used from typed functions, as we will discuss later.% Here, \lip{typy.std} is the ``standard library'' but does not benefit from special support in \lip{typy} itself.% Nothing in \lip{typy.std} benefits from special support from \lip{typy}, the core language package. We will see \lip{typy} being used directly below. %\texttt{typy} can be installed with a single command: \emph{omitted for review}. %\texttt{typy} is agnostic about the particular design choices that we made in our examples.

% %The top-level statements in an \texttt{typy} file, like the \lip{print} statement on line 10, are executed to control the compile-time behavior, rather than the run-time behavior, of the program. %That is, Python serves as the \emph{compile-time metalanguage} (and, as we will see shortly, the \emph{type-level language}) of \texttt{typy}. %For readers familiar with C/C++, Python can be thought of as serving a role similar to (but more general than) its preprocessor and template system (as we will see).
% %\subsection{Active Typechecking} 

% % Python is dynamically typed, but functions governed by \verb|typy| (those decorated with \lip{@fn} in this example) will be statically typed (Section \ref{s:atc}) and then translated to dynamically typed Python code (Section \ref{s:atr}), both under the control of the type constructors of the types used within them. These steps can occur either during a standalone compilation phase (Section \ref{compilation}) or just-in-time upon their first invocation (Section \ref{interactive}). 

% \paragraph{Types}


% \begin{codelisting}[t]
% \begin{lstlisting}
% ^\label{imports-start}^from typy.std import (record, string, string_in, 
%   proto, decimal, dyn, fn, printf) ^\label{imports-end}^

% ^\label{hello-ct}^print "Hello, static world!"

% ^\label{decl-Account}^Account = record[ 
%   'name' : string, 
%   'account_num' : string_in[r'\d{2}-\d{8}']
% ]

% ^\label{decl-Transfer-start}^Transfer = proto[
%   'amount' : decimal[2], 
%   'memo' : dyn,
%   'account' :: Account # this field is the prototype
% ]^\label{decl-Transfer-end}^

% ^\label{log-transfer-start}^@fn
% def log_transfer(t):
%   ^\label{log-transfer-docstring}^"""Logs a transfer to the console."""
%   ^\label{log-transfer-sig}^{t : Transfer}
%   ^\label{log-transfer-print}^printf("Transferring %s to %s.", 
%     t.amount.to_string, t.name)^\label{log-transfer-end}^

% ^\label{main-start}^@fn
% def __main__():
%   ^\label{account-start}^account = {
%     'name': "Annie Ace", 
%     'account_num': "00-00000001"} [: Account]^\label{account-end}^
%   ^\label{log_transfer-start}^log_transfer({'amount': 5.50, 'memo': None, 
%     'account': account})
%   log_transfer({'amount': 15.00, 'memo': "Rent",
%     'account': account})^\label{main-end}^
  
% print "Goodbye, static world!"
% \end{lstlisting}
% \caption{[\texttt{listing\ref{example}.py}] A \texttt{typy} program.}
% \label{example}
% \end{codelisting}

% Types are constructed programmatically in the static phase by applying a type constructor to a type index:  \lip{tycon[idx]}. %Unlike many statically-typed languages, types are constructed programmatically, rather than  declared ``literally'' (e.g. classes in Java, datatypes in ML). %Put another way, 
% %\texttt{typy} supports \emph{static computation at the level of types} and Python is its type-level language. The static language subsumes th%The compilation script can assign shorter names to types for convenience (removing the need for  facilities like \lip{typedef} in C or \lip{type} in Haskell). 
% In Listing \ref{example}, we construct several types and define short synonyms for them using the standard Python assignment construct:
% \begin{enumerate}
% \item On line \ref{decl-Account}, we construct a record type, as discussed in Sec. 1, and define the synonym \lip{Account}. The row labels are written as statically valued strings (and could be statically computed rather than written literally, e.g. \lip{'na' + 'me'}). 
% \item The row labeled \lip{'account_num'} has the \emph{regular string type} \lip{string_in[r'\\d\{2\}-\\d\{8\}']}. Values of a regular string type are  guaranteed to be in the regular language specified by the type index, a regular expression written as a \emph{raw string literal} (per Python's usual conventions). In this case, this expresses the invariant that the account number must consist of two digits followed by a dash followed by 8 digits. The semantics of this fragment are based on a core calculus we have recently specified in a workshop paper \cite{sanitation-psp14}. 
%  %Here, \lip{record} and \lip{decimal} are {indexed type constructors}. 
% %For convenience, the index for a \lip{record} type can be provided using borrowed Python  syntax for array slices (e.g. \lip{a[start:end]}) to approximate conventional notation for type annotations. 

% \item On line \ref{decl-Transfer-start}, we construct a prototypic object type \cite{Lie86}. The field \lip{'amount'} has type \lip{decimal[2]}, which classifies decimal numbers having two decimal places, and the field \lip{'memo'} has type \lip{dyn}, which classifies all dynamically typed Python values.\footnote{The phrase ``dynamically typed'' is widely regarded as as a misnomer from a type theoretic perspective; ``dynamically tagged'' would be more accurate. We are guests inside Python, however, so we will not wage this battle here.} The field labeled \lip{'account'} has type \lip{Account}. It specifies this with a double colon, which \lip{proto} takes to mean that the value of this field should be used as the prototype. This will affect the semantics of the field projection operation: when a field is not present in the object itself, it will defer to the prototype. This allows us to  share one account value across multiple transfers, while conveniently allowing clients to access its rows transparently, as if they were fields in the object itself. This can be seen as a statically typed variant of Javascript's object system (which is a simplified variant of Self's object system \cite{Ungar:Smith:oopsla:1987}). %To our knowledge, this type system is not  expressible in other simply typed languages (we do not consider dependently-typed logics without a distinct phase separation).
% \end{enumerate}



% \paragraph{Type Constructors} Type constructors are classes inheriting from \lip{typy.Type}. For example, a portion of \lip{typy.std.record} is shown in Listing \ref{record_init}. During the static phase, types are instances of these classes. The usual Python instantiation protocol (involving a method named \lip{__init__}) is not used, for reasons that we will cover below. Instead, as just described, types are instantiated using the subscript operator. This is implemented internally using Python's metaclass mechanism to overload the subscript operator on the class object \cite{python}. The tycon can validate and normalize  type indices by defining a class method called \lip{initialize_idx}, as shown in Listing \ref{record_init}. %When \verb|typy| delegates control to a tycon during AT+T, discussed below, it will do so by invoking other methods. Both \emph{class methods} (i.e. methods where the first argument is the tycon itself) and \emph{instance methods} (i.e. methods where the first argument is a type constructed by the tycon) are used by \verb|typy|.% It is important to keep in mind that these method calls occur during the static phase.%, and thus AT+T constitutes a form of compile-time metaprogramming.%These methods are, as a point of emphasis, written in Python and invoked statically, by the protocols we will detail in the remainder of this section. It is useful in some rhetorical contexts to refer to a tycon equipped with such statically evaluated functions as an \emph{active type constructor}.

% \paragraph{Typed Functions}

% \begin{codelisting}
% \begin{lstlisting}
% class record(typy.Type):
%   @classmethod
%   def initialize_idx(cls, idx):
%     # turns slices into a dict for convenience
%     idx = _dict_from_slices(idx)
%     # checks that idx is a map from strings to types
%     _validate_sig(idx) # not shown
%     return idx

%   # ... more methods defined in later listings ...
% \end{lstlisting}
% \caption{Tycons are classes during the static phase. They have the opportunity to validate and normalize the type index.}
% \label{record_init}
% \end{codelisting}

% A \emph{typed function} can be defined by decorating a Python function definition with a tycon, here \lip{fn}, as shown on lines \ref{log-transfer-start} and \ref{main-start} of Listing \ref{example}. 

% Decorator syntax in Python is syntactic sugar for applying the decorator to the function being decorated \cite{python}. Because the decorator value in this case is a tycon, which is a Python class, we must again use Python's  metaclasses and operator overloading, here of the call operator, to enable this usage. This is one reason why we must use subscript notation, rather than function application notation, for type construction (another being the usefulness of array slice syntax for record-like types, which is only available inside the subscript form). 

% The result of this decorator transformation is  an instance of \lip{typy.Function}. The underlying Python function is discarded after \lip{typy} extracts its abstract syntax tree and \emph{static environment}, i.e. its closure and the globals dictionary of the Python module it was defined within, using Python's reflection capabilities and the \lip{inspect} and \lip{ast} packages in Python's standard library \cite{python}. The \lip{ast} package exposes Python's parser and abstract syntax tree. The reader is encouraged to refer to its documentation (available online at \url{https://docs.python.org/2/library/ast.html#abstract-grammar}) to fully understand the example code in the remainder of this section. 

% Like standard Python functions, the body of a typed function can (optionally) begin with a \emph{docstring}. Unlike standard Python functions, the line immediately following the docstring specifies a \emph{type signature}, written by repurposing Python's dictionary literal syntax.\footnote{Python 3 introduced syntax for function type ascriptions, but we do not use it because 1) Python 3 is not yet widely adopted, for the reasons discussed in Sec. 1; and 2) it is awkward when one wishes to introduce parametric polymorphism (an issue we will not discuss at depth in this paper).} For example, the signature on line \ref{log-transfer-sig} of Listing \ref{example} specifies that the argument \lip{t} of \lip{log_transfer} must have type \lip{Transfer}. 

% The return type of the function is not specified here, so it will be inferred, as we will describe below. For clarity and in cases where it cannot be inferred, such as when writing a recursive function, the return type can be specified explicitly by a type signature of the following form:
%  \begin{lstlisting}[numbers=none]
%  {t : Transfer} >> unit
%  \end{lstlisting}
% Typed functions with no arguments and a return type that can be inferred, like \lip{__main__}, can omit the type signature if desired.

% As in languages like ML and Haskell, typed functions in \verb|typy| are themselves values of a function type. This type is constructed by applying the tycon used as the decorator to a tuple consisting of the argument and return types. In this example, \lip{log_transfer} has type \lip{fn[Transfer, unit]}. This function type is analagous to function types in ML and Haskell, e.g. \lstinline{Transfer -> unit}.\footnote{As of this writing, we have not decided whether to privilege \lip{typy.std.fn} with a monopoly over comparable infix syntax, e.g. \lip{Transfer >> unit}.}

% \subsection{Active Typechecking}\label{s:atc}
% Let us now trace through the process of typechecking the body of the function \lip{__main__}, which constructs a value of type \lip{Account} and then logs two \lip{Transfer}s by calling \lip{log_transfer}. In Sec. \ref{s:atr} we cover the subsequent active translation process. These two processes together constitute AT+T.

% \paragraph{Function Bodies}

% \begin{codelisting}[t]
%   % ^\label{normalize_idices_start}^@classmethod
%   % def normalize_idx(cls, idx):
%   %   # ... ensure that idx is a tuple of types ...
%   % @classmethod
%   % def normalize_inc_idx(cls, inc_idx):
%   %   # ... ensure that idx is a tuple of types, 
%   %   #     except return type ...^\label{normalize_idices_end}^
% \begin{lstlisting}
% class fn(typy.Type):
%   ^\label{syn_idx_FunctionDef}^@classmethod
%   def syn_idx_FunctionDef(cls, ctx, inc_idx, node):
%     if not hasattr(ctx, 'assn_ctx'): 
%       ctx.assn_ctx = { } # init. assignables context
%     # add function arguments to assn_ctx
%     ctx.assn_ctx.update(zip(node.args, inc_idx[:-1]))
%     # check each statement in body (post-type sig.)
%     for stmt in node.proper_body: ctx.check(stmt)
%     # synthesize return type from last stmt
%     last_stmt = node.proper_body[-1]
%     if isinstance(last_stmt, ast.Expr): 
%       rty = last_stmt.value.ty
%     else: rty = unit
%     # generate complete idx
%     return inc_idx[:-1] + (rty,)

%   ^\label{ana_FunctionDef}^def ana_FunctionDef(self, ctx, node):
%     arg_tys, rty = self.idx[0:-1], self.idx[-1]
%     if not hasattr(ctx, 'assn_ctx'): 
%       ctx.assn_ctx = { } # init. assignables context
%     # add fn name (for recursion) + args to assn_ctx
%     ctx.assn_ctx[node.name] = self
%     ctx.assn_ctx.update(zip(node.args, arg_tys))
%     # check all but last statement in body
%     for stmt in node.proper_body[0:-1]: ctx.check(stmt)
%     last_stmt = node.proper_body[-1]
%     # for last statement, if expr, analyze against rty
%     if isinstance(last_stmt, ast.Expr): 
%       ctx.ana(last_stmt.value, rty)
%     # otherwise, rty must be unit or dyn
%     elif rty == unit or rty == dyn: 
%       ctx.check(last_stmt)
%     else: raise typy.TypeError("<msg>", last_stmt)

%   @classmethod
%   def check_Assign_Name(cls, ctx, stmt):
%     x, e = stmt.target.id, stmt.value # cf. ast docs
%     if x in ctx.assn_ctx: ctx.ana(e, ctx.assn_ctx[x])
%     else: ctx.assn_ctx[x] = ctx.syn(e)

%   @classmethod
%   def check_Expr(cls, ctx, stmt):
%     ctx.syn(stmt.value)
    
%   @classmethod
%   ^\label{syn_Name}^def syn_Name(cls, ctx, e):
%     try: return ctx.assn_ctx[e.id]
%     except KeyError:
%       try: return ctx.syn_Lift(ctx.static_env[e.id])
%       except KeyError: raise typy.TypeError("...", e)
  
%   ^\label{default_asc}^@classmethod
%   def syn_default_asc_Str(cls, ctx, e): return string
% \end{lstlisting}
% \caption{A portion of the type constructor \texttt{typy.std.fn}.}
% \label{fn-top}
% \end{codelisting}
% After processing the docstring and type signature, if provided, \verb|typy| immediately hands control over the function body to the tycon provided as the decorator, i.e. \lip{fn} in this example. This happens by one of two methods, depending on whether the return type must be inferred.

% If the return type must be inferred, as in Listing \ref{example}, the semantics invokes the class method \lip{syn_idx_FunctionDef}, seen on line \ref{syn_idx_FunctionDef} of Listing \ref{fn-top}, passing it the \emph{semantic context} (an object that provides hooks into the semantics and a storage location for accumulating information during typechecking), the \emph{incomplete type index} (here, the pair \lip{(Transfer, Ellipsis)}, where \lip{Ellipsis} stands in for the unknown return type), and the syntax tree of the function definition. This method must return the complete type index (i.e. one without an \lip{Ellipsis}). The method in Listing \ref{fn-top} proceeds as follows: \begin{enumerate}
% \item It initializes an \emph{assignables context}, which is a dictionary that  tracks assignable variables and their types. It is stored in the semantic context. Its first entries are the function arguments and their corresponding types, extracted from the incomplete type index (by dropping the last element, written \lip{inc_idx[:-1]}).
% \item It asks \lip{typy}, by invoking the method \lip{ctx.check}, to typecheck each statement in the body (discussed below).
% \item If the abstract form of the final statement is \lip{ast.Expr} (i.e. a top-level expression), then its type is used as the return type (typechecking of expressions is also discussed below). Otherwise it is \lip{typy.std.unit}. Note that this choice of using the last expression as the implicit return value (and considering any statement of a form other than \lip{Expr} to have a trivial value) is made by \verb|fn|, not by \lip{typy}'s semantics.\footnote{The actual implementation of \texttt{fn} supports return statements as well, but we omit this logic here for simplicity.} %In this case, the return type is \lip{unit} because invocations of the \lip{printf} operator, also defined in \lip{typy.std}, have type \lip{unit} (i.e. \lip{printf} is used only for its effect). We will not cover its details here, but note that the format string can be typechecked statically.
% \item Having determined the return type, the complete type index is  generated and returned (by dropping the \lip{Ellipsis} and concatenating a 1-tuple of the return type). \verb|typy| will then use this to construct the function's type as described above.
% \end{enumerate}% A different function type constructor could be written to support \lip{return} statements (omitted for concision).% The choices is made by clients .

% Had the type signature specified a return type explicitly, then the type index would not need to be inferred. In this case, \lip{typy} constructs the complete type first, i.e. \lip{fn[Transfer, unit]}, then invokes its instance method \lip{ana_FunctionDef}, seen on line \ref{ana_FunctionDef} of Listing \ref{fn-tycon}. This method proceeds essentially as above, but has access to the complete index as \lip{self.idx}. Thus, if the last statement in the function is an expression, it is analyzed against the provided return type, rather than asked to synthesize a type (see below). %Functions that don't end in expressions can only have return types \lip{unit} or \lip{dyn} (again, a choice made by \verb|fn|, not the semantics).%. If it is, the expression . % Note that in this example, recursive functions are only supported when the ascription is a type, to avoid complications related to synthesizing a type for a recursive function call.
% %On lines \ref{log-transfer-start}-\ref{log-transfer-end}, we see the function \lip{log_transfer}. We follow Python conventions by starting with a documentation string  that informally specifies the behavior of the function (so standard documentation generators can be used). Before moving into the body, however, we  also give an \emph{argument signature} (line \ref{arg-sig}) stating that the type of \lip{t} is \lip{Transfer}, the prototypic object type defined above.

% %This again repurposes Python's array slice syntax to approximate conventional notation for type annotations. % (and following conventions for documentation strings, which also describe ``types'' at the end). % The ``keyword'' \lip{type} was chosen because it is the name of a built-in function in Python (and thus would not be used in the way shown for any other purpose).

% \paragraph{Statements}
% % \begin{codelisting}[t]
% % \begin{lstlisting}
% % #class fn(typy.Type): (cont'd) 
% % \end{lstlisting}
% % \caption{Some forms in the body of a function delegate to the type constructor of the function they are defined within (via class methods during typechecking).}
% % \label{fn-stmts}
% % \end{codelisting}
% The \lip{ctx.check} method called on each statement in the function body by the logic in  Listing \ref{fn-top} is provided by \verb|typy|. As is the pattern, this method delegates to a tycon determined based on the abstract form of the statement being checked. Most statements simply delegate back to the tycon of the function they appear within via a class method named \lip{check_AbsForm}, where \lip{AbsForm} is the name of the statement's abstract form as defined by Python's abstract syntax (or a combination of abstract form names in a few cases where finer distinctions  were necessary).

% For example, the class method \lip{check_Assign_Name}, seen in Listing \ref{fn-tycon}, is invoked for statements of the form \lip{name = expr}, as on line \ref{account-start} of Listing \ref{example}. In this example, the assignables context (the \lip{assn_ctx} field of the semantic context) is consulted to determine whether the name that is being assigned to already has a type due to a prior assignment, in which case the expression is analyzed against that type using \lip{ctx.ana}. If not, the expression must synthesize a type, using \lip{ctx.syn}, and a binding is added to \lip{assn_ctx}. %This interpretation of the assignment form cannot cause the function it appears in to return so \lip{None} is returned (Python returns \lip{None} by default if nothing is explicitly returned) and the subsumption rule applies trivially, so we do not need to define \lip{ana_rty_Assign_Name} separately.

% The class method \lip{check_Expr}, defined on line \ref{check_Expr} of Listing \ref{fn-tycon}, specifies that top-level expressions must synthesize a type.

% There are a number of other statement forms in Python's syntax, but \lip{typy} treats them all similarly, so we omit a detailed discussion of the delegation protocol (it is given in the \lip{typy} documentation).

% %Return statements, on the other, require separately defining return type synthesis and analysis logic. The class method \lip{syn_rty_Return} is called for return statements when a return type is not yet known. It must synthesize a return type based on the value being returned (defaulting to \lip{unit}). Subsequent return statements are processed by \lip{ana_rty_Return}, which checks the provided value against the known return type. If no value is provided, then the return type must be \lip{unit}. Note that this function does not need to return a value; if a \lip{typy.TypeError} is not raised, then analysis succeeds.
% %\todo{more examples?}


% \paragraph{Expressions} The methods \lip{ctx.ana} and \lip{ctx.syn} are also provided by \verb|typy|. As we just saw, \lip{ctx.ana} is invoked by tycons to request type analysis when the expected type of an expression is known. The expression and the expected type are provided as arguments. When the type of an expression must be locally inferred, \lip{ctx.syn} is invoked, taking only the expression as an argument and returning the synthesized type. In both cases, ill-typed expressions raise \lip{typy.TypeError}, which is equipped with an error message and a reference to the location where the error originated.  

% Like \lip{ctx.check}, these \lip{ctx.ana} and \lip{ctx.syn} methods do very little heavy lifting themselves. Instead, they again delegate control to a tycon by invoking a method with a name based on the abstract form of the expression, either \lip{ana_AbsForm} or \lip{syn_AbsForm}. We cover how the delegate is chosen for various forms below.

% Because we distinguish type analysis from type synthesis, the process of active typechecking can be understood as a novel form of \emph{bidirectional typechecking} \cite{Pierce:2000:LTI:345099.345100}. The usual \emph{subsumption principle} applies as follows: if analysis is requested and a \lip{syn_AbsForm} method is defined, then synthesis proceeds and then the synthesized type is checked for equality against the type provided for analysis. Type equality is defined by checking that the two type constructors are equal and that their indices are equal, according to Python's usual protocol. Two types with different tycons are never equal, to keep the burden of ensuring that typing respects type equality local to a single tycon.%  No form of subtyping is supported for the purposes of this paper.

% Let us now discuss how a delegated tycon is chosen for various expression forms in Python's syntax.

% \subparagraph{Literal Forms}
% Python's syntax includes literal forms for strings, numbers, tuples, lists, dictionaries and lambda functions. As we discussed in Sec. 1, we would like to be able to use these forms for values of types other than \lip{dyn}.%\footnote{Proponents of static type systems note that a type is usually understood as a  often take issue with the common use of the phrase ``dynamically typed'' for what is actually a system of dynamic \emph{tags}, rather than a purely ``syntactic discipline'' but we are guests within Python so we will abide by its terminology.}

% To support this, when \lip{typy} encounters an expression of a literal form  in an analytic position, the tycon of the type it is being analyzed against is delegated control. For example, on line \ref{account-start} of Listing \ref{example}, the dictionary literal form appears in an analytic position here because an \emph{explicit type ascription}, written \lip{[: Account]}, was provided. Note that this type ascription form repurposes Python's array slice syntax. Array slices can still be performed, because this form is only treated as an ascription when it contains a valid type. 

% \begin{codelisting}[t]
% \begin{lstlisting}
% class record(typy.Type): 
%   # ... continued from Listing 2 ...

%   def ana_Dict(self, ctx, e):
%     for lbl, value in zip(e.keys, e.values):
%       if isinstance(lbl, ast.Str):
%         if lbl.s in self.idx.rows: 
%           ctx.ana(value, self.idx[lbl.s])
%         else: raise typy.TypeError("<not found>", f)
%       else: raise typy.TypeError("<not a label>", f)
%     if len(self.idx.fields) != len(e.keys): 
%       raise typy.TypeError("<row missing>", e)

%   ^\label{syn_idx_Dict}^@classmethod
%   def syn_idx_Dict(self, ctx, inc_idx, e):
%     if inc_idx != Ellipsis: 
%       raise typy.TypeError("<bad index>", e)
%     return Signature((lbl.s, ctx.syn(value))
%       if isinstance(lbl, ast.Str)
%       else raise typy.TypeError("<bad label>", f)
%       for lbl, value in zip(e.keys, e.values))

%   ^\label{syn_Attribute}^def syn_Attribute(self, ctx, e):
%     if e.attr in self.idx.labels: 
%       return self.idx[e.attr]
%     else: raise typy.TypeError("...", e)
% \end{lstlisting}
% %
% %  def trans_type(self, target):
% %    if target.supports(examples.fp.EagerNAryProdLR):
% %      return target.EagerNAryProdLR()
% %    else if isinstance(target, examples.py.Dyn):
% %      return target.Dyn()
% \caption{A portion of the \texttt{typy.std.record} type constructor.}
% \label{record}
% \end{codelisting}

% Because \lip{Account} is a synonym for a record type, \verb|typy| invokes the \lip{ana_Dict} method shown on line \ref{ana_Dict} of Listing \ref{record}. This method analyzes each row value in the literal expression against the row type specified in the type index. Recall that the type index is an unordered mapping from field names to their types, so that type equality is up to reordering. Various error conditions are shown (in practice, the error messages would be more verbose than shown).

% The string literal forms inside the outermost record form on line \ref{account-start} of Listing \ref{example} do not need an ascription, because \lip{record} requests analysis against the appropriate row type. For example, the value of the field \lip{account_num} delegates to the tycon \lip{string_in} via the \lip{ana_Str} method, shown in Listing \ref{string_in}. This method, consistent with the static semantics we have specified in a recent workshop paper, simply statically checks the string against the regular expression provided as the type index \cite{sanitation-psp14}.

% Similary, the prototypic object literals passed to \lip{log_transfer} on lines \ref{log_transfer-start}-\ref{main-end} of Listing \ref{example} do not need ascriptions because they will be analyzed by the  argument type of \lip{log_transfer}, i.e. \lip{Transfer} (we omit the details of prototypic dispatch for concision; the function application form is discussed below).

% \subparagraph{Incomplete Type Ascription} An ascription placed directly on a literal can also be an \emph{incomplete type}. Incomplete types arise from the application of a tycon to a type index containing an ellipsis (a valid array slice form in Python; the value of an ellipsis is the constant \lip{Ellipsis}). For example, we can write \lstinline[basicstyle=\ttfamily\small]{(e1, e2) [: tuple[...]]}, or equivalently when the index consist solely of an ellipsis, \lstinline[basicstyle=\ttfamily\small]{(e1, e2) [: tuple]}, instead of \lstinline[basicstyle=\ttfamily\small]{(e1, e2) [: tuple[T1, T2]]} when \lstinline[basicstyle=\ttfamily\small]{e1} and \lstinline[basicstyle=\ttfamily\small]{e2} synthesize types \lstinline[basicstyle=\ttfamily\small]{T1} and \lip{T2}, respectively. The tycon is asked to synthesize the complete index via a class method named \lip{syn_idx_AbsForm}, where \lip{AbsForm} is the literal form. 

% Recall that the process for synthesizing the return type of a typed function also involved the use of an \lip{Ellipsis} (perhaps mysterious at the time). Indeed, a typed function definition missing a return type in its type signature is treated exactly like a   function literal ascribed an incomplete function type. This is more explicit when one considers lambda expression literals, e.g. we can write \lip{(lambda x, y: x + y) [: fn[i32, i32, ...]]} when a lambda function is needed in a synthetic position (this is rare in practice; recall that in an analytic position, e.g. when calling a higher-order function, no ascription would be needed at all). 
% More specifically, the default implementation of the \lip{syn_idx_Lambda} calls \lip{syn_idx_FunctionDef}. For \lip{fn}, this method, shown in Listing \ref{fn-tycon}, was discussed earlier.


% Records support partial type ascription as well, e.g.: 
% \vspace{3px}
% \begin{lstlisting}[numbers=none,basicstyle=\ttfamily\scriptsize]
% {"name": "Deepak Dynamic", "job": "Example"} [: record]
% \end{lstlisting}
% The implementation of \lip{syn_idx_Dict} is shown on line \ref{syn_idx_Dict} of Listing \ref{record}. It constructs the record signature by requesting type synthesis for each row value.

% This, however, puts us in a situation where the string literals appearing as row values above are in a synthetic position without an ascription. Here, \lip{typy} delegates control to the tycon of the function that the literal appears in, asking it to provide a ``default ascription'' by calling the class method \lip{syn_default_asc_Str}, shown on line \ref{default_asc} of Listing \ref{fn-tycon}. In this case, we simply return \lip{string}, so the type of the expression above is \lstinline[basicstyle=\ttfamily\small]{record['name' : string, 'age' : string]}. 

% A benefit of using Python as our static language is that it is relatively straightforward to define \lip{fn} instead such that it allows for different defaults using block-scoped ''pragmas'' in the static language via Python's \lip{with} statement \cite{python} (details omitted), e.g. 
% \begin{codelisting}[h]
% \begin{lstlisting}[numbers=none]
% with fn.defaults(Num=i64, Str=string, Dict=record):
%   @fn
%   def test(): {a: 1, b: "no ascriptions needed!"} 
% \end{lstlisting}
% \caption{Block-scoped settings for type constructors.}
% \label{defaultasc}
% \end{codelisting}
% \vspace{-10px}
% %{In Python 3, syntax for annotating functions (but unfortunately not lambdas) with argument and return type annotations directly  (purely as metadata) was introduced \cite{pep3107}:
% %\begin{lstlisting}[numbers=none]
% %@fn
% %def f(x : T) -> T: # ...
% %\end{lstlisting}
% %\vspace{-3px}
% %This can also be thought of as an incomplete type ascription, which we could write more explicitly \lip{@fn[...]},  because the literal itself contains the necessary information to synthesize the full index. We show the more universally available notation here because Python 2.\textit{x} is much more widely adopted as of this writing.

% %\paragraph{Compound Expressions} 
% %For expressions other than names and literals, the type synthesized by one or more subexpressions determines the type constructor delegated responsibility for synthesis and analysis.%The sub-expression that determines this is a function of the term constructor. 

% \subparagraph{Lowercase Names} When the form of an expression is \lip{ast.Name} and the identifier begins with a lowercase character, as when referring to the argument \lip{t} of \lip{log_transfer} or the assignable location \lip{account} in \lip{__main__}, the type constructor governing the function the term appears in is delegated control  via the class method \lip{syn_Name}, i.e. such names must synthesize a type. We see the definition of this method for \lip{fn} starting on line \ref{syn_Name} in Listing \ref{fn-tycon}. A name synthesizes a type if it is either in the assignables context or if it is in the static environment. In the former case, the type that was synthesized when the assignment occurred (by \lip{syn_Assign_Name}) is returned. 

% In the latter case, we must lift the static value to run-time and give it an appropriate type. This cannot always be done automatically (because it is a form of serialization). Instead, any Python value with a \lip{typy_syn_Lift} method returning an \lip{typy} type can provide explicit support for this operation. A few other constructs also have built in support. For the purposes of this section, this includes other typed \texttt{typy} functions (which synthesize their own type), untyped Python functions and classes (which synthesize types \lip{pyfun} and \lip{pyclass}, and thus can only be called with values of type \lip{dyn}), modules (which are given a \emph{singleton type} -- a type indexed by the module reference itself -- from which other values that can be lifted can be accessed as attributes, see below), and immutable Python data structures like numbers, strings and tuples. %In the future, we plan to detail how the interface between Python and \texttt{typy} can be enriched to support moving typed values in and out of untyped Python functions safely (by inserting checks, defined again by a type constructor, at the interface between typed and untyped code), but we do not have enough room to adequately detail this mechanism here (and it is tangential to our specific aims).


% \subparagraph{Targeted Expressions} Expression forms having exactly one sub-expression, like \lip{-e} (term constructor \lip{UnaryOp_USub}) or \lip{e.attr} (term constructor \lip{Attribute}), or where there may be multiple sub-expressions but the left-most one is the only one required, like \lip{e[slices]} (term constructor  \lip{Subscript}) or \lip{e(args)} (term constructor \lip{Call}), are called \emph{targeted expressions}. For these, the compiler first synthesizes a type for the left-most sub-expression, then calls either the \lip{ana_TermCon} or \lip{syn_TermCon} methods on that type. For example, we see type synthesis for the field projection operation on records defined starting on line \ref{syn_Attribute} of Listing \ref{record}.% We omit the implementation for concision, but \lip{fn} defines \lip{syn_Call} as one would expect, by simply checking each argument against the corresponding argument in the index. 

% \subparagraph{Binary Expressions} \begin{codelisting}[t]
% \begin{lstlisting}
% class string_in(typy.Type):
%   def ana_Str(self, ctx, e):
%     _check_rx(e.s, self.idx)

%   @classmethod
%   def syn_idx_Str(cls, ctx, e):
%     return _rx_of_str(e.s) # most specific rx

%   # treats string as string_in[".*"]
%   ^\label{handles_Add_with}^handles_Add_with = set([string]) 
%   ^\label{syn_BinOp_Add}^@classmethod
%   def syn_BinOp_Add(cls, ctx, e): 
%     rlang_left = self._rlang_from_ty(ctx.syn(e.left)) 
%     rlang_right = self._rlang_from_ty(ctx.syn(e.right))
%     return string_in[self._concatenate_langs(
%       rlang_left, rlang_right)]
% \end{lstlisting}
% \caption{Binary operations in \texttt{typy.std.string\_in}.}
% \label{string_in}
% \end{codelisting}
% \begin{codelisting}[t]
% \begin{lstlisting}
% def check_tycon(tycon):
%   for other_tycon in tycon.handles_Add_with: 
%     if tycon in other_tycon.handles_Add_with:
%       raise typy.AmbiguousTyconError("...", 
%         tycon, other_tycon) # (other binops analagous)
% \end{lstlisting}
% \caption{For each type constructor definition and binary operator, \texttt{typy} runs a modular handle set check to preclude ambiguity.}
% \label{check_tycon}
% \end{codelisting}
% The major remaining expression forms are the binary operations, e.g. \lip{e + e} or \lip{e < e}. These are notionally symmetric, so it would not be appropriate to simply give the left-most one precedence. Instead, we begin by attempting to synthesize a type for both subexpressions. If both synthesize a type with the same type constructor, or only one synthesizes a type, that type constructor is delegated responsibility via a class method, e.g. \lip{syn_BinOp_Add} on line \ref{syn_BinOp_Add} of Listing \ref{string_in}. 

% If both synthesize a type but with different type constructors (e.g. we want to concatenate a \lip{string} and a \lip{string_in[r]}), then we consult the \emph{handle sets} associated as a class attribute with each type constructor, e.g. \lip{handles_Add_with} on line \ref{handles_Add_with} of Listing \ref{string_in}. This is a set of other type constructors that the type constructor defining the handle set may potentially support binary operations with. When a type constructor is defined, the language checks that if \lip{tycon2} appears in \lip{tycon1}'s handler set, then \lip{tycon1} does \emph{not} appear in \lip{tycon2}'s handler set. This is a very simple modular analysis (rather than one that can only be performed at ``link-time''), shown in Listing \ref{check_tycon}, that ensures that the type constructor delegated control over each binary expression is deterministic and  unambiguous without arbitrarily preferring one subexpression position over another. This check is performed  by using a metaclass hook.% (technically, this can be disabled; we assume that clients are not importing potentially adversarial extensions in this work, though lifting this assumption raises quite interesting questions that we hope will be addressed by future work).

% \subsection{Active Translation}\label{s:atr}
% Once typechecking is complete, the compiler enters the translation phase. %The compiler creates an instance of a class inheriting from \lip{ace.Target} for use during this phase via the \lip{init_target} method (line \ref{pybase}.54). This object provides methods for code generation and supports features like fresh variable generation, adding imports and so on. It's interface is not constrained by \texttt{typy} (we will see what \texttt{typy} requires of a target below) and the mechanics of code generation are orthogonal to the  focus of this paper, so we will discuss it relatively abstractly. The simplest API would be string-based code generation. For the Python target we use here, we generate ASTs. The API is based directly on the \lip{ast} library, with a few additional conveniences just mentioned.
% This phase follows the same delegation protocol as the typechecking phase. Each \lip{check_}/\lip{syn_}/\lip{ana_TermCon} method has a corresponding \lip{trans_TermCon} method. These are all now instance methods, because all indices have been fully determined. 

% Examples of translation methods for the \lip{fn} and \lip{record} type constructors are shown in Listing \ref{trans}. The output of translation for our example is discussed in the next subsection. Translation methods have access to the context and node, as during typechecking, and must return a translation, which for our purposes, is simply another Python syntax tree (in practice, we offer some additional conveniences beyond \lip{ast}, such as fresh variable generation and lifting of imports and statements inside expressions to appropriate positions, in the module \lip{astx} distributed with the language). Translation methods can assume that the term is well-typed and the typechecking phase saves the type that was delegated control, along with the type that was assigned, as attributes of each term processed by the compiler. Note that not all terms need to have been processed by the compiler if they were otherwise reinterpreted by a type constructor given control over a parent term (e.g. the field names in a record literal are never treated as expressions, while they would be for a dictionary literal). %This protocol can be seen on lines \ref{context}.39-\ref{context}.47. 
% \begin{codelisting}[t]
% \begin{lstlisting}
% #class fn(typy.Type): (cont'd)
%   def trans_FunctionDef(self, ctx, node):
%     x_body = [ctx.trans(stmt) for stmt in node.body]
%     x_fun = astx.copy_with(node, body=x_body)
%     if node.name == "__main__": 
%       x_invoker = ast.parse(
%         'if __name__ == "__main__": __main__()')
%       return ast.Suite([x_fun, x_invoker])
%     else: return x_fun
  
%   def trans_Assign_Name(self, ctx, stmt):
%     return astx.copy_with(stmt, 
%       target=ctx.trans(stmt.target), 
%       value=ctx.trans(stmt.value))
    
%   def trans_Name(self, ctx, e):
%     if e.id in ctx.assn_ctx: return astx.copy(e)
%     else: return ctx.trans_Lift(
%       ctx.static_env[e.id])
    
% #class record(typy.Type): (cont'd)
%   def trans_Dict(self, ctx, e): 
%     if len(self.idx.fields) == 1: 
%       return ctx.trans(e.values[0])
%     ast_dict = dict(zip(e.keys, e.values))
%     return ast.Tuple((f, ctx.trans(ast_dict[f]))
%       for f, ty in self.idx)

%   def trans_Attribute(self, ctx, e):
%     if len(self.idx.fields) == 1: return ctx.trans(e)
%     else: return ast.Subscript(ctx.trans(e.value), 
%       ast.Index(ast.Num(self.idx.position_of(e.attr))))
% \end{lstlisting}
% \caption{Translation methods for the types defined above.}
% \label{trans}
% \end{codelisting}

% %Because we are targeting Python directly, most of our \lip{trans} methods are direct translations (factored out into a helper function). The main methods of interest that are not entirely trivial are \lip{trans_FunctionDef_outer} on lines \ref{pybase}.33-\ref{pybase}-37 and the two translation methods in Listing \ref{record}, which implement the logic described in Sec. \ref{compilation}. 
% %
% %Each type constructor must also specify a \lip{trans_type} method. This is important when targeting a typed language (so that the translations of type annotations can be generated, for example). As we will see in the next two sections, this is also critical to ensuring type safety. The language \emph{checks} not only that translations having a given type are well-typed, but that they have the type specified by this method (requiring that the target provide a \lip{is_of_type} method). Here, because we are simply targeting Python directly, the \lip{trans_type} method on line \ref{record}.49-\ref{record}.50 simply generates \lip{target.dyn}. 
% %
% %When this phase is complete, each node processed by the context will have a translation, available via the \lip{trans} attribute. In particular, each typed function has a translation. Note that some nodes are never processed by the context because they were reinterpreted by the delegate (e.g. the field names in a record literal), so they do not have translations (as expected, given our discussion above).
% %
% %To support external compilation, the target must have an \lip{emit} method that takes the compilation script's file name and a reference to a string generator (an instance of \lip{ace.util.CG}) and emits source code. The string generator we provide can track indentation levels (to support Python code generation and make generation for other languages more readable, for the purposes of debugging). It allows non-local string generation via the concept of user-defined \emph{locations}.  Each file that needs to be generated is a location and there can also be locations within a file (e.g. the imports vs. the top-level code), specified by a target the first time it finds that a necessary location is not defined. A generated entity (e.g. an import, class definition or function definition) can only be added once at a location. We saw this in Listing \ref{example-out} for the decimal-to-string conversion. The API will be discussed further in the appendix. 


% \subsection{Standalone Compilation}\label{compilation} 
% \begin{codelisting}[t]
% \begin{lstlisting}[style=Bash]
% $ `@ listing1.py`
% Hello, compile-time world!
% Goodbye, compile-time world!
% [@] _atout_listing1.py successfully generated.
% $ `python _atout_listing1.py`
% Transferring 5.50 to Annie Ace.
% Transferring 15.00 to Annie Ace.
% \end{lstlisting}
% \caption{Compiling \texttt{listing\ref{example}.py} using the \texttt{@} script.}
% \label{external-compilation}
% \end{codelisting}
% \begin{codelisting}[t]
% \begin{lstlisting}
% import typy.std.runtime as _at_i0_

% def log_transfer(t):
%   _at_i0_.print("Transferring %s to %s." % 
%     (_at_i0_.dec_to_str(t[0][0], 2), t[1][0]))

% def __main__():
%   account = ("Annie Ace", "00-00000001")
%   log_transfer((((5, 50), None), account))
%   log_transfer((((15, 0), "Rent payment"), account))
% if __name__ == "__main__": __main__()
% \end{lstlisting}
% \caption{[\texttt{\_atout\_listing\ref{example}.py}] The file generated in Listing \ref{external-compilation}.}
% \label{example-out}
% \end{codelisting}


% Listing \ref{external-compilation} shows how to invoke the \lip{@} compiler at the shell to typecheck and translate then execute \lip{listing1.py}. %Note that the {\texttt{print}} statements at the top  of the  compilation script were evaluated during compilation only. %These two steps can be combined by running \lip{ace listing1.py} (the intermediate file is not generated unless explicitly requested in this case).
% The \texttt{typy} compiler is itself a Python library and \lip{@} is a simple Python script that invokes it in two steps:
% \begin{enumerate}
% \item It evaluates the compilation script to completion.
% \item For any top-level bindings in the environment that are \texttt{typy} functions, it initiates typehecking and translation as described above. If no type errors are discovered, the ordered set of translations are collected (obeying order dependencies) and emitted. If a type error is discovered, an error is displayed.
% \end{enumerate}

% In our example, there are no type errors, so the file shown in Listing \ref{example-out}  is generated. This file is meant only to be executed, not edited or imported directly. The invariants necessary to ensure that execution does not ``go wrong'', assuming the extensions were implemented correctly, were checked statically and entities having no bearing on execution, like field names and types themselves, were erased. The dynamic semantics of the type constructors used in the program were implemented by translation to Python:% In particular, note: 
% \begin{enumerate}
% \item \lip{fn} recognized the function name \lip{__main__} as special, inserting the standard Python code to invoke it if the file is run directly.
% \item Records translated to tuples (the field names were not needed).
% \item Decimals translated to pairs of integers. String conversion is defined in a ``runtime'' package with a ``fresh'' name.
% \item Terms of type \lip{string_in[r"..."]} translated to strings. Checks were all performed  statically in this example.% Note that there are situations (e.g. if a string is read in from the console) that would necessitate an initial run-time check, but no further checks in downstream functions.
% \item Prototypic objects translated to pairs consisting of the fore and the prototype. Dispatch to the appropriate record based on the field name was static (line 5).
% \end{enumerate}


% \paragraph{Type Errors} Listing \ref{oops} shows an example of code containing several type errors. If analagous code were written in Python itself, these could only be found if the code was executed on the first day of the month (and not all of the issues would  immediately be caught as run-time exceptions). In \texttt{typy}, these can be found without executing the code (i.e. we have true static typechecking).

% \begin{codelisting}[t]
% \begin{lstlisting}
% from listing1 import fn, dyn, Account, Details, 
%   log_transfer
% import datetime
% @fn[dyn, dyn]
% def pay_oopsie(memo):
%   if datetime.date.today().day == 1: # @lang to Python
% ^\label{error-start}^    account = {nome: "Oopsie Daisy", 
%       account_num: "0-00000000"} [:Account] # (format)
%     details = {amount: None, memo: memo} [:Details]
%     details.amount = 10 # (immutable)
%     log_transfer((account, details)) # (backwards) ^\label{error-end}^
% print "Today is day " + str(datetime.date.today())
% pay_oopsie("Hope this works..") # Python to @lang
% print "All done."
% \end{lstlisting}
% \caption{[\texttt{listing\ref{oops}.py}] Lines \ref{error-start}-\ref{error-end} each have a type error.}
% \label{oops}
% \end{codelisting}
% \begin{codelisting}[t]
% \begin{lstlisting}[style=Bash]
% $ `python listing^\color{blue}\ref{oops}^.py`
% Today is day 2
% Traceback (most recent call last):
%   File "listing^\ref{oops}^", line 9, in <module>
% typy.TypeError: 
%     File "listing^\ref{oops}^.py", line ^\ref{error-start}^, col 12, in <module>: 
%       [record] Invalid field name: nome
% \end{lstlisting}
% \caption{Execution never proceeds into a function with a type error when using \texttt{typy} for implicit compilation.}
% \label{oops-exec}
% \end{codelisting}

% \subsection{Interactive Invocation}\label{interactive} \texttt{typy} functions over values of type \lip{dyn} can be invoked directly from Python. Typechecking and translation occurs upon first invocation (subsequent calls are cached; we assume that the static environment is immutable). We see this occurring when we execute the code in Listing \ref{oops} using the Python interpreter in Listing \ref{oops-exec}. 

% If a function captures any static values that do not support lifting, then that function can only be used via interactive invocation (there is no way to separate compile-time from run-time without lifting captured values). Such values when captured in an interactively invoked setting by default synthesize type \lip{dyn}, though they can provide a \lip{typy_syn_Capture} method that synthesizes a more specific type. We will see an example of this and a related use of phaseless capture in our discussion of \lip{typy.std.opencl} below.

% %\todo{move to future work section?}In future work, we plan to detail how to insert dynamic checks and wrapper objects, defined by active type constructors in a manner similar to how static checks are defined here, so that values that are not of type \lip{dyn} can be passed in and out of untyped Python code. Because these can be seen as implicit coercions to and from \lip{dyn}, and we do not aim to introduce this feature in this paper, we omit discussion of this topic. Explicit coercions can be implemented using the mechanisms described above. For example, \lip{string_in} provides an introductory form that checks a provided \lip{string}  or \lip{dyn} value dynamically, raising an exception in the case of failure: \lip{[raw_input()] : string_in[r"\\d+"]}.

% %.; for example, the unexpected use of \lip{None}\footnote{The notation \texttt{+T} constructs a \texttt{None}-able option type; see appendix.}). Static type checking allows us to find these errors during compilation. Listing \ref{oops-compilation} shows the result of attempting to compile this code. The compilation script completes (so that functions can refer to each other mutually recursively), then the typed functions in the top-level environment are typechecked. The typechecker raises an exception statically at the first error, and \lip{acec} prints it to the console as shown.

% %When encountering a compound term (e.g. \lip{t.amount} in Listing \ref{example}), the compiler defers control over type  and translation to the type of a designated subexpression (e.g. \lip{t}) according to a fixed fixed \emph{dispatch protocol}. Below are examples of the choices made in \texttt{typy}.\todo{full in appendix?}
% %\begin{itemize}
% %\item Responsibility over {\bf attribute access} (\texttt{e.attr}), {\bf subscripting} (\texttt{e[e1]}) and \textbf{calls} (\lip{e(e1, ..., en)}) and {\bf unary operations} (e.g. \lip{-e}) is handed to the type recursively assigned to \texttt{e}.
% %\item Responsibility over {\bf binary operations} (e.g. \lip{e1 + e2}) is first handed to the type assigned to the left operand. If it indicates that it cannot handle the operation, the type assigned to the right operand is handed responsibility. {Note that this operates like the corresponding rule in Python's \emph{dynamic} operator overloading mechanism; see Sec. \ref{related} for a discussion.}
% %\item Responsibility over \textbf{constructor calls} (\lip{[t](e1, ..., en)}), where \lip{t} is a \emph{compile-time Python expression} evaluating to an active type, is handed to that type. If \lip{t} evaluates to a \emph{family} of types, like \lip{clx.Cplx}, the active type is first generated via a class method, as discussed below.
% %%\item Responsibility over {\bf simple assignment statements}, is handed to the type of the variable on the left (which, as we will see below, is determined by the active base). If this type does not provide special assignment semantics, the base must handle it.%Destructuring assignment is also supported by a somewhat more complex protocol.\todo{clarify}
% %\end{itemize}
% %%
% %%\todo{To be revised from here on down}The core of \texttt{typy} consists of about 1500 lines of Python code implementing its primary concepts: generic functions, concrete functions, active types, active bases and active targets.  The latter three comprise \texttt{typy}'s extension mechanism. Extensions provide semantics to, and govern the compilation of, \texttt{typy} functions, rather than logic in \texttt{typy}'s core. %Indeed, the name ``\texttt{typy}'' might itself be an acronym: it is an ``active compilation environment''.


% % As a result, execution is essentially as fast as it could be given the target language chosen (Python). We will target an even faster language, \lip{OpenCL}, in Sec. \ref{targets}.

% %Because \lip{t} will be statically guaranteed to have type \lip{Transfer},  


% \section{More Examples}\label{examples}
% In the previous section, we showed examples of several interesting type system fragments implemented as libraries using \lip{typy}, including functional record types, a prototypic object system and regular string types. A more detailed description of regular string types and their implementation using \lip{typy} was recently published at a workshop \cite{sanitation-psp14}. Here, we showcase two more powerful examples that demonstrate the expressive power of the system: functional datatypes with nested pattern matching, and a type safe foreign function interface to the complete OpenCL language for many-core programming on devices like GPUs.

% \subsection{Functional Datatypes and Nested Pattern Matching}
% \begin{codelisting}[t]
% \begin{lstlisting}
% from typy import ty
% from typy.std import case, casetype, tuple, fn

% @ty
% def Tree(a): casetype[
%   "Empty",
%   "Leaf" : a,
%   "Node": tuple[Tree(a), Tree(a)]
% ]

% @fn # using Python 3 type annotation syntax
% def treesum(tree : Tree(dyn)) -> dyn:
%   case(tree) [
%     Empty: 0,
%     Leaf(x): x,
%     Node((l, r)): treesum(l) + treesum(r)
%   ]

% @fn
% def __main__():
%   treesum(Tree.Node(Tree.Leaf(5), Tree.Leaf(5)))
% \end{lstlisting}
% \caption{An example of case types and nested pattern matching.}
% \label{example-casetypes}
% \end{codelisting}
% \begin{codelisting}[t]
% \begin{lstlisting}
% def treesum(tree):
%   return (0 if tree[0] == 0 else 
%     tree[1] if tree[0] == 1 else 
%     treesum(tree[1][0]) + treesum(tree[1][1]))

% def __main__():
%   treesum((2, ((1, 5), (1, 5))))
% \end{lstlisting}
% \caption{The translation of Listing \ref{example-casetypes}. Case types are implemented as fast tagged values.}
% \label{example-casetypes-out}
% \end{codelisting}
% \begin{codelisting}[t]
% \begin{lstlisting}
% class _case(object):
%   def syn_Lift(self):
%     return caseop[()]
% case = _case()

% class caseop(typy.Type):
%   def syn_Call(self, ctx, e):
%     ctx.syn(e.args[0])
%     if len(e.args) != 0:
%       raise typy.TypeError("...", e)
%     return caseop_applied[()]

% class caseop_applied(typy.Type):
%   def syn_Subscript(self, ctx, e):
%     scrutinee = e.value.args[0]
%     rules = _rules_from(e.slices)
%     ty = None
%     for rule in rules:
%       bindings = bindings_from_pat(rule, scrutinee.ty)
%       _push_bindings(ctx, bindings)
%       cur_type = ctx.syn(rule.branch)
%       _pop_bindings(ctx, bindings)
%       if ty is not None and cur_type != ty:
%         raise typy.TypeError("inconsistent", rule)
%       else: ty = cur_type
%     return ty

%   def trans_Subscript(self, ctx, e):
%     scrutinee = e.value.args[0]
%     return scrutinee.ty.trans_pats(
%       _rules_from(e.slices))

% class casetype(typy.Type):
%   # ...
%   def type_pat_Call(self, ctx, p):
%     if p.func in self.idx: 
%       return self.idx[p.func]

%   def trans_pats(self, ctx, pats):
%     # ... turns sequence of patterns at same level 
%     # into a conditional expr...

% class tuple(typy.Type):
%   # ...
%   def type_pat_Tuple(self, ctx, p):
%     if len(p.elts) == len(self.idx):
%       return self.idx
%     else raise PatError("invalid pat length") 
% \end{lstlisting}
% \caption{The implementation of the case analysis operator uses intermediate type constructors that contain only typing logic but no translation logic. It also defines its own ``second-order'' extensibility mechanism.}
% \label{example-casetypes-impl}
% \end{codelisting}
% A powerful feature of modern functional languages like ML is support for nested pattern matching over datatypes (i.e. recursive labeled sum types), tuples, records and other types. For example, tree structures are well-modeled using recursive labeled sums, as we show in Listing \ref{example-casetypes}. Here, \lip{Tree} is a \emph{case type}, which is what we call labeled sums in \lip{typy.std}. \lip{Tree} is declared as a \emph{named type} using the \lip{@ty} annotation. This is a general mechanism in \lip{typy} for supporting recursive types that behave generatively, i.e. that are identified by a name. Functional datatypes in ML and similar languages are also generative in this way. The \lip{@ty} mechanism also supports parameterized types: here, \lip{a} stands in for any other type. The type index to \lip{casetype} is a series of cases with, optionally, associated types. Here, a (binary) tree can either be empty (no associated data), a leaf with an associated value of type \lip{a}, or an internal node, which takes a pair of trees as data. We use the \lip{typy.std.tuple} type constructor to represent pairs.

% The function \lip{treesum} on lines 11-17 takes a tree containing \lip{dyn} values and computes the sum of these values by nested pattern matching. The \lip{__main__} function constructs a tree and calls \lip{treesum} on it. The result of compiling Listing \ref{example-casetypes} is shown in Listing \ref{example-casetypes-out}. Case types translate to pairs consisting of a numeric tag and data, and case analysis translates to nested conditionals.

% Some relevant portions of the implementation of case analysis are shown in Listing \ref{example-casetypes-impl}. The object \lip{case} is an instance of the class \lip{_case}, which defines the method \lip{syn_Lift}. When the imported name \lip{case} is encountered in Listing \ref{example-casetypes}, this is called, per the discussion earlier on names in the static environment. The result is that \lip{case} synthesizes type \lip{caseop[()]}.

% The only purpose of the type constructor \lip{caseop} is to define the \lip{syn_Call} method, which ensures that the case operator is applied to a single well-typed scrutinee. The combined term \lip{case(tree)} then synthesizes type \lip{caseop_applied[()]}. Notice that neither method is paired with a \lip{trans_} method -- these types are merely ``bookkeeping'' for the compound case analysis form. Parts of this form are not meaningful by themselves, so translation would fail if, for example, \lip{case(tree)} was written by itself without cases.

% The \lip{caseop_applied} tycon defines the \lip{syn_Subscript} method, which actually processes the list of provided rules. First, the type of the scrutinee is asked to process the bindings from the pattern portion of each rule. This involves calling methods of the form \lip{type_pat_TermCon}, which must return types for all the sub-patterns. For example, the \lip{type_pat_Call} method of \lip{casetype} makes sure that the case named is defined, and if so returns the associated type, taken from its type index, as the type of the inner pattern. The process repeats recursively, so that a tuple pattern might then be processed by \lip{type_pat_Tuple}. Any type constructor can participate in this protocol by defining appropriate methods, so it is extensible. Note, however, that this is a ``second-order'' extensibility mechanism -- nothing in \verb|typy| itself supports pattern matching. Instead, \lip{case} itself defined this protocol in a library, demonstrating the power of using a flexible general-purpose language for writing extensions.

% The translation phase begins at the \lip{trans_Subscript} method of \lip{caseop_applied}. This simply hands responsibility for translating the entire set of rules to the type of the scrutinee. This may then hand control to the types of inner patterns (analagous to the typing phase, not shown). 

% By combining the functional behavior of the \lip{fn} type constructor, which dispenses with ``return'' statements, and creatively repurposing existing syntax, we are thus able to implement an essentially idiomatic statically-typed functional language, entirely as a library for Python. The constructs we defined can be composed arbitrarily with, e.g. the record types, object types or regular string types in the previous section, or the typed OpenCL FFI we will describe next, without any concern regarding syntactic or semantic conflicts. This is enabled entirely by taking a type constructor oriented view toward the extensibility problem. 

% \subsection{A Low-Level Foreign Function Interface to OpenCL} 

% \begin{codelisting}[t]
% \begin{lstlisting}
% import typy.std.opencl as opencl
% import numpy

% device = opencl.init_device(0)
% buffer = device.send(numpy.zeros((800000,))

% @opencl.template_fn
% def map(b, f):
%   gid = get_global_id(0) # data parallel thread ID
%   b[gid] = f(b[gid])

% @opencl.template_fn
% def shift(x):
%   return x + 5

% @opencl.template_fn
% def triple(x):
%   return 3 * x

% map(buffer, triple)
% map(buffer, shift)
% \end{lstlisting}
% \caption{An example use of our typed FFI to OpenCL, demonstrating both template functions and phaseless capture.}
% \label{example-opencl}
% \end{codelisting}

% \begin{codelisting}[t]
% \begin{lstlisting}[style=OpenCL]
% #pragma OPENCL EXTENSION cl_khr_fp64 : enable

% double shift(double x) {
%   return x + 5.0;
% }

% double triple(double x) {
%   return 3.0 * x;
% }

% kernel void map__0_(global double* b) {
%   size_t gid = get_global_id(0); 
%   b[gid] = triple(b[gid])
% }

% kernel void map__1_(global double* b) {
%   size_t gid = get_global_id(0);
%   b[gid] = shift(b[gid])
% }
% \end{lstlisting}
% \caption{The underlying code generated by \texttt{typy.std.opencl} as a string passed through \texttt{pyopencl}.}
% \label{example-opencl-out}
% \end{codelisting}
% \begin{codelisting}[t]
% \begin{lstlisting}
% class template_fn(typy.Type):
%   # ...
%   def syn_Call(self, ctx, e):
%     arg_types = [ctx.syn(arg) for arg in e.args]
%     return fn[fn.syn_idx_FunctionDef(ctx, self.idx.fn, 
%       (arg_types, Ellipsis))]

%   def trans_Call(self, ctx, e):
%     return _wrap_in_pyopencl(
%       e.ty.trans_FunctionDef(ctx, self.idx.fn))
% \end{lstlisting}
% \caption{A portion of the logic of OpenCL template functions, showing how they defer to the logic for standard OpenCL functions at each call site, rather than at the declaration site.}
% \label{example-opencl-templatefn}
% \end{codelisting}
% \begin{codelisting}[t]
% \begin{lstlisting}
% class Buffer(pyopencl.Buffer):
%   # ...
%   def typy_syn_Capture(self):
%     return global_ptr[
%       _np_dtypes_to_cltypes(self.dtype)]
% \end{lstlisting}
% \caption{The \texttt{opencl.Buffer} class represents OpenCL memory objects in global device memory, inheriting from the \texttt{pyopencl.Buffer} class. These supports phaseless capture at the corresponding global pointer type.}
% \label{example-opencl-buffer}
% \end{codelisting}

% Python is a common language in scientific computing and data analysis domains. The performance of large-scale analyses can be a bottleneck, so programmers often wish to write and interact with code written in a low-level language. OpenCL is designed precisely for this workflow, exposing a C-based data parallel language that can compile to a variety of specialized hardware (e.g. GPUs) via a standard API, as well as a runtime that allows a program to manage device memory. The \lip{pyopencl} package exposes these APIs to Python code and integrates them with the popular \lip{numpy} numeric array package \cite{klockner2011pycuda}. To compile an OpenCL function, however, users must write it as a string. This is neither idiomatic nor safe, because it defers typechecking to invocation-time. OpenCL is also often too low-level, not supporting even features found in other low-level languages like function overloading, function templates and higher-order functions.

% In Listing \ref{example-opencl}, we show usage of the \lip{typy.std.opencl} package, which implements the entirety of the OpenCL type system (which includes essentially the entirety of C99, plus some extensions to work with multiple memory spaces). Although the details are beyond the scope of this paper, we note that the mapping onto Python syntax was straightforward, particularly given the flexibility of analytic numeric literal forms, as described above, to support the wide variety of number types in C99-based languages. 

% After initializing a device, line 4 sends a \lip{numpy} array to the device, assigning \lip{buffer} its handle, an object inheriting from \lip{opencl.Buffer}. The type of the \lip{numpy} array is also extracted and stored in this object.

% On lines 7-18, we define three OpenCL template function. Let us consider the first, \lip{map}, which takes two arguments. A template function is one that does not have specified argument types, generally because many types might be valid, as in this case. The body of \lip{map} would be valid for any combination of types for \lip{b} and \lip{f} that support subscripting and calls as shown. Template functions thus defer typechecking to every invocation site, based on the types synthesized by the provided arguments. This is analagous to template functions in C++, but here we implicitly assume a distinct template parameter for each argument and implicitly instantiate these parameters at use sites. Technically, such functions have a singleton type, indexed by the function definition itself, as shown in Listing \ref{example-opencl-templatefn}. Note that  OpenCL itself doesn't have template functions; we are implementing them, essentially by extending the language across its FFI. 

% We see two applications of \lip{map} on lines 20 and 21. Note that these are in the dynamically typed portion of the code. By default, the first argument in each case, \lip{buffer}, would synthesize type \lip{dyn} and typechecking would fail (at line 10, technically, because \lip{dyn} doesn't support subscripting with the type that \lip{gid} synthesizes).  But we make use of the support for phaseless capture discussed in Sec. \ref{interactive}, which allows values to define a method, \lip{typy_syn_Capture}, that permits capture and, in this case, function application with values that would not otherwise support crossing into \lip{typy} functions with argument types other than \lip{dyn}. In this case, \lip{Buffer} objects synthesize a corresponding pointer type, represented by the tycon \lip{opencl.global_ptr} and indexed by the target type of the pointer, here \lip{opencl.double} because the initial \lip{numpy} array was an array of \lip{double}s and there is a mapping from \lip{numpy} data types to OpenCL types. The implementation is shown in Listing \ref{example-opencl-buffer}. Note that phaseless capture is distinct from lifting -- lifting requires a value persist from compile-time to run-time, but OpenCL buffers are clearly ephemeral. Phaseless capture can only occur when interactive invocation is being used.

% Each invocation of \lip{map} received the same buffer, but a different function. Because these were template functions, their types were distinct. As such, two different versions of map were generated, as shown in Listing \ref{example-opencl-out}. Indeed, without support for function pointers in OpenCL, code generation like this is the standard way of supporting ``higher-order'' functions like \lip{map}. Rather than having to do this manually, however, our type system handled this internally. 

% Though we do not show any other extensions atop the OpenCL library here, it is straightforward to implement variants of those described above that target their translation phase to OpenCL rather than Python. In many cases, the typechecking code can be inherited directly. This is, we argue, rather compelling: a decidedly low-level language like OpenCL can be extended with low-overhead versions of sophisticated features, like a prototypic object system or case types, modularly, via its foreign function interface from a scripting language like Python.

% \section{Active Type Constructors, Minimally}\label{theory}
% \newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
% \newcommand{\tri}{i}
% \newcommand{\trt}{\cev\tau}
% \renewcommand{\tnil}[1]{[]}
% \newcommand{\ltxt}[1]{\ell_{\text{#1}}}
% \newcommand{\abslbl}[1]{\FF{lbl}[{\texttt{#1}}]}
% \begin{figure*}[t!]
% \small
% $
% \begin{array}{ll}
% \F{import} \Phi_\text{nat}, \Phi_\text{lprod} & \FF{import}[\Phi_\text{nat}, \Phi_\text{lprod}](\\
% \F{static~let} \tvar{nat} = \fvar{nat}\texttt{\textbf{[}}\FF{nil}\texttt{\textbf{]}} & \FF{slet}[\FF{ty}[\fvar{nat}](\FF{nil})](\tvar{nat}.\\
% \F{let} zero = \texttt{0} : \tvar{nat} & \FF{let}(\FF{asc}[\tvar{nat}](\FF{lit}[\FF{lbl}[\texttt{0}]](\cdot)); zero.\\
% \F{let} one = zero.{\texttt{s}} & \FF{let}(\FF{targ}[\abslbl{s}](zero; \cdot); one.\\
% \F{let} plus = (\lambda x{:}\tvar{nat}. \lambda y{:}\tvar{nat}. &  \FF{let}(\FF{asc}[\FF{incty}[\fvar{fn}](\tvar{nat})](\lambda(x.\FF{asc}[\FF{incty}[\fvar{fn}](\tvar{nat})](\lambda(y.\\
% \quad x\cdot{\texttt{rec}}(y; \lambda p.\lambda r. r\cdot{\texttt{s}})) & \quad \FF{targ}[\abslbl{rec}](x; \lambda(p.\lambda(r.\FF{targ}[\abslbl{s}](r; \cdot)))))))); plus.\\
% \F{let} two = plus~one~one & \FF{let}(\FF{targ}[\FF{nil}](\FF{targ}[\FF{nil}](plus; one); one); two.\\
% \{{\texttt{one}} {=} one; {\texttt{two}} {=} two\} : \fvar{lprod}\texttt{[}\cdots\texttt{]}~~~~~~& \FF{asc}[\FF{incty}[\fvar{lprod}](\FF{nil})](\FF{lit}[\FF{cons}(\abslbl{one}; \FF{cons}(\abslbl{two}; \FF{nil}))](one; two)))))))\\
% % & \quad \\
% %\quad\quad \\
% %\quad\quad\quad & \quad\quad\quad \\
% %\{{\texttt{one}} {=} one; {\texttt{two}} {=} plus(one)(one)\} : \fvar{lprod} ~~~~& \FF{asc}[\FF{incty}[\fvar{lprod}](\FF{nil})](\\&
% %\quad 
% %one; \\&
% %\quad \FF{targ}[\FF{nil}](\FF{targ}[\FF{nil}](plus; one); one))))))))
% \end{array}
% $~~
% %$\begin{array}{rcl}
% %\fvar{tycon}\texttt{[}\sigma\texttt{]} & := & \FF{ty}[\fvar{tycon}](\sigma)\\
% %\fvar{tycon}\texttt{[}\cdots\sigma\cdots\texttt{]} & := & \FF{incty}[\fvar{tycon}](\sigma)\end{array}$
% %~~~~~~~~$
% %\begin{array}{rcl}
% %\fvar{tycon} & := & \FF{incty}[\fvar{tycon}](\FF{nil})\\
% %\fvar{tycon}\texttt{[}\cdots\texttt{]} & := & \FF{incty}[\fvar{tycon}](\FF{nil})\\
% %\end{array}$\\
% %~\\
% %\hspace{-50px}$
% %\tiny
% %\begin{array}{rcl}
% %\lambda x{:}\sigma.e & := & \FF{asc}[\FF{incty}[\fvar{fn}](\sigma)](\lambda(x.e))\\
% %(e_1, ..., e_n) & := & \FF{lit}[\FF{nil}](e_1; \ldots; e_n)\\
% %\{{\texttt{lbl}}_1=e_1, \ldots, {\texttt{lbl}}_n=e_n\} & := & \FF{lit}[\FF{cons}(\FF{lbl}[{\texttt{lbl}}_1]; \ldots\FF{cons}(\FF{lbl}[{\texttt{lbl}}_n]; \FF{nil}))](e_1; \ldots; e_n)\\
% %\text{(any numeric label)}~~~~{\texttt{n}} & := & \FF{lit}[\FF{lbl}[\texttt{n}]]()\\
% %e(e_1) & := & \FF{targ}[\FF{nil}](e; e_1)\\
% %e\cdot{\texttt{lbl}} & := & \FF{targ}[\FF{lbl}[{\texttt{lbl}}]](e; \cdot)\\
% %e\cdot{\texttt{lbl}}(e_1, ..., e_n) & := & \FF{targ}[\FF{lbl}[{\texttt{lbl}}]](e; e_1; \ldots; e_n)
% %%%(e_1, ..., e_n) & := & intro[()](e_1; ...; e_n)
% %%%n &  := & intro[n]()
% %%%s & := & intro[s]()
% %%%
% %%%e.l & := & elim[l](e)
% %%%e[e_1; ...; e_n] & := & elim[()](e_1, ..., e_n)
% %%%e.l(e_1; ...; e_n) & := & elim[l]
% %\end{array}
% %$
% %
% \caption{A program written using conventional concrete syntax, left, syntactically desugared to the abstract syntax on the right.}
% \label{atlam-example}
% \end{figure*}
% \begin{figure}[t!]
% \newcommand{\eqval}[1]{#1}
% \newcommand{\noneqval}[1]{#1}
% \small
% \[
% \begin{array}{rlcl}
% \text{programs} & \rho & ::= & \FF{import}[\Phi](e)\\
% \text{fragments} & \Phi & ::= & \emptyset \pipe \Phi, \fvar{tycon}=\{ \delta \}\\
% \text{tycon defs} & \delta & ::= & \FF{analit}=\sigma; ~\FF{synidxlit}=\sigma; \\
% & & & \FF{anatarg}=\sigma;~\FF{syntarg}=\sigma\\
% \text{expressions} & e & ::= & x \pipe \FF{let}(e; x.e) \pipe   \FF{slet}[\sigma](\tvar{x}.e) \pipe \FF{asc}[\sigma](e) \\
% & & \pipe & \lambda(x.e) \pipe \FF{lit}[\sigma](\bar{e}) \pipe \FF{targ}[\sigma](e; \overline{e}) \\
% \text{static terms} & \sigma & ::= & \tvar{x}  \pipe \noneqval{\lambda(\tvar{x}.\sigma)} \pipe\FF{ap}(\sigma; \sigma) \pipe \FF{fail}\\
%  & & \pipe & \eqval{\FF{ty}[\fvar{tycon}](\sigma)} \pipe \FF{tycase}[\fvar{tycon}](\sigma; \tvar{x}.\sigma; \sigma) \\
% & & \pipe & \eqval{\FF{incty}[\fvar{tycon}](\sigma)}  \\
% & & \pipe & \eqval{\FF{lbl}[{\texttt{lbl}}]} \pipe \FF{lbleq}(\sigma; \sigma; \sigma; \sigma) \\
% & & \pipe & \eqval{\FF{nil}} \pipe \eqval{\FF{cons}(\sigma; \sigma)} \pipe  \FF{listrec}(\sigma; \sigma; \tvar{x}.\tvar{y}.\sigma) \\
%  & & \pipe & \noneqval{\FF{arg}[e]} \pipe \FF{ana}(\sigma; \sigma; \tvar{x}.\sigma) \pipe \FF{syn}(\sigma; \tvar{x}.\tvar{y}.\sigma)\\
%  & & \pipe &  \noneqval{\titerm{\iota}}\\
% \text{internal terms} & \iota & ::= & \iup{\sigma} \pipe x \pipe \lambda(x.\iota) \pipe \FF{iap}(\iota; \iota) \\
% & & \pipe  &  \FF{inil} \pipe \FF{icons}(\iota; \iota) \pipe \FF{ilistrec}(\iota; \iota; x.y.\iota)

% \end{array}
% \]
% %\vspace{-15px}
% \caption{Abstract syntax of @$\lambda$. Metavariable $\fvar{tycon}$ ranges over type constructor names (assumed globally unique), ${\texttt{lbl}}$ over static labels, $x, y$ over expression variables and $\tvar{x}, \tvar{y}$ over static variables. We indicate that variables or static variables are bound within a term or static term by separating them with a dot, e.g. $\tvar{x}.\tvar{y}.e$, and abbreviate a sequence of zero or more expressions as $\overline{e}$. %Static values are shown annotated with either $\color{gray}=$ or $\color{gray}\neq$. Values having and containing only sub-terms with $\color{gray}=$ annotations are those for which syntactic and semantic equality coincide, and only such values are valid type indices, captured in the judgement $\sigma~\mathtt{eqval}_\Phi$.   % Highlighted terms are exclusively used by fragment providers and will be described further below; $\FF{arg}[e]$ is introduced only by the semantics and is a value, and the IL quotation form $\titerm{\iota}$ is a value if it does not contain any unquote forms, $\iup{\sigma}$.
% }
% \label{syntax-clients}
% \vspace{5px}
% \end{figure}
% %\begin{figure}[t]
% %\small
% %$\begin{array}{rcl}
% %\fvar{tycon}\texttt{[}\sigma\texttt{]} & := & \FF{ty}[\fvar{tycon}](\sigma)\\
% %\fvar{tycon}\texttt{[}\cdots\sigma\cdots\texttt{]} & := & \FF{incty}[\fvar{tycon}](\sigma)\end{array}$
% %~~~~~~~~$
% %\begin{array}{rcl}
% %\fvar{tycon} & := & \FF{incty}[\fvar{tycon}](\FF{nil})\\
% %\fvar{tycon}\texttt{[}\cdots\texttt{]} & := & \FF{incty}[\fvar{tycon}](\FF{nil})\\
% %\end{array}$\\
% %~\\
% %$
% %\begin{array}{rcl}
% %\lambda x{:}\sigma.e & := & \FF{asc}[\FF{incty}[\fvar{fn}](\sigma)](\lambda(x.e))\\
% %(e_1, ..., e_n) & := & \FF{lit}[\FF{nil}](e_1; \ldots; e_n)\\
% %\{{\texttt{lbl}}_1=e_1, \ldots, {\texttt{lbl}}_n=e_n\} & := & \FF{lit}[\FF{cons}(\FF{lbl}[{\texttt{lbl}}_1]; \ldots\FF{cons}(\FF{lbl}[{\texttt{lbl}}_n]; \FF{nil}))](e_1; \ldots; e_n)\\
% %\text{(any numeric label)}~~~~{\texttt{n}} & := & \FF{lit}[\FF{lbl}[\texttt{n}]]()\\
% %e(e_1) & := & \FF{targ}[\FF{nil}](e; e_1)\\
% %e\cdot{\texttt{lbl}} & := & \FF{targ}[\FF{lbl}[{\texttt{lbl}}]](e; \cdot)\\
% %e\cdot{\texttt{lbl}}(e_1, ..., e_n) & := & \FF{targ}[\FF{lbl}[{\texttt{lbl}}]](e; e_1; \ldots; e_n)
% %%%(e_1, ..., e_n) & := & intro[()](e_1; ...; e_n)
% %%%n &  := & intro[n]()
% %%%s & := & intro[s]()
% %%%
% %%%e.l & := & elim[l](e)
% %%%e[e_1; ...; e_n] & := & elim[()](e_1, ..., e_n)
% %%%e.l(e_1; ...; e_n) & := & elim[l]
% %\end{array}
% %$
% %\vspace{-5px}
% %\caption{\small Selected desugarings from a stylized conventional concrete syntax to abstract syntax.}
% %\label{desugaring}
% %\vspace{10px}
% %\end{figure}


% %

% We now turn our attention to a type theoretic formulation of the key mechanisms described above atop a minimal abstract syntax, shown in Fig. \ref{syntax-clients}. This syntax supports a \emph{purely syntactic desugaring} from a conventional concrete syntax, shown by example in Fig. \ref{atlam-example}. 

% \paragraph{Fragment Client Perspective} A program, $\rho$, consists of a series of fragment imports, $\Phi$, defining active type constructors for use in an expression, $e$. Expression forms can be indexed by static terms, $\sigma$. The abstract syntax of $e$ provides $\FF{let}$ binding of variables and for convenience, static values can also be bound to a static variable, $\tvar{x}$ (distinguished in bold for clarity), using $\FF{slet}$. Static terms have a \emph{static dynamics} and evaluate to \emph{static values} or fail. Static terms are analagous to top-level Python code in \verb|typy|, and external terms are analagous to terms inside typed functions.

% \subparagraph{Types and Ascriptions} An expression can be ascribed a {type} or an {incomplete type}, both  {static values} constructed, as in the introduction, by naming an imported type constructor, $\fvar{tycon}$, and providing a type index, another static value. The static language also includes lists and atomic \emph{labels} for use in compound indices. %The remaining static term constructors are of use mainly to fragment providers (see below).

% \subparagraph{Literal Desugaring} All concrete literals (other than lambda expressions, which are built in)  desugar to an abstract term of the form $\FF{lit}[\sigma](\overline{e})$, where $\sigma$ captures all static portions of the literal (e.g. a list of the labels used as field names in the labeled product literal in Fig. \ref{atlam-example}, or the numeric label used for the natural number zero) and $\overline{e}$ is a list of sub-expressions (e.g. the field values). % is only one term constructor corresponding to each basic method by which a type constructor is delegated responsibility over a term. 

% \subparagraph{Targeted Expression Desugaring} All non-introductory operations go through a targeted expression form (e.g. $e(e)$, or $e\cdot\texttt{lbl}$, or $e\cdot\texttt{lbl}(\overline{e})$; see previous section), which all desugar to an abstract term of the form $\FF{targ}[\sigma](e; \overline{e})$ where $\sigma$ again captures all static portions of the term (e.g. the label naming an operation, e.g. $\texttt{s}$ or $\texttt{rec}$ on natural numbers \cite{pfpl}), $e$ is the  target (e.g. the natural number being operated on, the function being called, or the record we wish to project out of) and $\overline{e}$ are all other arguments (e.g. the base and recursive cases of the recursor, or the argument being applied).% We conjecture that formalizing these semantics would not present a substantial challenge.


% \paragraph{Bidirectional Active Typechecking and Translation}
% \newcommand{\atjsynX}[3]{\Gamma \vdash_\fvalCtx #1 \Rightarrow #2 \leadsto #3}
% \newcommand{\atjanaX}[3]{\Gamma \vdash_\fvalCtx #1 \Leftarrow #2 \leadsto #3}
% \newcommand{\atjerrX}[1]{\Gamma \vdash_\fvalCtx #1~ \mathtt{error}}
% \begin{figure*}[t!]
% \small
% $\fbox{\inferrule{}{\atjsynX{e}{\sigma}{\iota}}}$~~
% $\fbox{\inferrule{}{\atjanaX{e}{\sigma}{\iota}}}$~~
% $\Gamma ::= \emptyset \pipe \Gamma, x \Rightarrow \sigma$
% %$\fbox{\inferrule{}{\atjerrX{e}}}$
% \renewcommand{\MathparLineskip}{\lineskiplimit=.6\baselineskip\lineskip=.6\baselineskip plus .2\baselineskip}
% \begin{mathpar}
% \inferrule[att-subsume]{
%   \atjsynX{e}{\sigma}{\iota}
% }{
%   \atjanaX{e}{\sigma}{\iota}
% }

% \inferrule[att-var]{
%   x \Rightarrow \sigma \in \Gamma
% }{
%   \atjsynX{x}{\sigma}{x}
% }

% \inferrule[att-ana-let]{
%   \atjsynX{e_1}{\sigma_1}{\iota_1}\\\\
%   \Gamma, x \Rightarrow \sigma_1 \vdash_\fvalCtx e_2 \Leftarrow \sigma_2 \leadsto \iota_2
% }{
%   \atjanaX{\FF{let}(e_1; x.e_2)}{\sigma_2}{\FF{iap}(\lambda(x.\iota_2); \iota_1)}
% }

% \inferrule[att-syn-let]{
%   \atjsynX{e_1}{\sigma_1}{\iota_1}\\\\
%   \Gamma, x \Rightarrow \sigma_1 \vdash_\fvalCtx e_2 \Rightarrow \sigma_2 \leadsto \iota_2
% }{
%   \atjsynX{\FF{let}(e_1; x.e_2)}{\sigma_2}{\FF{iap}(\lambda(x.\iota_2); \iota_1)}
% }

% \inferrule[att-syn-asc]{
%     \sigma \Downarrow_{\emptyset; \emptyset} \sigma'\\
%      \sigma'~\mathtt{ty}_\fvalCtx\\
%   \atjanaX{e}{\sigma'}{\iota}
% }{
%   \atjsynX{\FF{asc}[\sigma](e)}{\sigma'}{\iota}
% }

% \inferrule[att-ana-slet]{
%    \sigma_1 \Downarrow_{\emptyset; \emptyset} \sigma_1'\\
%     \atjanaX{[\sigma_1'/\tvar{x}]e}{\sigma_2}{\iota}
% }{
%   \atjanaX{\FF{slet}[\sigma_1](\tvar{x}.e)}{\sigma_2}{\iota}
% }

% \inferrule[att-syn-slet]{
%    \sigma_1 \Downarrow_{\emptyset; \emptyset} \sigma_1'\\
%     \atjsynX{[\sigma_1'/\tvar{x}]e}{\sigma_2}{\iota}
% }{
%   \atjsynX{\FF{slet}[\sigma_1](\tvar{x}.e)}{\sigma_2}{\iota}
% }

% \inferrule[att-ana-lam]{
%    \Phi_\text{fn} \subset \Phi\\ \sigma_1~\mathtt{ty}_\fvalCtx\\\sigma_2~\mathtt{ty}_\fvalCtx\\
%   \Gamma, x \Rightarrow \sigma_1 \vdash_\fvalCtx e \Leftarrow \sigma_2 \leadsto \iota
% }{
%   \atjanaX{\lambda(x.e)}{\FF{ty}[\fvar{fn}](\FF{cons}(\sigma_1; \FF{cons}(\sigma_2; \FF{nil})))}{\lambda(x.\iota)}
% }

% \inferrule[att-syn-idx-lam]{
%    \Phi_\text{fn} \subset \Phi\\ \sigma_1~\mathtt{ty}_\fvalCtx\\
%   \Gamma, x \Rightarrow \sigma_1 \vdash_\fvalCtx e \Rightarrow \sigma_2 \leadsto \iota
% }{
%   \atjsynX{\FF{asc}[\FF{incty}[\fvar{fn}](\sigma_1)](\lambda(x.e))}{\FF{ty}[\fvar{fn}](\FF{cons}(\sigma_1; \FF{cons}(\sigma_2; \FF{nil})))}{\lambda(x.\iota)}
% }
% %
% %\inferrule[att-lam]{
% % \tau_1 \Downarrow_{\Gamma;\fvalCtx} \tau_1'\\
% %   \trepof{\tau_1'} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\\\
% % \Gamma, x \Rightarrow \tau_1' \vdash_\fvalCtx e \Rightarrow \tau_2 \leadsto \iota
% %%  \iota \hookrightarrow_\fvalCtx \iota'\\
% %%    \sigma \hookrightarrow_\fvalCtx \sigma'
% %%  \ddbar{\fvar{Arrow}}{\fvalCtx}{\trepof{\tau_1'}}{\sbar_1}\\
% % %\delfromtau{$\Xi_0$}{\fvalCtx}{\tau_1'}{\sabs}\\\\
% %}{
% % \atjsynX{\elam{x}{\tau_1}{e}}{\ttype{arrow}{(\tau_1', \tau_2)}}{\ilam{x}{\sigma'}{\iota}}
% %}

% \inferrule[att-ana-lit]{
%   \vdash_\fvalCtx \FF{analit}(\fvar{tycon})=\sigmat{def}\\
%   \mathtt{args}(\overline{e})=\sigmat{args}\\\\
%   \sigmat{def} ~\sigmat{tyidx} ~\sigmat{tmidx} ~\sigmat{args} \Downarrow_{\Gamma; \fvalCtx} \titerm{\iota}
% % \FF{ap}(\FF{ap}(\FF{ap}(\sigmat{def}; \sigmat{tyidx}); \sigmat{tmidx}); \sigmat{args}) \Downarrow_{\Gamma;\fvalCtx} \titerm{\iota}
% % \trepof{\ttype{tycon}{\tauidx'}} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\
% }{
%   \atjanaX{\FF{lit}[\sigmat{tmidx}](\overline{e})}{\FF{ty}[\fvar{tycon}](\sigmat{tyidx})}{\iota}
% }

% \inferrule[att-syn-idx-lit]{
%   \vdash_\fvalCtx \FF{synidxlit}(\fvar{tycon})=\sigmat{def}\\
%   \mathtt{args}(\overline{e})=\sigmat{args}\\\\
%   \sigmat{def} ~\sigmat{incidx} ~\sigmat{tmidx} ~\sigmat{args} \Downarrow_{\Gamma;\fvalCtx} \FF{cons}(\sigmat{tyidx}; \FF{cons}(\titerm{\iota}; \FF{nil}))\\
% % \FF{ty}[\fvar{tycon}](\sigmat{tyidx})~\mathtt{ty}_\fvalCtx
% % \FF{ap}(\FF{ap}(\FF{ap}(\sigmat{def}; \sigmat{incidx}); \sigmat{tmidx}); \sigmat{args}) \Downarrow_{\Gamma;\fvalCtx}\FF{cons}(\sigmat{tyidx}; \FF{cons}(\titerm{\iota}; \FF{nil}))
% }{
%   \atjsynX{\FF{asc}[\FF{incty}[\fvar{tycon}](\sigmat{incidx})](\FF{lit}[\sigmat{tmidx}](\overline{e}))}{\FF{ty}[\fvar{tycon}](\sigmat{tyidx})}{\iota}
% }
% %
% %\inferrule[att-i-asc-ty]{
% % \atjanaX{I}{\tau}{\iota}
% %}{
% % \atjsynX{I : \tau}{\tau}{\iota}
% %}
% %
% %\inferrule[att-i-asc-tycon]{
% % \vdash_\fvalCtx \FF{isyn}(\fvar{tycon})=\taudef\\\\
% % \taudef~\tauidx~((\Gamma; e_1)? {::}{\ldots}{::}(\Gamma; e_n)? {::} []) \Downarrow_{\Gamma;\fvalCtx} \tden{\titerm{\iota}}{\ttype{tycon}{\tauidx'}}\\
% %     \trepof{\tau_1'} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\
% % \Gamma \vdash_\fvalCtx \iota : \sigma
% %}{
% % \atjsynX{\FF{intro}[\tauidx](e_1; \ldots; e_n)] :: \fvar{tycon}}{\ttype{tycon}{\tauidx'}}{\iota'}
% %}

% \inferrule[att-ana-targ]{
%   \atjsynX{e_\text{targ}}{\FF{ty}[\fvar{tycon}](\sigmat{tyidx})}{\iota_\text{targ}}\\\\
%   \vdash_\fvalCtx \FF{anatarg}(\fvar{tycon})=\sigmat{def}\\
%   \mathtt{args}(\overline{e})=\sigmat{args}\\\\
%   \sigmat{def} ~\sigmat{tyidx} ~\titerm{\iota_\text{targ}} ~\sigmat{ty} ~\sigmat{tmidx} ~\sigmat{args} \Downarrow_{\Gamma;\fvalCtx} \titerm{\iota}
% % \FF{ap}(\FF{ap}(\FF{ap}(\FF{ap}(\FF{ap}(\sigmat{def}; \sigmat{tyidx}); \titerm{\iota}); \sigma); \sigmat{tmidx}); \sigmat{args}) \Downarrow_{\Gamma;\fvalCtx} \titerm{\iota'}
% }{
%   \atjanaX{\FF{targ}[\sigmat{tmidx}](e_\text{targ}; \overline{e})}{\sigmat{ty}}{\iota}
% }

% \inferrule[att-syn-targ]{
%   \atjsynX{e_\text{targ}}{\FF{ty}[\fvar{tycon}](\sigmat{tyidx})}{\iota_\text{targ}}\\\\
%   \vdash_\fvalCtx \FF{syntarg}(\fvar{tycon})=\sigmat{def}\\
%   \mathtt{args}(\overline{e})=\sigmat{args}\\\\
%   \sigmat{def} ~\sigmat{tyidx} ~\titerm{\iota_\text{targ}} ~\sigmat{tmidx} ~\sigmat{args} \Downarrow_{\Gamma;\fvalCtx} \FF{cons}(\sigmat{ty}; \FF{cons}(\titerm{\iota}; \FF{nil}))\\
%   \sigmat{ty}~\mathtt{ty}_\fvalCtx
% }{
%   \atjsynX{\FF{targ}[\sigmat{tmidx}](e_\text{targ}; \overline{e})}{\sigmat{ty}}{\iota'}
% }
% \end{mathpar}
% \caption{Bidirectional active typechecking and translation. For concision, we use standard functional notation for static function application.}
% \label{atj}
% \end{figure*}
% %In this section, we will describe the foundations of \emph{active typechecking and translation} by constructing a core lambda calculus, @$\lambda$. 
% %A \emph{program} in @$\lambda$, $\rho$, consists of a series of fragment definitions, $\phi$, for use by an external term, $e$. An example of an @$\lambda$ program, $\rho_{\text{example}}$, that uses {fragment definitions} $\phi_{\texttt{nat}}$ and $\phi_{\texttt{lprod}}$, defining primitive natural numbers and labeled products (i.e. ordered records), is shown in Fig. \ref{nat-sugared}. In \texttt{typy},  fragment definitions are packaged separately; we do not include this in the core calculus for simplicity.
% %
% %In this example, we see the introduction and elimination forms for natural numbers, functions and labeled tuples all being used. However, in the core syntax, shown (from the perspective of fragment clients) in Fig. \ref{client-syntax}, there are only two external introductory forms, $\lambda x.e$ and $\FF{intro}[\sigma](\overline{e})$, and one external elimination form, $e\cdot\FF{elim}[\sigma](\overline{e})$, where $\overline{e}$ is shorthand for zero or more semicolon-separated external terms, called the \emph{arguments} of the introduction or elimination operation. The forms used in Fig. \ref{nat-sugared} are \emph{derived forms}, defined in terms of these core forms in Fig. \ref{derived}. 
% %
% %Both fragments and external terms can contain \emph{static terms}, $\sigma$, which are evaluated statically to \emph{static values}, $\hat\sigma$. The static language is itself a simply-typed lambda calculus. We call the types of static terms \emph{kinds}, $\kappa$, by convention. Static variables are written in bold font, $\tvar{x}$, to emphasize that they are distinct from variables bound by external terms, written $x$. For example, in the example in Figure \ref{nat-sugared}, $\tvar{nat}$ is a static variable bound by the fragment $\phi_{\texttt{nat}}$ to the natural number type. Types have kind $\kTypeBlur$ and are introduced by naming a \emph{type constructor}, written in small-caps, e.g. $\fvar{nat}$ and $\fvar{lprod}$, and providing an \emph{index}, which is a static term of the \emph{index kind} of the type constructor.  For example, $\fvar{nat}$ is indexed by $\kunit$ (because there is only one natural number type), so $\tvar{nat}$ is defined to be $\ttype{nat}{\tunit}$, and $\fvar{lprod}$ is indexed by $\klist{\kpair{\FF{L}}{\kTypeBlur}}$, classifying static lists pairing \emph{labels} with types. Labels are static values of kind $\FF{L}$, written abstractly using the metavariables containing $\ell$ in our examples. 
% %
% %Before typechecking the external term in a program, the program is kind checked and its static terms are normalized to static values.  We write static values using the hatted metavariable $\hat \sigma$. Other syntactic metavariables in our system also have hatted forms if they can contain static terms. The desugared, statically normalized version of the example in Figure \ref{nat-sugared}, $\hat\rho_{\text{example}}$, is shown in Figure \ref{nat-normal}. %The syntax of @$\lambda$ from the perspective of a fragment client is given in Fig. \ref{atlam-syntax}. We will see a few additions relevant only to fragment providers when we discuss defining fragments below.
% %
% %\subsection{External Terms}
% % a program consists of first \emph{kind checking} its static terms, then normalizing its static terms, then typechecking the external term and,  simultaneously, \emph{translating} it to a term, $\iota$, in the \emph{typed internal language}. The dynamic behavior of an external term is determined entirely by its translation to this language. Internal types are written $\tau$. The internal language is only exposed to fragment providers, not to clients (i.e. normal developers).


% %We will now give a core typed lambda calculus, @$\lambda$, that captures the semantics described in the previous sections. It is intended to make precise how active typechecking and translation works and how our mechanism relates to existing work on bidirectional typechecking, type-level computation and typed compilation while abstracting away from the details of Python's syntax and imposing a stronger type-level semantics that will allow us to state metatheoretic properties of interest. We will assume a fixed base for functions (providing the standard semantics of lambda functions) and target language, which we here call the \emph{internal language}.
%  % program written in core @$\lambda$, without syntactic sugar and with all static terms normalized, is shown in Fig. \ref{nat-desugared}. The syntax of core @$\lambda$ is shown in Fig. \ref{grammar} and the syntactic desugarings are shown in Fig. \ref{desugaring}. The definition of $\phi_{\texttt{nat}}$ is shown in Fig. \ref{nat-atfrag}. We will discuss these in the following sections.

% %In this section, we will develop an ``actively typed'' version of the simply-typed lambda calculus with simply-kinded type-level computation called $\lam\texttt{typy}$. More specifically, the level of types, $\tau$, will itself form a simply-typed lambda calculus. \emph{Kinds} classify type-level terms in the same way that types conventionally classify expressions. Types become just one kind  of type-level value (which we will write $\kTypeBlur$, though it is also variously written $\star$, \lip{T} and \lip{Type} in various settings). Rather than there being a fixed set of type and operator constructors, we allow the programmer to declare new  constructors, and give their static and dynamic semantics by writing type-level functions. The kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction will allow us to prove strong type safety, decidability and conservativity theorems.
% %
% %The syntax of Core $\lam\texttt{typy}$ is given in Fig. \ref{grammar}. An example of a program defining type and operator constructors that can be used to construct an active embedding of G\"odel's \textbf{T} into $\lam\texttt{typy}$ is given in Fig. \ref{nat}. We will discuss its semantics and how precisely the embedding, seen being used starting on line 15 to ultimately compute the sum of two and two, works as we go on. Natural numbers can, of course, be isomorphically embedded in existing languages, with a similar usage and asymptotic performance profile (up to function call overhead as an abstract type, for example). We will provide more sophisticated examples where this is less feasible later on (and note that type abstraction is an orthogonal mechanism).
% %a type constructor declaration, $\fvar{nat}$, indexed trivially, together with three operator constructors, also all indexed trivially, that implement the standard introductory forms for natural numbers as well as the recursor operator (as in G\"odel's T \cite{pfpl}). Following the type constructor declaration, we apply $\fvar{nat}$ with the trivial index, $\tunit$, to form the type $\tvar{nat}$. Finally, we write an external term that uses the operators associated with $\fvar{nat}$ and the built-in constructor $\fvar{Parr}$, governing partial functions, to define an addition function and compute the addition of the natural numbers  two and two. We will introduce a more convenient concrete syntax in later portions of this thesis; for now we will restrict ourselves to the abstract syntax so that this example can directly aid in understanding the semantics.

% %\begin{figure}[t]
% %\small
% %$$\begin{array}{rrcl}  
% %%\textbf{programs} & 
% %\text{programs} & \rho & ::= & \F{using}\phi~\F{in}e \\
% %\text{fragments} & \phi &  ::= & \F{tycon}\fvar{tycon}~\F{of}\kappat{tyidx}~{\{}\F{iana} \tau_1; \F{esyn} \tau_2; \F{rep} \tau_3{\}} %\pipe \phi; \phi %\\
% %%\\
% %%& \pipe & 
% %\pipe \F{def}\tvar{t} : \kappa = \tau \pipe \phi; \phi%\pdef{t}{\kappa}{\tau}{\progsort} \pipe 
% %\\ %\pfam{\familyDf}{\progsort}
% %%\\&  \pipe & \pdef{t}{\kappa}{\tau}{\progsort}  \pipe e
% %%\pipe \pdef{t}{\kappa}{\tau}{\progsort} 
% %\text{external terms}& e & ::= & \evar{x} \pipe \F{let} x = e_1~\F{in} e_2 \pipe \lambda x.e \pipe e : \tau\\
% %&& \pipe & \FF{intro}[\taut{opidx}](\splat{e}{1}{n})\pipe e\cdot\FF{elim}[\taut{opidx}](\splat{e}{1}{n}) \\
% %
% %\text{type-level terms} & \tau   & ::=   &   \tvar{t} \pipe \tifeq{\tau_{1}}{\tau_{2}}{\kappa}{\tau_{3}}{\tau_{4}} \pipe \terr  \\
% % && \pipe & 
% % %\F{let}\tvar{t}{:}\kappa=\tau_1~\F{in} \tau_2 \pipe 
% %                           \tlam{t}{\kappa}{\tau} \pipe 
% %                           \tapp{\tau_1}{\tau_2} \\
% %&&\pipe&                     
% %                           \tnil{\kappa} \pipe \tcons{\tau_1}{\tau_2} \pipe 
% %                                      \tfold{\tau_1}{\tau_2}{\tvar{t}_{hd}}{\tvar{t}_{tl}}{\tvar{t}_{rec}}{\tau_3}
% %                           \\
% %     && \pipe  &     \ell  \\
% %     && \pipe &  
% %   \tunit \pipe 
% %                           \tpair{\tau_{1}}{\tau_{2}} \pipe 
% %                           \tfst{\tau} \pipe 
% %                           \tsnd{\tau} 
% %                           \\  
% %     && \pipe & \tinl{\kappa_1, \kappa_2}{\tau_1} \pipe \tinr{\kappa_1, \kappa_2}{\tau_2} \pipe \tsumcase{\tau}{t}{\tau_1}{t}{\tau_2}\\
% %&&\pipe  &   \ttype{tycon}{\taut{tyidx}} \pipe  \tfamcase{\tau}{tycon}{t}{\tau_1}{\tau_2}\\
% %%                            \\                          %       & & \pipe & \tfamcase{\tau}{Fam}{x}{\tau_1}{\tau_2}\\
% %%                                                
% %%\\     &  \pipe & \tden{\tau_2}{\tau_1} \pipe \terr %\pipe \tden{\ibar}{\tau}^{\checkmark} 
% %%\pipe \ttypeof{\tau} \pipe \itransof{\tau} \\% \tdencase{\tau}{x}{t}{\tau_1}{\tau_2}\\
% %% %& & \pipe & 
% %%%                           \tdencase{\tau}{y}{x}{\tau_1}{\tau_2}
% %%%                            \\
% %%
% % && \pipe & \titerm{\iota}
% %     \pipe \titype{\sigma} \pipe \trepof{\tau} \\
% %   && \pipe &  (\Gamma; e)? \pipe \FF{syn}(\tau_1; \tvar{t}_{ty},\tvar{t}_{trans}.\tau_2) \pipe \FF{ana}(\tau_1; \tau_2; \tvar{t}_{trans}.\tau_3)\\
% %%\textbf{external terms}         & e & ::= & \evar{x} \pipe 
% %%%                           \efix{x}{\tau}{e} \pipe 
% %%                            \elam{\evar{x}}{\tau}{e} \pipe 
% %%                            \eop{tycon}{op}{
% %%                              \tauidx
% %%                            }{
% %%                                  \splat{e}{1}{n}
% %%                            } \\
% %%                  &     &   &   \\
% %%
% %%
% %%                        
% %%%\text{deabstracted}& \iota & ::= & \mathcal{G}[\iota, \sigma]\\
% %%                        
% %%\\
% %%\\
% %%              
% %%% &  & \pipe & \tvalof{\tau_1}{\tau_2} \pipe \iup{\tau} \\
% %%%                         \trepof{\tau} \pipe \dup{\tau}\\
% %%\text{translational IL} & \bar{\iota} & ::= & x \pipe \ifix{x}{\bar \sigma}{\bar \iota} 
% %%  %\pipe \ilam{x}{\bar \sigma}{\bar \iota} \pipe \iapp{\bar \iota_1}{\bar \iota_2} 
% %%  \pipe \cdots \pipe \itransof{\tau} \\           
% %% & \bar{\sigma} & ::= & \darrow{\bar \sigma_1}{\bar \sigma_2} \pipe \cdots \pipe \dup{\tau} \pipe \trepof{\tau} \\
% %%%\text{abstracted} & \sabs & ::= & \darrow{\sabs_1}{\sabs_2} \pipe \cdots \pipe \sabsrep{\tau}
% %%                      \\
% %\text{kinds} & \kappa  & ::= & \karrow{\kappa_1}{\kappa_2} \pipe \klist{\kappa} \pipe \FF{L} \pipe \kunit \pipe \kpair{\kappa_1}{\kappa_2} \pipe \ksum{\kappa_1}{\kappa_2} \pipe \kTypeBlur \pipe \kITerm \pipe \kIType \pipe \Q
% %%                          \klabel \pipe
% %%                          \klist{\kappa} \pipe
% %%                        \kunit \pipe 
% %%                        \kpair{\kappa_{1}}{\kappa_{2}} \\
% %%                        &&\pipe&
% %%                        \ksum{\kappa_1}{\kappa_2} \pipe
% %%                        \kTypeBlur \pipe \kDen \pipe 
% %%                        \kIType               
% %%%\textbf{ops signature}     & \Theta  & ::= & \kOpEmpty \pipe \kOp{\Theta}{op}{\kappai}\\
% %%%                                   &   &   & \\
% %\\
% %\text{typing contexts} & \Gamma & ::= & \emptyset \pipe \Gamma, x \Rightarrow \tau\\
% %\\
% %\text{internal terms} & \iota  & ::= & \evar{x} \pipe 
% %                       \ifix{\evar{x}}{\sigma}{\iota} \pipe
% %                       \ilam{\evar{x}}{\sigma}{\iota} \pipe 
% %                       \iapp{\iota_{1}}{\iota_{2}} \pipe n \pipe \iota_1 \pm \iota_2 \pipe \FF{if0}(\iota; \iota_1; \iota_2)\\
% %   && \pipe &  \iunit \pipe \ipair{\iota_1}{\iota_2} \pipe \ifst{\iota} \pipe \isnd{\iota} \pipe \iup{\tau} 
% %%\\&\pipe&               \cdots \pipe  
% %%                        n \pipe \iop{\iota_{1}}{\iota_{2}} \pipe \iIfEq{\iota_{1}}{\iota_{2}}{\dint}{\iota_{3}}{\iota_{4}} \\
% %%& \pipe & 
% %%                        \iunit \pipe
% %%                        \ipair{\iota_{1}}{\iota_{2}} \pipe 
% %%                        \ifst{\iota} \pipe
% %%                        \isnd{\iota} 
% %%\\&\pipe&                       
% %%                         \iinl{\sigma_2}{\iota_1} \pipe \iinr{\sigma_1}{\iota_2} \\&\pipe & \icase{e}{x}{e_1}{x}{e_2} \\
% %\\ \text{internal types} & \sigma  & ::= &    \darrow{\sigma_1}{\sigma_2} \pipe \dint \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} %\pipe \dint \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} \pipe \dsum{\sigma_1}{\sigma_2}  
% % \pipe \dup{\tau}
% %\end{array}$$
% %%\vspace{-10pt}
% %\caption{\small Syntax of Core @$\lambda$. Here, $x$ ranges over external and internal language variables, $\tvar{t}$ ranges over type-level variables, $\fvar{tycon}$ ranges over type constructor names, $\ell$ ranges over labels, $n$ ranges over integers and $\pm$ ranges over standard binary operations over integers. The introductory form for arguments, $(\Gamma; e)?$, should only be constructed by the compiler, not by type constructor providers.
% %\label{grammar}}
% %\end{figure}
% %\begin{figure*}[t]
% %\small
% %\begin{tabular}{r l l l}
% % & \textbf{initial} & \textbf{statically normal}\\
% %\textbf{programs} & $\rho ::=$ & $\hat\rho ::=$\\
% % & ~~$\FF{using}~{\phi}~\FF{in}~e$ & ~~$\FF{using}~{\hat\phi}~\FF{in}~\hat e$\\
% %\textbf{fragment decls.} & $\phi ::=$ & $\hat\phi ::=$\\
% %tycon decls. & ~~$\FF{tycon}~\fvar{tycon} : \Theta = \theta$ & ~~$\FF{tycon}~\fvar{tycon} : \Theta = \hat\theta$\\
% %static binding & ~~$\FF{def}~\tvar{x} : \kappa = \sigma$ & \\
% % & ~~$\phi; \phi$ & ~~$\hat\phi; \hat\phi$\\
% %\textbf{static terms} & $\sigma ::=$ & $\hat\sigma ::= $ &\textbf{kinds}~~~  $\kappa ::= $\\
% %static values & ~~$\hat \sigma$\\
% %static variables & ~~$\tvar{x}$ & ~ & ~\\
% %types & ~~$\ttype{tycon}{\sigma}$ & ~~$\ttype{tycon}{\hat\sigma}$ & ~~$\kTypeBlur$\\
% %%static products & ~~$\tunit$ & ~~$\tunit$ & ~~$\kunit$\\
% %%& ~~$(\sigma, \sigma)$ & ~~$(\hat\sigma, \hat\sigma)$ & ~~$\kpair{\kappa}{\kappa}$\\
% %%static sums & ~~$\FF{in}[\ell](\sigma)$ & ~~$\FF{in}[\ell](\hat\sigma)$ & ~~$\Sigma[\{\overline{\ell \hookrightarrow \kappa}\}]$\\
% %static lists & ~~$[]_\kappa$ & ~~$[]_\kappa$ & ~~$\klist{\kappa}$\\
% % & ~~$\sigma :: \sigma$ & ~~$\hat\sigma :: \hat\sigma$ & \\
% %static labels & ~~$\ell$ & ~~$\ell$ & ~~$\FF{L}$\\
% %\\
% %\textbf{external terms} & $e ::= $ & $\hat e ::=$ \\
% %variables & ~~$x$ & ~~$x$\\
% %binding & ~~$\FF{let}~x=e~\FF{in}~e$ & ~~$\FF{let}~x=\hat{e}~\FF{in}~\hat{e}$\\
% %fixpoints & ~~$\FF{fix}~x{:}\sigma~\FF{is}~e$ & ~~$\FF{fix}~x{:}\hat\sigma~\FF{is}~\hat e$\\
% %ascription & ~~$e : \sigma$ & ~~$\hat{e} : \hat\sigma$\\
% %%introductory forms & ~~$\nu$ & ~~$\hat\nu$\\
% %%eliminatory forms & ~~$\eta$ & ~~$\hat\eta$\\
% %% & $\nu ::= $ & $\hat \nu ::=$\\
% %analytic lambda & ~~$\lambda x.e$ & ~~$\lambda x.\hat{e}$\\
% %%synthetic lambda & ~~$\lambda x {:} \sigma.e$ & ~~$\lambda x{:}\hat\sigma.\hat e$\\
% %analytic intro & ~~$\FF{intro}[\sigma](\overline{e})$ & ~~$\FF{intro}[\hat\sigma](\overline{\hat e})$\\
% %%synthetic intro & ~~$\fvar{tycon}\cdot\FF{intro}[\sigma](\overline{e})$ & ~~$\fvar{tycon}\cdot\FF{intro}[\hat \sigma](\overline{\hat e})$\\
% %% & $\eta ::= $ & $\hat\eta ::=$\\
% %synthetic elim & ~~$e\cdot\FF{elim}[\sigma](\overline{e})$ & ~~$\hat e\cdot\FF{elim}[\hat \sigma](\overline{\hat e})$\\
% %\end{tabular}
% %\caption{Syntax from the perspective of fragment clients (i.e. normal developers)}
% %\end{figure*}
% %\begin{figure*}[t]
% %\small
% %\hspace{-5px}
% %$
% %\begin{array}{rclr}
% %%\FF{intro}(e_1; \ldots; e_n) & := & \FF{intro}[()](e_1; \ldots; e_n)\\
% %\sigma\langle e_1; \ldots; e_n \rangle & := & \FF{intro}[\tfst{\sigma}](e_1; \ldots; e_n) : \tsnd{\sigma} & \text{assisted intro}\\
% %%\langle\sigmaidx\rangle(e_1; \ldots; e_n) & := & \FF{intro}[\sigmaidx](e_1; \ldots; e_n)\\
% %%[\sigmaidx](e_1; \ldots; e_n) & := & \FF{intro}[\sigmaidx](e_1; \ldots; e_n)\\
% %\{ \ell_1{=}~e_1, \ldots, \ell_n{=}~e_n \} & := & \FF{intro}[\ell_1 :: \ldots :: \ell_n :: []_{\FF{L}}](e_1; \ldots; e_n) & \text{labeled seq.}  \\
% %(e_1, ..., e_n) & := & \FF{intro}[()](e_1; \ldots; e_n) & \text{unlabeled seq.}\\
% %%n & := & \FF{intro}[n]() & \text{number literal} \\
% %%s & := & \FF{intro}[s]() & \text{string literal}\\
% %%e\cdot\FF{elim}(e_1; \ldots; e_n) & := & e\cdot\FF{elim}[()](e_1; \ldots; e_n)\\
% %e~e_1 & := & e\cdot\FF{elim}[()](e_1) & \text{application}\\
% %\sigma~e~\langle e_1; \ldots; e_n \rangle  & := & e\cdot\FF{elim}[\sigma](e_1; \ldots; e_n) & \text{assisted elim}\\
% %e.\ell & := & e\cdot\FF{elim}[\ell]() & \text{projection}\\
% %%\F{case}e~\{\ell_1\langle x_1 \rangle \Rightarrow e_1~|~\ldots~|~\ell_n\langle x_n \rangle \Rightarrow e_n\} & := & e\cdot\FF{elim}[\ell_1 :: \ldots :: \ell_n :: []_{\FF{L}}]( & \text{case analysis}\\
% %% & & \quad \lambda x_1.e_1; \ldots; \lambda x_n.e_n)\\
% %\sigma_1 \rightharpoonup \sigma_2 & := & \ttype{parr}{(\sigma_1, \sigma_2)} & \text{arrow types}
% %%!TEX encoding = UTF-8 Unicode
% %\begin{figure*}
% %\newcommand{\tcl}[1]{\multicolumn{2}{l}{#1}}
% %\small
% %\begin{tabular}{r l l l l}
% % & \tcl{\textbf{initial}} & \textbf{statically normal} \\
% %\textbf{tycon definitions} & \tcl{$\delta ::=$}  & $\hat\delta ::=$ \\
% % & \tcl{~~$\FF{iana}~\sigma; \FF{esyn}~\sigma; \FF{rep}~\sigma$} & ~~$\FF{iana}~\hat\sigma; \FF{esyn}~\hat\sigma; \FF{rep}~\hat\sigma$\\
% % \\
% %\textbf{(static language, cont.)} & \tcl{$\sigma ::= ...$} & $\hat\sigma ::= ...$ & $\kappa ::= ...$\\
% %%type elim & \tcl{~~$\tfamcase{\sigma}{tycon}{x}{\sigma}{\sigma}$} & \\
% %%static product elim & \tcl{~~$\sigma.\FF{prl}$} &\\
% %% & \tcl{~~$\sigma.\FF{prr}$} &\\
% %%static sum elim & \tcl{~~$\FF{case}~\sigma~\{\overline{l\langle\tvar{x}\rangle\Rightarrow\sigma}\}$} &\\
% %%static list elim & \tcl{~~$\FF{listrec}(\sigma; \sigma; \tvar{h}, \tvar{t}, \tvar{r}.\sigma)$} & ~\\
% %decidable equality & \tcl{~~$\tifeq{\hat\sigma}{\hat\sigma}{\kappa}{\hat\sigma}{\hat\sigma}$} & ~ & ~\\
% %type error & \tcl{~~$\FF{tyerr}$} & ~ \\
% %%static functions & \tcl{~~$\tlam{x}{\kappa}{\sigma}$} & ~~$\tlam{x}{\kappa}{\sigma}$ & ~~$\karrow{\kappa}{\kappa}$\\
% %% & \tcl{~~$\sigma~\sigma$} & ~ & ~\\
% %term translations & \tcl{~~$\titerm{\tri}$} & ~~$\titerm{\iota}$ & ~~$\kITerm$\\
% %type translations & \tcl{~~$\titype{\trt}$} & ~~$\titype{\tau}$ & ~~$\kIType$\\
% % & \tcl{~~$\trepof{\sigma}$} & ~\\
% %arguments & \multicolumn{3}{c}{\color{gray}(introduced only by typechecker)} & ~~$\Q$\\
% % & \tcl{~~$\FF{ana}(\sigma; \sigma; \tvar{x}.\sigma)$} & \\
% % & \tcl{~~$\FF{syn}(\sigma; \tvar{x}, \tvar{t}.\sigma)$} & \\
% %\\
% %\textbf{internal language} & $\tri ::= $ & $\trt ::= $ & $\iota ::= $ & $\tau ::=$\\
% %splice & ~~$\iup{\sigma}$ & ~~$\dup{\sigma}$ & \\
% %variables & ~~$x$ &  & ~~$x$ & \\
% %%fixpoints & ~~$\FF{fix}~x{:}\trt~\FF{is}~\tri$ & & ~~$\FF{fix}~x{:}\tau~\FF{is}~\iota$\\
% %%functions & ~~$\lambda x{:}\trt.\tri$ & ~~$\trt \rightharpoonup \trt$ & ~~$\lambda x{:}\tau.\iota$ & ~~$\tau \rightharpoonup \tau$\\
% %% & ~~$\tri~\tri$ & ~ & ~~$\iota~\iota$\\
% %%products & ~~$()$ & ~~$\dunit$ & ~~$()$ & ~~$\dunit$\\
% %% & ~~$(\tri, \tri)$ & ~~$\dpair{\trt}{\trt}$ & ~~$(\iota, \iota)$ & ~~$\dpair{\tau}{\tau}$\\
% %% & ~~$\tri.\FF{prl}$ & ~ & ~~$\tri.\FF{prl}$ & ~\\
% %%  & ~~$\tri.\FF{prr}$ & ~ & ~~$\tri.\FF{prr}$ & ~\\
% %%%products & ~~$(\overline{l=\tri})$ & ~~$\Pi[\overline{(l, \trt)}]$ & ~~$(\overline{l = \iota})$ & ~~$\Pi[\overline{(l, \tau)}]$\\
% %%% & ~~$\tri\cdot l$ & ~ & ~~$\iota\cdot l$ & ~\\
% %%sums & ~~$\FF{in}[\ell](\tri)$ & ~~$\Sigma[\{\overline{\ell \hookrightarrow \trt}\}]$ & ~~$\FF{in}[\ell](\iota)$ & ~~$\Sigma[\{\overline{\ell \hookrightarrow \tau}\}]$\\
% %% & ~~$\FF{case}~ \tri ~\{\overline{\ell(x) \Rightarrow \tri}\}$ & ~ & ~~$\FF{case}~\iota~\{\overline{\ell(x)\Rightarrow \iota}\}$ & ~\\
% %%integers & ~~$\FF{int}[z]$ & ~~$\FF{int}$ & ~~$\FF{int}[z]$ & ~~$\FF{int}$\\
% %% & ~~$\tri \pm \tri$ & ~ & ~~$\iota \pm \iota$ & ~\\
% %% & ~~$\FF{if0}(\tri; \tri; \tri; \tri)$ & ~ & ~~$\FF{if0}(\iota; \iota; \iota; \iota)$ & ~
% %% %& ~~$
% %\end{tabular}
% %\caption{Syntax from the perspective of fragment providers also includes an internal language and static language.}
% %\end{figure*}
% %\begin{figure*}
% %\small
% %\begin{tabular}{r l l}
% %external typing context & $\Gamma ::= \emptyset \pipe \Gamma, x \Rightarrow \ttype{tycon}{\hat\sigma}$\\
% %\\
% % & \textbf{initial} & \textbf{statically normal} \\
% %\textbf{static language, cont.} & $\sigma ::= ...$ & $\hat\sigma ::= ...$\\
% %argument intro & ~~$\FF{arg}(\Gamma; \hat e)$ & ~~$\FF{arg}(\Gamma; \hat e)$
% %\end{tabular}
% %\caption{Syntax from the perspective of the semantics / compiler.}
% %\end{figure*}
% \newcommand{\fkctx}{\mathcal{F}}
% \newcommand{\ok}[1]{#1~\mathtt{ok}}
% \newcommand{\normalize}[3]{#1 \Downarrow_{#2} #3}
% \newcommand{\normalizeX}[2]{\normalize{#1}{\Phi}{#2}}
% \newcommand{\serr}[1]{#1~\mathtt{tyerr}}
% %\begin{figure*}
% %\small
% %\begin{tabular}{r l l}
% %\textbf{context} & \textbf{syntax} & \textbf{well-formedness}\\
% %fragment kind context & $\fkctx ::= \fkctx_0 \pipe \fkctx, \fvar{tycon}[\kappa]~\{\FF{intro}[\kappa]; \FF{elim}[\kappa]\}$ & $\vdash \fkctx$\\
% %kinding context & $\Delta ::= \Delta \pipe \Delta, \tvar{x} : \kappa$\\
% %fragment context & $\Phi ::= \Phi_0 \pipe \Phi, \fvar{tycon}~\{\hat\delta\}$ & $\vdash \Phi \sim \fkctx$\\
% %external typing context & $\Gamma ::= \emptyset \pipe \Gamma, x \Rightarrow \hat\sigma$ & $\vdash_\Phi \Gamma$\\
% %internal typing context & $\Omega ::= \emptyset \pipe \Omega, x : \tau$\\
% %~
% %\end{tabular}
% %\\
% %\\
% %\begin{tabular}{l l l l l}
% %\textbf{kinding}& \multicolumn{2}{l}{\textbf{static normalization}} & \multicolumn{2}{l}{\textbf{program compilation}}\\
% %$\ok{\rho}$ & $\normalize{\rho}{}{\hat\rho}$ & $\serr{\rho}$ & $\hat\rho \leadsto \iota$\\
% %$\Delta \vdash_\fkctx {\phi}\sim\fkctx$ & $\normalizeX{\phi}{\hat\phi}$ & $\serr{\phi}$ & $\hat\phi \sim \Phi$\\
% %$\Delta \vdash_\fkctx \delta\sim\{\kappa; \kappa\}$ & $\normalizeX{\delta}{\hat\delta}$ & $\serr{\delta}$ & \multicolumn{2}{l}{\textbf{active typechecking and translation}}\\
% %$\Delta \vdash_\fkctx \sigma : \kappa$ & $\normalizeX{\sigma}{\hat \sigma}$ & $\serr{\sigma}$ & $\Gamma \vdash_\Phi \hat e \Leftarrow \hat\sigma \leadsto \iota$ & $\Gamma \vdash_\Phi \hat e \Rightarrow \hat\sigma \leadsto \iota$\\
% %$\Delta \vdash_\fkctx \ok{e}$ & $\normalizeX{e}{\hat e}$ & $\serr{e}$ & $\vdash_\Phi \hat\sigma \leadsto \tau$ & $\vdash_\Phi \Gamma \leadsto \Omega$\\
% %$\Delta \vdash_\fkctx \ok{\tri}$ & $\normalizeX{\tri}{\iota}$ & $\serr{\tri}$ & \multicolumn{1}{l}{\textbf{internal statics}} & \textbf{internal dynamics}\\
% %$\Delta \vdash_\fkctx \ok{\trt}$ & $\normalizeX{\trt}{\tau}$ & $\serr{\trt}$ & $\Omega \vdash \iota : \tau$ & $\iota \mapsto \iota$~~~~~~~~$\iota~\mathtt{val}$
% %\end{tabular}
% %\caption{Summary of judgements}
% %\end{figure*}

% %
% %\begin{figure*}
% %\small
% %$
% %\begin{array}{lcl}
% %\delta_{\texttt{nat}} & := & \quad \FF{iana}~\tlam{opidx}{\kunit+\kunit}{\tlam{tyidx}{\kunit}{\tlam{args}{\klist{\FF{Arg}}}{
% % \tsumcase{\tvar{opidx}\\
% %&&\quad\quad}{\_}{\tvar{isnil}~\tvar{args}~\titerm{0}\\
% %&&\quad\quad}{\_}{\tvar{decons1}~\tvar{args}~\tlam{a}{\FF{Arg}}{\FF{ana}(\tvar{a}; \ttype{nat}{\tunit}; \tvar{x}.\titerm{\iup{\tvar{x}}+1})}}
% %}}}\\
% %&&\quad \FF{esyn}~\tlam{opidx}{\kunit}{\tlam{tyidx}{\kunit}{\tlam{x}{\kITerm}{\tlam{args}{\klist{\FF{Arg}}}{\tvar{decons2}~\tvar{args}~\tlam{a1}{\FF{Arg}}{\tlam{a2}{\FF{Arg}}{\\
% %&&\quad\quad \FF{syn}(\tvar{a1}; \tvar{t1}, \tvar{i1}.\\
% %&&\quad\quad\quad \F{let}\tvar{t2} : \kTypeBlur = \ttype{arrow}{(\ttype{nat}{\tunit}, \ttype{arrow}{(\tvar{t1}, \tvar{t1})})}~\F{in} \\
% %&&\quad\quad\quad \FF{ana}(\tvar{a2}; \tvar{t2}; \tvar{i2}.(\tvar{t1},\\
% %&&\quad\quad\quad\quad \titerm{\iapp{(\ifix{f}{\darrow{\dint}{\dup{\trepof{\tvar{t2}}}}}{\ilam{x}{\dint}{\\&&\quad\quad\quad\quad\quad\quad \FF{if0}(x; \iup{\tvar{i1}}; {
% %   %\\&\quad\quad\quad\quad\quad\quad
% %   \iapp{\iup{\tvar{i2}}~(x-1)}{(\iapp{f}{(x-1)})}})}}\\
% %   &&\quad\quad\quad\quad\quad)}{\iup{\tvar{x}}}})))}}}}}}\\
% %&&\quad \FF{rep}~\tlam{tyidx}{\kunit}{\titype{\dint}}\\
% %\end{array}
% %$
% %\caption{\small The definition of the nat type constronstructor.}
% %\end{figure*}
% %\begin{figure}
% %\small
% %\begin{flalign}
% %& \F{tycon}\fvar{record}~\F{of}\klist{\kpair{\FF{L}}{\kTypeBlur}}~\{\\
% %& \quad \FF{iana}~\{\tlam{i}{\kpair{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{\klist{\FF{L}}}}{\tlam{a}{\klist{\Q}}{\\& \quad\quad\tfold{\tvar{zip3}~\tfst{\tvar{i}}~\tsnd{\tvar{i}}~\tvar{a}}
% %  {\titerm{\tunit}}{h}{t}{r}{
% % \\&\quad\quad\quad\tifeq{\tfst{\tvar{first}~\tvar{h}}}{\tvar{second}~\tvar{h}}{\FF{L}}{
% % \\&\quad\quad\quad\quad
% % \F{let}~\tvar{x}:\kITerm~=~\FF{ana}(\tvar{third}~\tvar{h}; \tsnd{\tvar{first}~\tvar{h}})
% %~\FF{in}
% %\\ & \quad\quad\quad\quad \tfold{\tvar{t}}{\tvar{x}
% %}{\_}{\_}{\_}{\titerm{(\iup{\tvar{x}},\iup{\tvar{r}})}}  }{\terr}
% %%     \F{let}~\tvar{tt}:\FF{TT}=~\FF{syn}(\tsnd{\tsnd{\tvar{h}}}) \F{in}\\
% %%\\     & \quad\quad\quad 
% %}
% %}}\}\\
% %& \quad \FF{isyn}~\{\tlam{i}{\klist{\FF{L}}}{\tlam{a}{\klist{\Q}}{
% %\\&\quad\quad \tfold{\tvar{zip2}~\tvar{i}~\tvar{a}}{\tden{\titerm{()}}{\ttype{record}{\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}}}{h}{t}{r}{
% %\\&\quad\quad\quad \F{let}~\tvar{htt}:\FF{TT}~=~\FF{syn}(\tsnd{\tvar{h}})
% %~\FF{in}
% %\\&\quad\quad\quad \F{let}~\tvar{hty}:\kTypeBlur~=~\ttypeof{\tvar{htt}}
% %\\&\quad\quad\quad \F{let}~\tvar{ty}:\kTypeBlur~=~\ttype{record}{(\tfst{\tvar{h}}, \tvar{hty})::\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}
% %~\FF{in}
% %\\&\quad\quad\quad \tfold{\tvar t}{\tden{\itransof{\tvar{htt}}}{\tvar{ty}}}{\_}{\_}{\_}{
% %\\&\quad\quad\quad\quad \tden{\titerm{(\iup{\itransof{\tvar{htt}}},\iup{\itransof{\tvar{r}}})}}{\tvar{ty}}}}
% %}}\}\\
% %& \quad \FF{esyn}~\{\tlam{i}{\FF{L}}{\tlam{a}{\klist{\Q}}{
% % \tvar{decons1}~\tvar{a}~\tlam{ty}{\kTypeBlur}{\tlam{x}{\kITerm}{
% %\\&\quad\quad    \tfamcase{\tvar{ty}}{record}{sig}{
% %\\&\quad\quad\quad \tfold{\tvar{sig}}{\terr}{h}{t}{r}{
% %\\&\quad\quad\quad\quad \tifeq{\tfst{\tvar{h}}}{\tvar{i}}{\FF{L}}{\tfold{\tvar{t}}{\tden{\tvar{x}}{\tsnd{\tvar{h}}}}{\_}{t'}{\_}{
% %\\&\quad\quad\quad\quad\quad \tfold{\tvar{t'}}{\tden{\titerm{\ifst{\iup{\tvar{x}}}}}{\tsnd{\tvar{h}}}}{\_}{\_}{\tvar{r'}}{
% %\\&\quad\quad\quad\quad\quad\quad \tden{\titerm{\isnd{\iup{{\itransof{\tvar r}}}}}}{\tsnd{\tvar{h}}}}}\\&\quad\quad\quad\quad\hspace{-3px}}{\tvar{r}}
% %}\\&\quad\quad\hspace{-3px}}{\terr}
% % }}
% %}}\\
% %& \quad \FF{rep}~\{\tlam{i}{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{   \\& \quad \quad \tfold{\tvar{i}}{\titype{\dunit}}{s}{j}{r}{
% %     \tfold{\tvar{j}}{\trepof{\tsnd{\tvar{s}}}}{\_}{\_}{\_}{\\
% %   &\quad\quad\quad
% %     \titype{\dpair{\dup{\trepof{\tsnd{\tvar{s}}}}}{\dup{\tvar{r}}}}
% %     }
% %   }}\}\\
% %& \}; \\
% %& \F{let}\tvar{R} : \kTypeBlur = \ttype{record}{(\ell_1, \ttype{record}{\tnil{{\FF{L}\times\kTypeBlur}}}) :: \tnil{{\FF{L}\times\kTypeBlur}}}~\F{in}\\
% %&\F{let}id = \elam{x}{\tvar{R}}{\{\ell_1=~x\cdot\ell_1\} :: \fvar{record}}\\
% %&\F{let}triv = id[\{\ell_1 = \{\}\}]\cdot\ell_1
% %%&\F{let}one = \FF{intro}[\ell_\text{succ}]
% %%&\F{let}plus = \elam{x}{\tvar{nat}}{\elam{y}{\tvar{nat}}{\FF{elim}[()](x; y; \\
% %%& \quad \elam{r}{\tvar{nat}}{intro[\ell_\text{succ}](r) : \tvar{nat}}})}
% %\end{flalign}
% %\caption{The definition of the record type in $\lam\texttt{typy}$ using the desugarings in Figure \ref{desugaring} and with the addition of simple let bindings for both type and term variables (not shown). Some type-level helper functions also omitted for concision.}
% %\label{record-theory}
% %\end{figure}


% %These correspond to the premises of the \emph{central compilation judgement} $\pkcompiles{\rho}{\iota}$:
% %\[
% %\inferrule[p-compiles]{
% % \emptyset \vdash_{\fvalCtx_0} \rho\\
% %%  \progOK{\emptyset}{\fvalCtx_0}{\rho}\\
% % \pcompiles{\fvalCtx_0}{\rho}{\iota}
% %}{\pkcompiles{\rho}{\iota}}
% %\]
% %This anchors our exposition; we will describe how it is derived (i.e. how to write a compiler for $\lam\texttt{typy}$) in the following sections. 
% The static semantics are specified by the \emph{bidirectional active typechecking and translation judgements} shown in Fig. \ref{atj}, which relate an expression, $e$, to a {type}, $\sigma$, and a \emph{translation}, $\iota$, under \emph{typing context} $\Gamma$ using imported {fragments} $\Phi$. The judgement form $\atjsynX{e}{\sigma}{\iota}$ specifies \emph{type synthesis} (the type is an ``output''), whereas $\atjanaX{e}{\sigma}{\iota}$ specifies \emph{type analysis} (the type is an ``input''). This can be seen as combining a bidirectional type system (in the style of Pierce and Turner \cite{Pierce:2000:LTI:345099.345100} and a number of subsequent formalisms and languages, e.g. Scala) with an elaboration semantics in the style of the Harper-Stone semantics for Standard ML \cite{Harper00atype-theoretic}. Our language of \emph{internal terms}, $\iota$, includes only functions and lists for simplicity. The form $\iup{\sigma}$ is used as an ``unquote'' operator, and will appear only in intermediate portions of a typing derivation, not in a translation (discussed below).

% The first two rows of rules in Fig. \ref{atj} are essentially standard. \rulename{att-subsume} specifies the subsumption principle described in the previous section: if a type can be synthesized, then the term can also be analyzed against that type. We decide type equality purely syntactically here. \rulename{att-var} specifies that variables always synthesize types and elaborate identically. The typing context, $\Gamma$, maps variables to types in essentially the conventional way \cite{pfpl}. The rules $\rulename{att-ana-let}$ and $\rulename{att-syn-let}$ first synthesize a type for the bound value, then add this binding to the context and analyze or synthesize the body of the binding. The translation is to an internal function application, in the conventional manner. $\rulename{att-asc}$ begins by normalizing the provided index and checking that it is a type (Fig. \ref{tleval}, top). If so, it analyzes the ascribed expression against that type. The rules $\rulename{att-ana-slet}$ and $\rulename{att-syn-slet}$ eagerly evaluate the provided static term to a static value, then immediately perform the substitution (demonstrating the phase separation) in the expression before analysis or synthesis proceeds.%The normalization judgement for type-level terms is written $\tau \Downarrow_{\Gamma;\fvalCtx} \tau'$. The kinding rules (not shown here) guarantee that normalization of type-level terms cannot go wrong (we will refine what precisely this means later).

% \paragraph{Lambdas}
% The rule \rulename{att-ana-lam} performs type analysis on a lambda abstraction, $\lambda(x.e)$. If it succeeds, the translation is the corresponding lambda in the internal language. The type constructor $\textsc{fn}$ is included implicitly in $\Phi$ and must have a type index consisting of a pair of types (pairs are here just lists of length 2 for simplicity). Because both the argument type and return type are known, the body of the lambda is analyzed against the return type after extending the context with the argument type. This is thus the usual type analysis rule for functions in a bidirectional setting.  

% The rule \rulename{att-syn-idx-lam} covers the case where a lambda abstraction has an incomplete type ascription   providing only the argument type. This corresponds to the concrete syntax seen in the definition of $plus$ in Fig. \ref{atlam-example}. Here, the return type must be synthesized by the body. 

% These two rules can be compared to the rules in Listing \ref{fn-top}. The main difference is that in @$\lambda$, the language itself manages variables and contexts, rather than the type constructor. This is largely for simplicity, though it does limit us in that we cannot define function type constructors that require alternative or additional contexts. Addressing this in the theory is one avenue for future work. 

% Function application can be defined directly as a targeted expression, as seen in the example, which we will discuss below.
% \paragraph{Fragment Provider Perspective}
% \begin{figure}[t]
% \small
% \centering
% $
% \begin{array}{l}
% \Phi_\text{fn} := \fvar{fn}=\{\FF{analit}=\FF{nil}; \FF{synidxlit}=\FF{nil}; \FF{anatarg}=\FF{nil}; \\
% ~\quad \FF{syntarg}=\lambda \tvar{tyidx}.\lambda \tvar{ifn}.\lambda \tvar{tmidx}.\lambda \tvar{args}.\\ 
% \quad\quad  \tvar{isnil}~\tvar{tmidx}~(\tvar{decons1}~\tvar{args}~\lambda \tvar{arg}. \tvar{decons2} ~\tvar{tyidx}~\lambda \tvar{inty}.\lambda \tvar{rty}.\\
% \quad\quad\quad  \FF{ana}(\tvar{arg}; \tvar{inty}; \tvar{ia}.\tvar{pair}~\tvar{rty}~\titerm{\FF{iap}(\iup{\tvar{ifn}};\iup{\tvar{ia}})}))\}
% \end{array}
% $
% \caption{The \textsc{fn} fragment defines function application.}
% \label{fn-tycon}
% \vspace{10px}
% \end{figure}
% \begin{figure}[t]
% \small
% \centering
% $
% \begin{array}{l}
% \Phi_\text{nat} := \fvar{nat}=\{\FF{analit}=\lambda \tvar{tyidx}.\lambda \tvar{tmidx}.\lambda \tvar{args}.\\ 
% \quad\quad\tvar{isnil}~\tvar{tyidx}~(\tvar{lbleq}~\tvar{tmidx}~\tvar{lbl}[\texttt{0}]~(\tvar{isnil}~\tvar{args}~ \titerm{\tvar{inil}}));\\
% \quad  \FF{synidxlit}=\lambda \tvar{incidx}.\lambda \tvar{tmidx}.\lambda \tvar{args}.\\
% \quad\quad \tvar{isnil}~\tvar{incidx}~(\tvar{lbleq}~\tvar{tmidx}~\tvar{lbl}[\texttt{0}]~(\tvar{isnil} ~\tvar{args}~(\tvar{pair}~\FF{nil}~\titerm{\tvar{inil}})));\\
% \quad  \FF{anatarg}=\lambda \tvar{tyidx}.\lambda \tvar{i1}.\lambda \tvar{ty}.\lambda \tvar{tmidx}.\lambda \tvar{args}.\\
%   \quad\quad \tvar{lbleq}~\tvar{tmidx}~\FF{lbl}[\texttt{rec}]~(\tvar{decons2}~\tvar{args}~
%     \lambda \tvar{arg1}.\lambda \tvar{arg2}.\\
%     \quad \quad\quad  \FF{ana}(\tvar{arg1}; \tvar{ty}; \tvar{i2}.\FF{ana}(\tvar{arg2}; \tvar{fn2ty}~\FF{ty}[\fvar{nat}](\FF{nil})~\tvar{ty}~\tvar{ty}; \tvar{i3}.\\
%     \quad\quad\quad\quad \titerm{\FF{ilistrec}(\iup{\tvar{i1}}; \iup{\tvar{i2}}; x, y.\FF{iap}(\FF{iap}(\iup{\tvar{i3}}; x); y))})));\\
% \quad \FF{syntarg} = \lambda \tvar{tyidx}.\lambda \tvar{i1}.\lambda \tvar{tmidx}.\lambda \tvar{args}.\\
% \quad\quad \tvar{lbleq}~\tvar{tmidx}~\FF{lbl}[\texttt{s}]~(\tvar{isnil}~\tvar{args}~(\\
% \quad\quad\quad \tvar{pair}~\FF{ty}[\fvar{nat}](\tvar{nil})~\titerm{\FF{icons}(\FF{inil};\iup{\tvar{i1}})}))\}
% \end{array}
% $
% \caption{The $\fvar{nat}$ fragment, based on G\"odel's \textbf{T} \cite{pfpl}.}
% \vspace{10px}
% \label{nat-tycon}
% \end{figure}
% \begin{figure}[t]
% \small
% \centering
% $
% \begin{array}{l}
% \Phi_\text{lprod} := \fvar{lprod}=\{\FF{analit} = \lambda \tvar{tyidx}.\lambda \tvar{tmidx}.\lambda \tvar{args}. \\
% \quad\quad \FF{listrec}(\tvar{zipexact3}~\tvar{tyidx}~\tvar{tmidx}~\tvar{args}; \titerm{\FF{inil}}; \tvar{h}. \tvar{ri}.\\
% \quad\quad\quad \tvar{decons3} ~\tvar{h} ~\lambda \tvar{idxitem}.\lambda \tvar{lbl}.\lambda \tvar{e}.\tvar{decons2}~\tvar{idxitem}~\lambda \tvar{lblidx}. \lambda \tvar{tyidx}. \\
% \quad\quad\quad\quad \tvar{lbleq}~\tvar{lbl}~\tvar{lblidx}~(\FF{ana}(\tvar{e}; \tvar{tyidx}; \tvar{i}.\titerm{\FF{icons}(\iup{\tvar{i}}, \iup{\tvar{ri}})})));\\
% \quad\FF{synidxlit} = \lambda \tvar{incidx}.\lambda \tvar{tmidx}.\lambda \tvar{args}.\\
% \quad\quad \FF{listrec}(\tvar{zipexact2}~\tvar{tmidx}~\tvar{args}; \tvar{pair} ~\FF{nil}~ \titerm{\FF{inil}}; \tvar{h}. \tvar{r}.\\
% \quad\quad\quad \tvar{decons2} ~\tvar{h}~\lambda \tvar{lbl}.\lambda \tvar{e}.\tvar{decons2}~\tvar{h}~\lambda \tvar{ridx}.\lambda \tvar{ri}.\FF{syn}(\tvar{e};  \tvar{ty}. \tvar{i}. \\
% \quad\quad\quad\quad \tvar{pair}~\FF{cons}(\tvar{pair}~\tvar{lbl}~ \tvar{ty}; \tvar{ridx})~\titerm{\FF{icons}(\iup{\tvar{i}}; \iup{\tvar{ri}})}));\\
% \quad \FF{anatarg}=\FF{nil}; \text{\color{gray} (destructuring let could be implemented here)}\\ % 
% \quad \FF{syntarg} = \lambda \tvar{tyidx}.\lambda \tvar{i}.\lambda \tvar{lbl}.\lambda \tvar{args}.\\
% \quad\quad \tvar{isnil}~\tvar{args}~(\tvar{pair} ~(\tvar{lookup}~\tvar{lbl}~\tvar{tyidx})~\\
% \quad\quad\quad \titerm{\FF{iap}(\FF{iap}(nth;\iup{\tvar{i}}); \iup{\tvar{itermofn} ~(\tvar{posof}~\tvar{lbl}~\tvar{tyidx})})})
% \end{array}
% $
% \caption{The $\fvar{lprod}$ fragment (labeled products are like records, but the field order matters; cf. Listing \ref{record}).}
% \label{lprod-tycon}
% \vspace{10px}
% \end{figure}
% \begin{figure}[t]
% \small
% $\fbox{\inferrule{}{\sigma~\mathtt{ty}_\fvalCtx}}$
% \vspace{-20px}
% \renewcommand{\MathparLineskip}{\lineskiplimit=.6\baselineskip\lineskip=.6\baselineskip plus .2\baselineskip}
% \begin{mathpar}
% \inferrule[ty]{
%   \FF{ty}[\fvar{tycon}](\sigma) \Downarrow_{\emptyctx; \fvalCtx} \FF{ty}[\fvar{tycon}](\sigma)
% }{\FF{ty}[\fvar{tycon}](\sigma)~\mathtt{ty}_\fvalCtx}
% \end{mathpar}
% $\fbox{\inferrule{}{\sigma \Downarrow_{\Gamma;\fvalCtx} \sigma}}$~
% %$\fbox{\inferrule{}{\sigma \Downarrow_{\Gamma;\fvalCtx}^= \sigma}}$~
% \vspace{-15px}
% \begin{mathpar}
% %\inferrule[repof]{
% % \tau \Downarrow_{\Gamma;\fvalCtx} \ttype{tycon}{\tauidx}\\
% % \vdash_\fvalCtx \FF{rep}(\fvar{tycon}) = \taurep\\
% % \taurep~\tauidx \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}
% %}{
% % \FF{repof}(\tau) \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}
% %}
% %
% \inferrule[n-ty]{
%   \fvar{tycon} \in \text{dom}(\fvalCtx)\\
%   \sigma \Downarrow_{\Gamma;\fvalCtx} \sigma'
% }{\FF{ty}[\fvar{tycon}](\sigma)~\Downarrow_{\Gamma;\fvalCtx} \FF{ty}[\fvar{tycon}](\sigma')}
% \\
% \inferrule[n-arg]{ }{\FF{arg}[e]~\Downarrow_{\Gamma;\fvalCtx} \FF{arg}[e]}

% \inferrule[n-ana]{
%   \sigma_1 \Downarrow_{\Gamma;\fvalCtx} \FF{arg}[e]\\
%   \sigma_2 \Downarrow_{\Gamma;\fvalCtx} \sigmat{ty}\\
%   \sigmat{ty}~\mathtt{ty}_\fvalCtx\\\\
%   \atjanaX{e}{\sigmat{ty}}{\iota}\\
%   [\titerm{\iota}/\tvar{x}]\sigma_3 \Downarrow_{\Gamma;\fvalCtx} \sigma_3'
% }{
%   \FF{ana}(\sigma_1; \sigma_2; \tvar{x}.\sigma_3) \Downarrow_{\Gamma;\fvalCtx} \sigma_3'
% }

% \inferrule[n-syn]{
%     \sigma_1 \Downarrow_{\Gamma;\fvalCtx} \FF{arg}[e]\\
%   \atjsynX{e}{\sigma}{\iota}\\\\
%     [\sigma/\tvar{x}_{1}, \titerm{\iota}/\tvar{x}_{2}]\sigma_2 \Downarrow_{\Gamma;\fvalCtx} \sigma_2'
% }{
%   \FF{syn}(\sigma_1; \tvar{x}_{1}.\tvar{x}_{2}.\sigma_2) \Downarrow_{\Gamma;\fvalCtx} \sigma_2'
% }

% \inferrule[n-quote]{
%   \iota \curlyveedownarrow_{\Gamma;\fvalCtx} \iota'
% }{\titerm{\iota} \Downarrow_{\Gamma;\fvalCtx} \titerm{\iota'}}
% \end{mathpar}
% $\fbox{\inferrule{}{\iota \curlyveedownarrow_{\Gamma;\fvalCtx} \iota}}$
% \vspace{-15px}
% \begin{mathpar}


% \inferrule[q-uq]{\sigma \Downarrow_{\Gamma;\fvalCtx} \titerm{\iota}}{\iup{\sigma} \curlyveedownarrow_{\Gamma;\fvalCtx} \iota}

% \inferrule[q-x]{ }{x \curlyveedownarrow_{\Gamma;\fvalCtx} x}

% \inferrule[q-lam]{\iota \curlyveedownarrow_{\Gamma;\fvalCtx} \iota'}
% {\lambda(x.\iota) \curlyveedownarrow_{\Gamma;\fvalCtx} \lambda (x.\iota')}
% %
% %\inferrule[q-ap]{\iota_1 \curlyveedownarrow_{\Gamma;\fvalCtx} \iota_1'\\
% %\iota_2 \curlyveedownarrow_{\Gamma;\fvalCtx} \iota_2'}{
% %\FF{iap}(\iota_1; \iota_2) \curlyveedownarrow_{\Gamma;\fvalCtx} \FF{iap}(\iota_1'; \iota_2')}
% \end{mathpar}
% \caption{Selected normalization rules for the static language.}
% \vspace{10px}
% \label{tleval}
% \end{figure}

% Fragment providers define type constructors by defining ``methods'', $\delta$, that control analysis and synthesis literals and targeted expressions. These are static functions invoked by the final four rules in Fig. \ref{atj}, which we will describe next. The type constructors $\fvar{fn}$, $\fvar{nat}$ and $\fvar{lprod}$ are shown in Figs. \ref{fn-tycon}-\ref{lprod-tycon}, respectively. They use helper functions for working with labels ($\tvar{lbleq}$), lists ($\tvar{isnil}$, $\tvar{decons1}$, $\tvar{decons2}$, $\tvar{decons3}$, $\tvar{zipexact2}$, $\tvar{zipexact3}$, and $\tvar{pair}$), lists of pairs interpreted as finite mappings ($\tvar{lookup}$ and $\tvar{posof}$) and translating a static representation of a number to an internal representation of that number ($\tvar{itermofn}$). We also assume an internal function $nth$ that retrieves the $n$th element of a list. All of these are standard or straightforward and omitted for concision. Failure cases for the static helper functions evaluate to $\FF{fail}$. Derivation of the typing judgement does not continue if a $\FF{fail}$ occurs (corresponding to an \lip{typy.TypeError} propagated directly to the compiler).

% \subparagraph{Literals} The rule $\rulename{att-ana-lit}$, invokes the $\FF{analit}$ method of the type constructor of the type the literal is being analyzed against, asking it to return a translation (a value of the form $\titerm{\iota}$). The type and term index are provided, as well as a list of \emph{reified arguments}: static values of the form $\FF{arg}[e]$. The $\fvar{fn}$ type constructor does not implement this (functions are introduced only via lambdas). The $\fvar{nat}$ type constructor implements this by checking that the term index was the label corresponding to $\texttt{0}$ and no arguments were provided. The $\fvar{lprod}$ type constructor is more interesting: it folds over each corresponding item in the type index (a pair consisting of a label and a type), the term index (a label) and the argument list, checking that the labels match and programmatically analyzing the argument against the type it should have. A reified argument $\sigma_1$ against a type $\sigma_2$, binding the translation to $\tvar{x}$ in $\sigma_3$ if successful (and failing otherwise) with the static term $\FF{ana(\sigma_1; \sigma_2; \tvar{x}.\sigma_3)}$, the semantics of which are in Fig. \ref{tleval}. Labeled products translate to lists by recursively composing the translations of the field values using the ``unquote'' form, $\iup{\sigma}$, which is eliminated during normalization (also seen in Fig. \ref{tleval}). This function can be compared to the method \lip{ana_Dict} in Listing \ref{record}.

% Literals with an incomplete type ascription can synthesize a type by rule \rulename{att-syn-idx-lit}. The $\FF{synidxlit}$ method of type constructor of the partial ascription is called with the incomplete type index, the term index and the arguments as above, and must return a pair consisting of the complete type index and the translation. Again, $\fvar{fn}$ does not implement this. The $\fvar{nat}$ type constructor supports it, though it is not particularly interesting, as $\fvar{nat}$ is always indexed trivially, so it follows essentially the same logic as in $\FF{analit}$. The $\fvar{lprod}$ type constructor is more interesting: in this case, when the incomplete type index is trivial (as in the example in Fig. \ref{atlam-example}), the list of pairs of labels and types must be synthesized from the literal itself. This is done by programatically synthesizing a type and translation for a reified argument using $\FF{syn(\sigma_1; \tvar{x}.\tvar{y}.\sigma_2)}$, also specified in Fig. \ref{tleval}. The type index and translation are recursively formed. This can be compared to the class method \lip{syn_idx_Dict} in Listing \ref{record}.

% \subparagraph{Targeted Terms} Targeted terms are written $\FF{targ}[\sigmat{tmidx}](e_\text{targ}; \overline{e})$. The type constructor of the type recursively synthesized by the \emph{target}, $e_\text{targ}$, is delegated control over analysis and synthesis via the methods $\FF{anatarg}$ and $\FF{syntarg}$, respectively, as seen in rules \rulename{att-ana-targ} and \rulename{att-syn-targ}. Both receive the type index, the translation of the target, the term index and the reified arguments. The former also receives the type being analyzed and only needs to produce a translation. The latter must produce a pair consisting of a type and a translation.

% The $\fvar{fn}$ type constructor defines function application by straightforwardly implementing $\FF{syntarg}$.  Because of subsumption, $\FF{anatarg}$ need not be separately defined.

% The $\fvar{nat}$ type constructor defines the successor operation synthetically and the recursor operation analytically (because it has two branches that must have the same type). The latter analyzes the second argument against a function type, avoiding the need to handle binding itself. Natural numbers translate to lists, so the $\FF{nat}$ recursor can be implemented straightforwardly using the list recursor. In a practical implementation, we might translate natural numbers to integers and use a fixpoint computation instead.

% The $\fvar{lprod}$ type constructor defines the projection operation synthetically, using the helper functions mentioned above to lookup the appropriate item in the type index. Note that one might also define an analytic targeted operation on labeled products corresponding to pattern matching, e.g. $\FF{let}~\{\texttt{a}{=}x, \texttt{b}{=}y\}=r~\FF{in}~e$. \verb|typy| supports this using Python's syntax for destructuring assignment, but we must omit the details.

% \paragraph{Metatheory} The formulation shown here guarantees that type synthesis actually produces a type, given well-formed contexts. The definitions are straightforward and the proof is a simple induction. We write $\fvalCtx~\mathtt{fragment}$ for fragments with no duplicate tycon names and closed tycon definitions only, and $\Gamma~\mathtt{ctx}_\fvalCtx$ for typing contexts that only map variables to types constructed with tycons in $\fvalCtx$. %We plan to prove the full proofs in a companion tech report (available upon request).

% \begin{theorem}[Synthesis]
% If $\fvalCtx~\mathtt{fragment}$ and $\Gamma~\mathtt{ctx}_\fvalCtx$ and $\atjsynX{e}{\sigma}{\iota}$ then $\sigma~\mathtt{ty}_\fvalCtx$.
% \end{theorem}

% We also have that importing additional type constructors cannot change the semantics of a previously well-typed term, assuming that naming conflicts have been resolved by some extrinsic mechanism. Indeed, the proof is an essentially trivial induction because of the way we have structured our mechanism. The type constructor delegated responsibility over a term is deterministically determined irrespective of the structure of $\fvalCtx$. %We plan to provide full proofs of these theorems in a technical report. 

% \begin{theorem}[Stable Extension]
% If $\fvalCtx~\mathtt{fragment}$ and $\Gamma~\mathtt{ctx}_\fvalCtx$ and $\atjsynX{e}{\sigma}{\iota}$ and $\fvalCtx'~\mathtt{fragment}$ and $\text{dom}(\fvalCtx) \cap \text{dom}(\fvalCtx') = \emptyset$ then ${\Gamma}\vdash_{\fvalCtx;\fvalCtx'}{e}\Rightarrow{\sigma}\leadsto{\iota}$.
% \end{theorem}

% Other metatheoretic guarantees about the translation cannot be provided in the formulation as given. However, inserting straightforward checks to guarantee that the translation is a closed term, and provide simple mechanisms for hygiene, would be simple, but are omitted to keep our focus on the basic structure of the calculus as a descriptive artifact. 
% %\noindent
% %\subsubsection{Rules} We briefly describe each rule and relevant additional judgements below:
% %
% %\noindent
% %The rule $\rulename{att-lam}$ says that lambda terms can only be analyzed against arrow types and translate to lambda terms in the internal language.
% %
% %The rule $\textsc{\textbf{att-intro-ana}}$ says that generalized introductory forms can only be analyzed against a type. That type constructor is consulted to extract its \emph{introductory operator definition}, written (e.g. in Figure \ref{nat-atfrag}) after the $\FF{iana}$ keyword as a type-level function. This function is given the provided operator index, the type's type index and a list encapsulating the operator's  \emph{arguments}. An argument is reified as a type-level term of the form $(\Gamma, e)?$ and has kind $\FF{Arg}$. Note that only the compiler should construct such a term (in practice, this could be enforced purely syntactically, so we do not enforce this judgmentally here). The operator definition is responsible for producing a translation, or evaluating to $\terr$ if this is not possible. A translation is simply a quoted internal term, written $\titerm{\iota}$, with kind $\kITerm$. Like quoted internal types, quoted internal terms support an unquote form, written $\iup{\tau}$, which is normalized away (in a capture-avoiding manner, not shown).
% %
% %Natural numbers have two introductory forms, each indexed trivially. Because there is only one generalized introductory form, we instead index the operator by a simple sum kind with two trivial cases, $\kunit + \kunit$. The first case corresponds to the zero operator and the second to the successor. In the introductory operator definition, we see in the first case that the translation $0$ is produced after checking that no arguments were provided (using a simple helper function, $\tvar{isnil}$, not shown). In the second case, we first extract the single argument (using the helper function $\tvar{decons1}$, not shown). We then analyze it against the type $\ttype{nat}{\tunit}$ using one of the elimination forms for arguments, $\FF{ana}(\tau_1; \tau_2; \tvar{t}_{trans}.\tau_3)$. If it succeeds, we construct the appropriate translation based on the translation of the argument. The normalization rule for successful analysis, $\textsc{ana}$, is shown in Figure \ref{tleval}. 
% %
% %Note also that lambda is technically the introductory form for the $\fvar{arrow}$ type constructor, but because it requires manipulating the context, it must be built in to @$\lambda$. We do not currently support type system fragments that require adding or manipulating existing contexts.
% %
% %The rule $\rulename{att-elim-syn}$ says that elimination forms synthesize a type. This is done again by consulting a type-level function, called the \emph{eliminatory operator definition}, associated with a type constructor, here the type constructor of the type  recursively synthesized for the primary operand, $e$. This definition is invoked with the type index, the operator index, the translation of the primary operand and a list of arguments. Because elimination forms synthesize a type, this definition must produce both a type and a translation. Representational consistency is then checked.
% %
% %Note that the elimination form for the $\fvar{arrow}$ type constructor can be defined in this manner (because it does not require binding variables), so application is simply syntactic sugar, rather than a built-in operator in the external language.
% %
% %The constructor context, $\Phi$, tracks user-defined type  constructors introduced in the ``imported'' fragments. %Note that fragments can also export type-level values bound to type-level variables (written in bold font, e.g. $\tvar{nat}$, $\tvar{s}$, $\tvar{z}$, etc.), but substitution will be performed prior to typechecking, so they need not be tracked by $\Phi$. % Weakening and exchange of the constructor context closely related to the issue of conservativity that we will return to. Each constructor is identified by name, so there is no analog to contraction. 
% % This form of semantics can be seen as lifting into the language specification the first stage of a type-directed compiler like the TIL compiler for Standard ML \cite{tarditi+:til-OLD} and has some parallels to the Harper-Stone semantics for Standard ML \cite{Harper00atype-theoretic}. There, as in Wyvern, external terms were given meaning by elaboration from the EL to an IL having the same type system. Here the two languages have different type systems, so we call it a translation rather than an elaboration (arranging the judgement form slightly differently to emphasize this distinction, cf. above).

% %In @$\lambda$, the internal language (IL) provides partial functions (via the generic fixpoint operator of Plotkin's PCF), simple product  types and integers for the sake of our example (and as a nod toward practicality on contemporary machines). In practice, the internal language could be any typed  language with a specification for which type safety and decidability of typechecking have been satisfyingly determined. In Sec. \ref{ace}, we will see how the internal language can itself be made user-definable. The internal type system serves as a ``floor'': guarantees that must hold for terms of any type (e.g. that out-of-bounds access to memory never occurs) must be maintained by the internal type system. User-defined constructors can enforce invariants stronger than those the internal type system maintains at particular types, however. Performance is also ultimately limited by the internal language and downstream compilation stages that we do not here consider (safe compiler extension has been discussed in previous work, e.g. \cite{conf/pldi/TatlockL10}).
% %
% %The external language has a fixed syntax with six forms: variables, $\FF{let}$-bindings, lambda terms, type ascription and generalized introductory and elimination forms. As we will see, the generalized introductory form is given meaning by the type it is analyzed against (similar to the protocol for TSLs in Wyvern), and the elimination form is given meaning by the type of the external term being eliminated ($e$). This represents an internalization into the language of Gentzen's inversion principle \cite{gentzen}\todo{what to cite for this?}. Fig. \ref{desugaring} shows how to recover more conventional introductory and elimination forms by a purely syntactic desugaring.%\footnote{Note that for even more flexibility, we could also include TSLs, but we choose to avoid that for simplicity (and to show that, strictly speaking, one need not have an extensible syntax to have an extensible type system, and \textit{vice versa}).}
% %
% %\subsection{Types and Type-Level Computation}\label{types}
% %@$\lambda$ supports, and makes extensive use of, simply-kinded type-level computation. Specifically, type-level terms, $\tau$, themselves form a typed lambda calculus. The classifiers of type-level terms are called \emph{kinds}, $\kappa$, to distinguish them from  \emph{types}, which are  type-level values of kind $\kTypeBlur$. %In \texttt{typy}, the type-level language (together with the level of programs) is written in Python, which can be thought of as having a rather limited kind system (with one kind, \lip{dyn}). Here, we are able to more precisely discuss the kinds of values in the type-level language. 
% %As in Sec. \ref{att}, types are formed by applying a \emph{type constructor} to an \emph{index}. User-defined type constructors are declared in fragment definitions using $\FF{tycon}$. Each constructor in the program must have a unique name, written e.g. \fvar{nat} or \fvar{lprod}. %\footnote{We assume naming conflicts can be avoided by some extrinsic mechanism.} 
% %A type constructor must also declare an \emph{index kind}, $\kappat{tyidx}$. A type is introduced by applying a type constructor to an index of this kind, written $\ttype{Tycon}{\taut{tyidx}}$.  For example, the type of natural numbers is indexed trivially (i.e. by kind $\kunit$), so it is written $\ttype{Nat}{\tunit}$.
% %
% % To permit the embedding of interesting type systems, the type-level language includes several kinds other than $\kTypeBlur$. We lift several functional data structures to the type level: here, only unit ($\kunit$), binary products ($\kpair{\kappa_1}{\kappa_2}$), binary sums ($\ksum{\kappa_1}{\kappa_2}$) and lists ($\klist{\kappa}$), in addition to labels (introduced as $\ell$, possibly with a subscript, having kind \FF{L}).  The type constructor $\fvar{nat}$ is indexed trivially because there is only one natural number type, but $\fvar{lprod}$ would be indexed by a list of pairs of {labels}  and types. The type constructor $\fvar{arrow}$ is included in the initial constructor context, $\fvalCtx_0$, and has index kind $\kpair{\kTypeBlur}{\kTypeBlur}$.
% % As with the internal language, in practice, one could include a richer programming language and retain the spirit of the calculus, as long as it does not introduce general recursion at the type level. For example, our desugarings add support for number literals and string literals, which require adding numbers and strings to the type-level language (and providing a means for lifting them from the type-level language to the internal language, as we will discuss).
% %  
% % %We see a record type, abbreviated $\tvar{R}$, constructed on line 26. 
% %
% %The kind $\kTypeBlur$ also has an elimination form, $\tfamcase{\tau}{Tycon}{x}{\tau_1}{\tau_2}$ allowing the extraction of a type index by case analysis against a contextually-available type constructor. To a first approximation, one might think of type constructors as constructors of a built-in open datatype \cite{conf/ppdp/LohH06}, $\kTypeBlur$, at the type-level. Like open datatypes, there is no notion of exhaustiveness so the default case is required for totality. %We will see where this is used shortly.
% %
% %
% %
% %%We will write the kind of types as $\kTypeBlur$, though it is also written $\star$ or \lip{Type} in various similarly structured languages (see Sec. \ref{related-work}). 
% %% Rather than there being a fixed set of type constructors, we allow the programmer to declare new type  constructors, and give the static and dynamic semantics of their associated operators, by writing type-level functions. In the semantics for this calculus, our kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction allow us to prove strong type safety, decidability and conservativity theorems.
% %
% %
% %
% %% and integers ($\dint$). We also include labels ($\klabel$), written in a slanted font, e.g. $\tlabel{myLabel}$, which are string-like values that only support comparison and play a distinguished role in the expanded syntax, as we will later discuss. Our first example, $\fvar{nat}$, is indexed trivially, i.e. by unit kind, $\kunit$, so there is only one natural number type, $\ttype{nat}{\tunit}$, but we will show examples of type constructors that are indexed in more interesting ways in later portions of this work. For example, $\fvar{LabeledTuple}$ has index kind $\klist{\kpair{\klabel}{\kTypeBlur}}$. 
% % 
% % Type constructors are not first-class; they do not themselves have arrow kind as in some kind systems (e.g.  \cite{watkins2008specifying}; Ch. 22 of \emph{PFPL} describes a related system \cite{pfpl}). The type-level language does, however, include total functions of arrow kind, written $\karrow{\kappa_1}{\kappa_2}$. Type constructor application can be wrapped in a type-level function to emulate a first-class or uncurried version of a type constructor for convenience.% (indeed, such a wrapper could be generated automatically, though we do not do so). 
% %
% %Two type-level terms of kind $\kTypeBlur$ are equivalent if they apply the same constructor, identified by name, to equivalent indices. Going further, we ensure that deciding type equivalence requires only checking for syntactic equality after normalization by imposing the restriction that equivalence at a type constructor's index kind must be decidable in this way. Our treatment of equivalence in the type-level language is thus quite similar to the treatment of term-level equality using ``equality types'' in a language like Standard ML.
% %% A kind $\kappa$ is an  \emph{equality kind} if $\kEq{\kappa}$ can be derived (see appendix). 
% %Conditional branching on the basis of equality at an equality kind can be performed in the type-level language. Equivalence at arrow kind is not decidable by our criteria, so type-level functions cannot appear within type indices. This also prevents general recursion from arising at the type level. Without this restriction, a type-level function taking a type as an argument could ``smuggle in'' a self reference as a type index, extracting it via case analysis (continuing our analogy to open datatypes, this is closely related to the positivity condition for inductive datatypes in total functional languages like Coq).% as maintaining the metatheoretic guarantee that typing respects type equivalence would impose a substantial burden in such a setting.% (a na\"ive approach to this would impose non-trivial extrinsic proof obligations onto extension developers that, unlike in others in this thesis, could threaten type safety).

% %Every type constructor also defines type-level functions called $\FF{iana}$ and $\FF{esyn}$, which we will describe below, and a \emph{representation schema}, a type-level function that associates with every type an internal type. We will return to this after introducing operators.
% %
% %\subsection{Core External Forms and Desugaring}\label{opcons}
% %The syntax for external terms (Figure \ref{grammar}) contains variables, $\lambda$ terms, three generalized introductory forms and a single generalized elimination form. The introductory forms are either unascribed, ascribed with a type or ascribed with a type constructor, as in our discussion of \texttt{typy}. These generalized forms take a single a type-level value as an {index} and $n \geq 0$ arguments, which are other external terms. To better motivate this choice, we can give a purely syntactic desugaring of a Python-like syntax with labels to these forms, shown in Figure \ref{desugaring}. It is instructive to rewrite lines 27-28 of Figure \ref{record-theory} using these desugarings.% In \texttt{typy}, desugarings can be user-defined 
% %
% %User-defined operator constructors are declared using \textsf{opcon}.  For reasons that we will discuss, our calculus associates every operator  constructor with a type constructor. The \emph{fully-qualified name} of every operator constructor, e.g. $\fvar{nat}.\opvar{z}$, must be unique. Operator constructors, like type constructors, declare an index kind, $\kappaidx$. In our first example, all the operator constructors are indexed trivially (by index kind $\kunit$), but other examples use more interesting indices. For example, in SML, the projection operator \lip{#3} can be applied to an $n$-tuple, $e$, iff $n \geq 3$. Note that it thus cannot be a function with a standard arrow type. Notionally, \lip{#} is an operator constructor and \lip{3} is its index. In an active embedding of $n$-tuples into $\lam\texttt{typy}$, this would be written $\eop{Tuple}{prj}{3}{e}$ (we will nearly recover ML's syntax later). $\fvar{LabeledTuple}.\opvar{prj}$ is the operator constructor used to access a field of a labeled tuple, so it has index kind $\klabel$. An operator itself is, notionally, selected by indexing an operator constructor, e.g. $\fvar{nat}.\opvar{s}\langle \tunit \rangle$, but technically neither operator constructors nor operators are first-class at any level (additional machinery would be needed, e.g. an \textsf{Op} kind, but this is not fundamental to our calculus). 
% %Instead, in the external language, an operator constructor is applied by simultaneously providing an index and  $n \geq 0$ \emph{arguments}, written $\eop{Tycon}{op}{\tauidx}{\splat{e}{1}{n}}$\footnote{It may be helpful to distinguish between type/operator constructors and \emph{term formers}. There are term formers at all levels in the calculus. For example, operator constructor application and $\lambda$ are  external term formers, and type constructor application is a type-level term former. We might write these following Harper's conventions for abstract syntax to highlight this distinction \cite{pfpl}: $\mathtt{lam}[\tau](x.e)$, $\mathtt{ocapp}[\fvar{Tycon}, \opvar{op}, \tauidx](\splat{e}{1}{n})$ and $\texttt{tcapp}[\fvar{Tycon}](\tau)$.}. For example, on line 18 of Fig. \ref{nat}, we see the operator constructors $\fvar{nat}.\opvar{z}$ and $\fvar{nat}.\opvar{s}$ being applied to compute $two$. %\footnote{Although our focus here is entirely on semantics, a brief note on syntax: in the expanded syntax, the trivial indices and empty argument lists can be omitted, so we could write \texttt{Nat.s(Nat.s(Nat.z))}. With the ability to ``open'' a type's operators into the context, we could shorten this still to \texttt{s(s(z))}. Alternatively, with the ability to define a TSL in a manner similar to that in Sec. \ref{aparsing}, we might instead just write \texttt{2}.}


% %The expressive and metatheoretic power of the calculus arises from how the rules for the active typing judgement handle these  generalized forms. Rather than fixing the specification of a finite collection of operator constructors and tasking the \emph{compiler} with deciding a typing derivation on its basis, the specification instead delegates to a type-level function associated with a type constructor. In the core calculus, this is one of three functions, called \FF{iana}, \FF{isyn} and \FF{esyn} in the grammar.\footnote{This means that any particular type constructor only supports a single introduction and elimination desugaring. This is a minor inconvenience in some cases that is resolved in \texttt{typy} by the use of methods.} We see how this is done with the rules for the active typing judgements, given in Figure \ref{atj}.

% %The rule $\rulename{att-lam}$ must take into account the fact that, because we support type-level computation, the type annotation on the argument may not be in normal form. Thus, we evaluate it to normal form. Note that the kinding rules (not shown) will guarantee that, because $\tau_1$ is of kind $\kTypeBlur$, its normal form, $\tau'_1$, is of the form $\ttype{tycon}{\tauidx}$ for some normal $\tauidx$. To generate an appropriate internal type annotation in the translation, we need to compute the representation type of $\tau_1'$. This involves calling the representation function associated with its type constructor (rule \fvar{repof} in Figure \ref{tleval}). The form $\titerm{\sigma}$ is a \emph{quoted internal type}. Note in the syntax that there is an unquote form, $\dup{\tau}$, which allows us to compose internal types compositionally without needing to expose an elimination form. Indeed, it is an interesting facet of our calculus that we never need to examine syntax trees directly to implement extensions. The evaluation semantics remove quotations, so the normal form of a quoted internal type contains an internal type with no quotations.
% %
% %Lambda functions are the only introductory form requiring special support in the calculus (because they need to  manipulate the context; see Discussion). The next three rules show how any other abstractions that we define (e.g. records, decimals, etc.) make use of a generalized introductory form. 
% %
% %The rule  $\rulename{att-i-unasc}$ shows that unascribed introductory forms can only be analyzed against a type. Given such a type, the rule extracts a definition, named $\FF{iana}$, from the type constructor (cf. the \lip{ana_Dict} and \lip{trans_Dict} methods earlier). It calls this function with a pair containing the operator and type index and a list of \emph{reified arguments}, which have kind $\Q$ and introductory form $(\Gamma; e)?$. These are only constructed by the compiler (there would be no corresponding form in the concrete syntax). Their purpose is to allow the definition to programatically invoke synthesis and analysis as needed using the $\FF{ana}$ and $\FF{syn}$ operators. We can see the $\fvar{record}$ type constructor doing so on line 5 of Figure \ref{record-theory} to ensure that the field value provided as an argument has the same type as the corresponding label (accomplished by simultaneously folding over all three pieces of input data). The rule for performing analysis, $\fvar{ana}$, is in Figure \ref{tleval}. If it succeeds, analysis returns a \emph{translation}, which is a quoted internal term with kind $\kITerm$ and introductory form $\titerm{\iota}$. Like quoted internal types, there is an unquote form that is eliminated during evaluation. The operator definition uses this to construct a translation for the record. As before, empty records translate to units and records with a single field are unadorned. In this example, records with two or more fields translate to nested tuples. 
% %
% %Because we have a representation type associated with the type, we can check that the translation is \emph{representationally consistent} (the final premise). As we will discuss, representational consistency combined with type safety of the internal language implies type safety overall (it arises as a strengthening of the inductive hypothesis needed to prove type safety, and is closely related to work on \emph{type-preserving compilation} in the TIL compiler for Standard ML, \cite{tarditi+:til-OLD}).
% %
% %The next rule, $\rulename{att-i-asc-ty}$ states that introductory forms ascribed with a type are analyzed against that type.
% %
% %Introductory forms ascribed with a type constructor can, according to the final introductory rule, $\rulename{att-i-asc-tycon}$, synthesize a type via the definition $\FF{isyn}$. It is passed the operator index (there is no type index, since we only have a type constructor) and a list of arguments, as before. In this case, it must return not just a translation, but also a type. We use the form $\tden{\tau_2}{\tau_1}$ for such a pairing, which has kind $\FF{TT}$. The type and translation can be extracted from it using the appropriately named elimination forms (indeed, as given, it is merely a pair, but we give it special syntax for clarity -- it looks like the conclusion of the typing judgement -- and for other reasons that will become clear in future work). To synthesize a type from a list of labels and arguments, we must be able to synthesize types from the arguments. The $\FF{syn}(\tau)$ operator permits this, per rule $\fvar{syn}$, seen being used in Listing \ref{record-theory}. We check representational consistency of the result, as before.
% %
% %Finally, we show the rule for elimination forms. It operates by first synthesizing a type for the ``primary'' subterm, then extracting the $\FF{esyn}$ definition from its type constructor. As before, it is called with the arguments and representational consistency is checked. Note that the primary subterm is itself the first argument (though we have already synthesized a type for it, it is more uniform and clear to allow the operator  to do so again, which is done in our example by the $\tvar{decons1}$ helper function).

% %\subsection{Representational Consistency Implies Type Safety}\label{repcon}
% %A key invariant that our operator definitions maintain is that well-typed external terms of type $\ttype{Nat}{\tunit}$ always translate to well-typed internal terms of internal type $\dint$, the representation type of $\ttype{Nat}{\tunit}$. Verifying this for the zero case is simple. For the translation produced by the successor case to be of internal type $\dint$ requires that $\iup{\tvar{x}}$ be of internal type $\dint$. Because it is the result of analyzing an argument against $\ttype{Nat}{\tunit}$, and the only other introductory form is the zero case, this holds inductively. We will discuss the recursor later, but it also maintains this invariant inductively.
% %
% %This invariant would not hold if, for example, we allowed the type and translation: $$({{\ttype{Nat}{\tunit},\titerm{(0, ())}}})$$
% %\noindent
% %In this case, there would be two different internal types, $\dint$ and $\dpair{\dint}{\dunit}$,  associated with a single external type, $\ttype{Nat}{\tunit}$. This would make it impossible to reason \emph{compositionally} about the translation of an external term of type $\ttype{Nat}{\tunit}$, so our implementation of the successor would produce ill-typed translations in some cases but not others. Similarly, we wouldn't be able to write functions over all natural numbers because there would not be a well-typed translation to give to such a function. This violates type safety: there are now well-typed external terms, according to the active typechecking and translation judgement, for which evaluating the corresponding translation would ``go wrong''. 
% %
% %To reason compositionally about the semantics of well-typed external terms when they are given meaning by translation to a typed internal language, the system must maintain the following property: for every  type, $\tau$, there must exist an internal type, $\sigma$, called its \emph{representation type}, such that the translation of every external term of type $\tau$ has internal type $\sigma$. This principle of \emph{representational consistency} arises essentially as a strengthening of the inductive hypothesis necessary to prove that all well-typed external terms translate to well-typed internal terms because $\lambda$ and successor are defined compositionally. It is closely related to the concept of \emph{type-preserving compilation} developed by Morrisett et al. for the TIL compiler for SML \cite{tarditi+:til-OLD}, here lifted into the language. Our judgements check this extension correctness property directly by typechecking each translation produced by a user extension (the translations of, for example, lambda terms will be inductively representationally consistent, so no additional check is needed).%That is, instead of being used as a necessary condition for compiler correctness, we are using it as a sufficient condition for type safety. 
% %%We emphasize the distinction between translation (which in our calculus endows terms with a dynamic semantics) and compilation (which must preserve the dynamic semantics of terms). %In the presence of extensions, checking for representational consistency becomes subtle if we wish to guarantee conservativity, as we will discuss in the next subsection.
% %
% %%For the semantics to ensure that representational consistency is maintained by all operator definitions, we require, as briefly mentioned in Sec. \ref{types}, that each type constructor declare a \emph{representation schema} after the keyword \textsf{schema}. This is a type-level function of kind $\karrow{\kappaidx}{\kIType}$, where $\kappaidx$ is the index kind of the type constructor. The kind $\kIType$ has introductory form $\titype{\bar \sigma}$ and no elimination form, and is similar to the kind $\kITerm$ introduced above. There are two forms of \emph{translational internal type}, $\bar \sigma$, that do not correspond to forms in $\sigma$. As with translational internal terms, these are included to allow an internal type to be formed compositionally:
% %%\begin{enumerate}
% %%\item $\dup{\tau}$ splices in another translational internal type $\tau$
% %%\item $\trepof{\tau}$ refers to the representation type of type $\tau$
% %%\end{enumerate}
% %%
% %%When the semantics (i.e. the compiler) needs to determine the representation type of the type $\ttype{Tycon}{\tauidx}$ it simply applies the representation schema of $\fvar{Tycon}$ to ${\tauidx}$. Note that the representation type of a type cannot be extracted directly from within the type-level language, again for reasons that we will discuss below. 
% %%
% %%These additional forms are not needed by the representation schema of $\fvar{nat}$  because it is trivially indexed. In Fig. \ref{tuple}, we will discuss an example of the type constructor $\fvar{Tuple}$, implementing the semantics of $n$-tuples by translation to nested binary products. Here, the representation schema refers to the representations of the tuple's constituent types and is computed by folding over a list, so both of these are used\todo{add this example from appendix of ESOP}. 
% %
% %Translational internal terms can contain translational internal types. We see this in the definition of the recursor on natural numbers. The type assignment is the arbitrary type, $\tvar{t2}$. We cannot know what the representation type of $\tvar{t2}$ is, so we refer to it abstractly using $\trepof{\tvar{t2}}$. Luckily, the proof of representational consistency of this operator is parametric over the representation type of $\tvar{t2}$. 

% %\subsection{Metatheory}\label{metatheory}
% %\newcommand{\F}[1]{\textsf{#1}~}
% %\newcommand{\FF}[1]{\textsf{#1}}
% %\newcommand{\Q}{\FF{Arg}}
% %\renewcommand{\tnil}[1]{[]}
% %\begin{figure}[t]
% %\small
% %$$\begin{array}{rcl} 
% %%\textbf{programs} & 
% %\rho & ::= & \F{tycon}\fvar{tycon}~\F{of}\kappaidx~{\{}TC{\}}; \rho \pipe e\\ %\pfam{\familyDf}{\progsort}
% %%\\&  \pipe & \pdef{t}{\kappa}{\tau}{\progsort}  \pipe e
% %%\pipe \pdef{t}{\kappa}{\tau}{\progsort} 
% % TC & ::= & \F{iana} \{\tau\}; \F{isyn} \{\tau\}; \F{esyn} \{\tau\}; \F{rep} \{\tau\}\\
% %\\ 
% %e & ::= & \evar{x} \pipe \elam{\evar{x}}{\tau}{e} \pipe I \pipe I : \tau \pipe I :: \fvar{tycon} \pipe E\\
% %I & ::= &  \FF{intro}[\tauidx](\splat{e}{1}{n})\\
% %E & ::= & e\cdot\FF{elim}[\tauidx](\splat{e}{1}{n})\\
% %\\
% % \tau  & ::=   &   \tvar{t} \pipe 
% %                           \tlam{t}{\kappa}{\tau} \pipe 
% %                           \tapp{\tau_1}{\tau_2}  
% %\\&\pipe&                      
% %                           \tnil{\kappa} \pipe \tcons{\tau_1}{\tau_2} \pipe 
% %                                      \tfold{\tau_1}{\tau_2}{h}{t}{r}{\tau_3}
% %                           \\
% %     & \pipe &     \ell \pipe 
% %   \tunit \pipe 
% %                           \tpair{\tau_{1}}{\tau_{2}} \pipe 
% %                           \tfst{\tau} \pipe 
% %                           \tsnd{\tau} \pipe \cdots 
% %                           \\  
% %%     & \pipe & \tinl{\kappa_2}{\tau_1} \pipe \tinr{\kappa_1}{\tau_2} \\
% %%     &\pipe& \tsumcase{\tau}{t}{\tau_1}{t}{\tau_2} \\
% %&\pipe &   \ttype{tycon}{\tauidx} \\&\pipe& \tfamcase{\tau}{tycon}{x}{\tau_1}{\tau_2}\\
% %&\pipe &           \tifeq{\tau_{1}}{\tau_{2}}{\kappa}{\tau_{3}}{\tau_{4}} 
% %%                            \\                          %       & & \pipe & \tfamcase{\tau}{Fam}{x}{\tau_1}{\tau_2}\\
% %%                                                
% %\\      &  \pipe & \tden{\tau_2}{\tau_1} \pipe \terr %\pipe \tden{\ibar}{\tau}^{\checkmark} 
% %\pipe \ttypeof{\tau} \pipe \itransof{\tau} \\% \tdencase{\tau}{x}{t}{\tau_1}{\tau_2}\\
% %% %& & \pipe & 
% %%%                           \tdencase{\tau}{y}{x}{\tau_1}{\tau_2}
% %%%                            \\
% %%
% % & \pipe & \titerm{\iota}
% %     \pipe \titype{\sigma} \pipe \trepof{\tau} \\
% %   & \pipe &  (\Gamma; e)? \pipe \FF{syn}(\tau) \pipe \FF{ana}(\tau; \tau')\\
% %%\textbf{external terms}         & e & ::= & \evar{x} \pipe 
% %%%                           \efix{x}{\tau}{e} \pipe 
% %%                            \elam{\evar{x}}{\tau}{e} \pipe 
% %%                            \eop{tycon}{op}{
% %%                              \tauidx
% %%                            }{
% %%                                  \splat{e}{1}{n}
% %%                            } \\
% %%                  &     &   &   \\
% %%
% %%
% %%                        
% %%%\text{deabstracted}& \iota & ::= & \mathcal{G}[\iota, \sigma]\\
% %%                        
% %%\\
% %%\\
% %%              
% %%% &  & \pipe & \tvalof{\tau_1}{\tau_2} \pipe \iup{\tau} \\
% %%%                         \trepof{\tau} \pipe \dup{\tau}\\
% %%\text{translational IL} & \bar{\iota} & ::= & x \pipe \ifix{x}{\bar \sigma}{\bar \iota} 
% %%  %\pipe \ilam{x}{\bar \sigma}{\bar \iota} \pipe \iapp{\bar \iota_1}{\bar \iota_2} 
% %%  \pipe \cdots \pipe \itransof{\tau} \\           
% %% & \bar{\sigma} & ::= & \darrow{\bar \sigma_1}{\bar \sigma_2} \pipe \cdots \pipe \dup{\tau} \pipe \trepof{\tau} \\
% %%%\text{abstracted} & \sabs & ::= & \darrow{\sabs_1}{\sabs_2} \pipe \cdots \pipe \sabsrep{\tau}
% %%                      \\
% %\kappa & ::= & \karrow{\kappa_1}{\kappa_2} \pipe \klist{\kappa} \pipe \FF{L} \pipe \kunit \pipe \kpair{\kappa_1}{\kappa_2} \pipe \cdots \\
% % & \pipe & \kTypeBlur \pipe \FF{TT} \pipe \kITerm \pipe \kIType \pipe \Q
% %%                          \klabel \pipe
% %%                          \klist{\kappa} \pipe
% %%                        \kunit \pipe 
% %%                        \kpair{\kappa_{1}}{\kappa_{2}} \\
% %%                        &&\pipe&
% %%                        \ksum{\kappa_1}{\kappa_2} \pipe
% %%                        \kTypeBlur \pipe \kDen \pipe 
% %%                        \kIType               
% %%%\textbf{ops signature}     & \Theta  & ::= & \kOpEmpty \pipe \kOp{\Theta}{op}{\kappai}\\
% %%%                                   &   &   & \\
% %\\
% %\\
% %\iota  & ::= & \evar{x} \pipe 
% %                       \ifix{\evar{x}}{\sigma}{\iota} \pipe
% %                       \ilam{\evar{x}}{\sigma}{\iota} \pipe 
% %                       \iapp{\iota_{1}}{\iota_{2}} 
% %                       \\ & \pipe & \iunit \pipe \ipair{\iota_1}{\iota_2} \pipe \ifst{\iota} \pipe \isnd{\iota} \pipe \cdots \pipe \iup{\tau}
% %%\\&\pipe&               \cdots \pipe  
% %%                        n \pipe \iop{\iota_{1}}{\iota_{2}} \pipe \iIfEq{\iota_{1}}{\iota_{2}}{\dint}{\iota_{3}}{\iota_{4}} \\
% %%& \pipe & 
% %%                        \iunit \pipe
% %%                        \ipair{\iota_{1}}{\iota_{2}} \pipe 
% %%                        \ifst{\iota} \pipe
% %%                        \isnd{\iota} 
% %%\\&\pipe&                       
% %%                         \iinl{\sigma_2}{\iota_1} \pipe \iinr{\sigma_1}{\iota_2} \\&\pipe & \icase{e}{x}{e_1}{x}{e_2} \\
% %\\ \sigma  & ::= &    \darrow{\sigma_1}{\sigma_2} \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} %\pipe \dint \pipe \dunit \pipe \dpair{\sigma_1}{\sigma_2} \pipe \dsum{\sigma_1}{\sigma_2}  
% %\pipe \cdots \pipe \dup{\tau}
% %\end{array}$$
% %%\vspace{-10pt}
% %\caption{\small Syntax of Core $\lam\texttt{typy}$. Here, $x$ ranges over external and internal language variables, $\tvar{t}$ ranges over type-level variables, $\fvar{tycon}$ ranges over type constructor names and $\ell$ ranges over labels.
% %\label{grammar}}
% %\end{figure}
% %\begin{figure}
% %\small
% %\begin{flalign}
% %& \F{tycon}\fvar{record}~\F{of}\klist{\kpair{\FF{L}}{\kTypeBlur}}~\{\\
% %& \quad \FF{iana}~\{\tlam{i}{\kpair{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{\klist{\FF{L}}}}{\tlam{a}{\klist{\Q}}{\\& \quad\quad\tfold{\tvar{zip3}~\tfst{\tvar{i}}~\tsnd{\tvar{i}}~\tvar{a}}
% %  {\titerm{\tunit}}{h}{t}{r}{
% % \\&\quad\quad\quad\tifeq{\tfst{\tvar{first}~\tvar{h}}}{\tvar{second}~\tvar{h}}{\FF{L}}{
% % \\&\quad\quad\quad\quad
% % \F{let}~\tvar{x}:\kITerm~=~\FF{ana}(\tvar{third}~\tvar{h}; \tsnd{\tvar{first}~\tvar{h}})
% %~\FF{in}
% %\\ & \quad\quad\quad\quad \tfold{\tvar{t}}{\tvar{x}
% %}{\_}{\_}{\_}{\titerm{(\iup{\tvar{x}},\iup{\tvar{r}})}}  }{\terr}
% %%     \F{let}~\tvar{tt}:\FF{TT}=~\FF{syn}(\tsnd{\tsnd{\tvar{h}}}) \F{in}\\
% %%\\     & \quad\quad\quad 
% %}
% %}}\}\\
% %%& \quad \FF{isyn}~\{\tlam{i}{\klist{\FF{L}}}{\tlam{a}{\klist{\Q}}{
% %%\\&\quad\quad \tfold{\tvar{zip2}~\tvar{i}~\tvar{a}}{\tden{\titerm{()}}{\ttype{record}{\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}}}{h}{t}{r}{
% %%\\&\quad\quad\quad \F{let}~\tvar{htt}:\FF{TT}~=~\FF{syn}(\tsnd{\tvar{h}})
% %%~\FF{in}
% %%\\&\quad\quad\quad \F{let}~\tvar{hty}:\kTypeBlur~=~\ttypeof{\tvar{htt}}
% %%\\&\quad\quad\quad \F{let}~\tvar{ty}:\kTypeBlur~=~\ttype{record}{(\tfst{\tvar{h}}, \tvar{hty})::\tnil{{\kpair{\FF{L}}{\kTypeBlur}}}}
% %%~\FF{in}
% %%\\&\quad\quad\quad \tfold{\tvar t}{\tden{\itransof{\tvar{htt}}}{\tvar{ty}}}{\_}{\_}{\_}{
% %%\\&\quad\quad\quad\quad \tden{\titerm{(\iup{\itransof{\tvar{htt}}},\iup{\itransof{\tvar{r}}})}}{\tvar{ty}}}}
% %%}}\}\\
% %%& \quad \FF{esyn}~\{\tlam{i}{\FF{L}}{\tlam{a}{\klist{\Q}}{
% %%  \tvar{decons1}~\tvar{a}~\tlam{ty}{\kTypeBlur}{\tlam{x}{\kITerm}{
% %%\\&\quad\quad   \tfamcase{\tvar{ty}}{record}{sig}{
% %%\\&\quad\quad\quad \tfold{\tvar{sig}}{\terr}{h}{t}{r}{
% %%\\&\quad\quad\quad\quad \tifeq{\tfst{\tvar{h}}}{\tvar{i}}{\FF{L}}{\tfold{\tvar{t}}{\tden{\tvar{x}}{\tsnd{\tvar{h}}}}{\_}{t'}{\_}{
% %%\\&\quad\quad\quad\quad\quad \tfold{\tvar{t'}}{\tden{\titerm{\ifst{\iup{\tvar{x}}}}}{\tsnd{\tvar{h}}}}{\_}{\_}{\tvar{r'}}{
% %%\\&\quad\quad\quad\quad\quad\quad \tden{\titerm{\isnd{\iup{{\itransof{\tvar r}}}}}}{\tsnd{\tvar{h}}}}}\\&\quad\quad\quad\quad\hspace{-3px}}{\tvar{r}}
% %%}\\&\quad\quad\hspace{-3px}}{\terr}
% %%  }}
% %%}}\\
% %%& \quad \FF{rep}~\{\tlam{i}{\klist{\kpair{\FF{L}}{\kTypeBlur}}}{  \\& \quad \quad \tfold{\tvar{i}}{\titype{\dunit}}{s}{j}{r}{
% %%    \tfold{\tvar{j}}{\trepof{\tsnd{\tvar{s}}}}{\_}{\_}{\_}{\\
% %%    &\quad\quad\quad
% %%    \titype{\dpair{\dup{\trepof{\tsnd{\tvar{s}}}}}{\dup{\tvar{r}}}}
% %%    }
% %%  }}\}\\
% %%& \}; \\
% %%& \F{let}\tvar{R} : \kTypeBlur = \ttype{record}{(\ell_1, \ttype{record}{\tnil{{\FF{L}\times\kTypeBlur}}}) :: \tnil{{\FF{L}\times\kTypeBlur}}}~\F{in}\\
% %%&\F{let}id = \elam{x}{\tvar{R}}{\{\ell_1=~x\cdot\ell_1\} :: \fvar{record}}\\
% %%&\F{let}triv = id[\{\ell_1 = \{\}\}]\cdot\ell_1
% %%%&\F{let}one = \FF{intro}[\ell_\text{succ}]
% %%%&\F{let}plus = \elam{x}{\tvar{nat}}{\elam{y}{\tvar{nat}}{\FF{elim}[()](x; y; \\
% %%%& \quad \elam{r}{\tvar{nat}}{intro[\ell_\text{succ}](r) : \tvar{nat}}})}
% %\end{flalign}
% %\caption{The definition of the record type in  using the desugarings in Figure \ref{desugaring} and with the addition of simple let bindings for both type and term variables (not shown). Some type-level helper functions also omitted for concision.}
% %
% %\label{record-theory}
% %\end{figure}
% %We will now give a core typed lambda calculus, $\lam\texttt{typy}$, that captures the semantics described in the previous sections. It is intended to make precise how active typechecking and translation works and how our mechanism relates to existing work on bidirectional typechecking, type-level computation and typed compilation while abstracting away from the details of Python's syntax and imposing a stronger type-level semantics that will allow us to state metatheoretic properties of interest. We will assume a fixed base for functions (providing the standard semantics of lambda functions) and target language, which we here call the \emph{internal language}.
% %
% %The syntax of $\lam\texttt{typy}$ is shown in Figure \ref{grammar} and an example \emph{program} that defines a type constructor, $\fvar{record}$, with a semantics similar to that given in Listing \ref{record}\footnote{Technically, this defines labeled tuples, because the order of labels matters.}, using it to write an identity function and compute the value $triv$ (the empty record), is shown in Figure \ref{record-theory}.
% %
% %%In this section, we will develop an ``actively typed'' version of the simply-typed lambda calculus with simply-kinded type-level computation called $\lam\texttt{typy}$. More specifically, the level of types, $\tau$, will itself form a simply-typed lambda calculus. \emph{Kinds} classify type-level terms in the same way that types conventionally classify expressions. Types become just one kind  of type-level value (which we will write $\kTypeBlur$, though it is also variously written $\star$, \lip{T} and \lip{Type} in various settings). Rather than there being a fixed set of type and operator constructors, we allow the programmer to declare new  constructors, and give their static and dynamic semantics by writing type-level functions. The kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction will allow us to prove strong type safety, decidability and conservativity theorems.
% %%
% %%The syntax of Core $\lam\texttt{typy}$ is given in Fig. \ref{grammar}. An example of a program defining type and operator constructors that can be used to construct an active embedding of G\"odel's \textbf{T} into $\lam\texttt{typy}$ is given in Fig. \ref{nat}. We will discuss its semantics and how precisely the embedding, seen being used starting on line 15 to ultimately compute the sum of two and two, works as we go on. Natural numbers can, of course, be isomorphically embedded in existing languages, with a similar usage and asymptotic performance profile (up to function call overhead as an abstract type, for example). We will provide more sophisticated examples where this is less feasible later on (and note that type abstraction is an orthogonal mechanism).
% %%a type constructor declaration, $\fvar{nat}$, indexed trivially, together with three operator constructors, also all indexed trivially, that implement the standard introductory forms for natural numbers as well as the recursor operator (as in G\"odel's T \cite{pfpl}). Following the type constructor declaration, we apply $\fvar{nat}$ with the trivial index, $\tunit$, to form the type $\tvar{nat}$. Finally, we write an external term that uses the operators associated with $\fvar{nat}$ and the built-in constructor $\fvar{Parr}$, governing partial functions, to define an addition function and compute the addition of the natural numbers  two and two. We will introduce a more convenient concrete syntax in later portions of this thesis; for now we will restrict ourselves to the abstract syntax so that this example can directly aid in understanding the semantics.
% %
% %\subsection{Overview}\label{programs}
% %
% %A \emph{program}, $\rho$, consists of a series of constructor declarations followed by an external term, $e$.  Compiling a program consists of first \emph{kind checking} it (see below), then typechecking the external term and simultaneously {translating} it to a term, $\iota$, in the {typed internal language}. 
% %%These correspond to the premises of the \emph{central compilation judgement} $\pkcompiles{\rho}{\iota}$:
% %%\[
% %%\inferrule[p-compiles]{
% %%  \emptyset \vdash_{\fvalCtx_0} \rho\\
% %%% \progOK{\emptyset}{\fvalCtx_0}{\rho}\\
% %%  \pcompiles{\fvalCtx_0}{\rho}{\iota}
% %%}{\pkcompiles{\rho}{\iota}}
% %%\]
% %%This anchors our exposition; we will describe how it is derived (i.e. how to write a compiler for $\lam\texttt{typy}$) in the following sections. 
% %The key judgements are the \emph{bidirectional active typing judgements}  (Fig. \ref{att}, which we describe starting in Sec. \ref{opcons}). They relate an external term, $e$, to a {type}, $\tau$, called its \emph{type assignment}, and an internal term, $\iota$, called its \emph{translation}, under \emph{typing context} $\Gamma$ and \emph{constructor context} $\Phi$. The first is synthesis, the second analysis.
% %\[\Gamma \vdash_\fvalCtx e \Rightarrow \tau \leadsto \iota
% %~~~~~~~
% %\Gamma \vdash_\fvalCtx e \Leftarrow \tau \leadsto \iota\]
% %%\[\ecompilesAX{e}{\tau}{\iota}\]
% %
% %The typing context, $\Gamma$, maps variables to types in essentially the conventional way (\cite{pfpl} contains the necessary background for this section). The constructor context, $\Phi$, tracks user-defined type  constructors. % Weakening and exchange of the constructor context closely related to the issue of conservativity that we will return to. Each constructor is identified by name, so there is no analog to contraction.
% %
% %The dynamic behavior of an external term is determined entirely by its translation to the internal language, which has a conventional operational semantics. This form of semantics can be seen as lifting into the language specification the first stage of a type-directed compiler like the TIL compiler for Standard ML \cite{tarditi+:til-OLD} and has some parallels to the Harper-Stone semantics for Standard ML, where external terms were also given meaning by elaboration from the EL to an IL \cite{Harper00atype-theoretic}. %
% %
% %In $\lam\texttt{typy}$, the internal language (IL) provides partial functions (via the generic fixpoint operator of Plotkin's PCF) and simple product  types for the sake of our example. In practice, the internal language could be any typed  language with a specification for which type safety and decidability of typechecking have been satisfyingly determined. The internal type system serves as a ``floor'': guarantees that must hold for terms of any type (e.g. that out-of-bounds access to memory never occurs) must be maintained by the internal type system. User-defined constructors can enforce invariants stronger than those the internal type system maintains at particular types, however. Performance is also ultimately limited by the internal language and downstream compilation stages that we do not here consider (safe compiler extension has been discussed in previous work, e.g. \cite{conf/pldi/TatlockL10}).
% %
% %
% %\subsection{Types and Type-Level Computation}\label{types}
% %$\lam\texttt{typy}$ supports, and makes extensive use of, simply-kinded type-level computation. Specifically, type-level terms, $\tau$, themselves form a typed lambda calculus. The classifiers of type-level terms are called \emph{kinds}, $\kappa$, to distinguish them from  \emph{types}. Types are  type-level values of kind $\kTypeBlur$. In \texttt{typy}, the type-level language (together with the level of programs) is written in Python, which can be thought of as having a rather limited kind system (with one kind, \lip{dyn}). Here, we are able to more precisely discuss the kinds of values in the type-level language. 
% %
% %In most languages, types are formed by applying one of a collection of \emph{type constructors} to zero or more \emph{indices}. In $\lam\texttt{typy}$, the situation is notionally similar. User-defined type constructors can be declared at the top of a program (or lifted to the top, in practice) using \textsf{tycon}. Each constructor in the program must have a unique name, written e.g. \fvar{record}.\footnote{We assume naming conflicts can be avoided by some extrinsic mechanism.} A type constructor must also declare an \emph{index kind}, $\kappaidx$. 
% %
% % To permit the embedding of interesting type systems, the type-level language includes several kinds other than $\kTypeBlur$. We lift several functional data structures to the type level: here, only unit ($\kunit$), binary products ($\kpair{\kappa_1}{\kappa_2}$) and lists ($\klist{\kappa}$), in addition to labels (introduced as $\ell$, possibly with a subscript, having kind \FF{L}).  Our record type constructor is indexed by a list of pairs of {labels}  and types (a signature, in essence; line 1). The type constructor $\fvar{arrow}$ is included in the initial constructor context, $\fvalCtx_0$ and has index kind $\kpair{\kTypeBlur}{\kTypeBlur}$.
% % In practice, one could include a richer functional programming language and retain the spirit of the calculus, as long as it does not introduce general recursion at the type level. 
% %  
% %A type is introduced by applying a type constructor to an index of this kind, written $\ttype{Tycon}{\tauidx}$.  For example, the type of natural numbers is indexed trivially, so it is written $\ttype{Nat}{\tunit}$. We see a record type, abbreviated $\tvar{R}$, constructed on line 26. 
% %
% %The kind $\kTypeBlur$ also has an elimination form, $$\tfamcase{\tau}{Tycon}{x}{\tau_1}{\tau_2}$$ allowing the extraction of a type index by case analysis against a contextually-available type constructor. To a first approximation, one might think of type constructors as constructors of a built-in open \cite{conf/ppdp/LohH06}, $\kTypeBlur$, at the type-level. Like open datatypes, there is no notion of exhaustiveness so the default case is required for totality. We will see where this is used shortly.
% %
% %
% %
% %%We will write the kind of types as $\kTypeBlur$, though it is also written $\star$ or \lip{Type} in various similarly structured languages (see Sec. \ref{related-work}). 
% %% Rather than there being a fixed set of type constructors, we allow the programmer to declare new type  constructors, and give the static and dynamic semantics of their associated operators, by writing type-level functions. In the semantics for this calculus, our kind system combined with techniques borrowed from the typed compilation literature and a form of type abstraction allow us to prove strong type safety, decidability and conservativity theorems.
% %
% %
% %
% %% and integers ($\dint$). We also include labels ($\klabel$), written in a slanted font, e.g. $\tlabel{myLabel}$, which are string-like values that only support comparison and play a distinguished role in the expanded syntax, as we will later discuss. Our first example, $\fvar{nat}$, is indexed trivially, i.e. by unit kind, $\kunit$, so there is only one natural number type, $\ttype{nat}{\tunit}$, but we will show examples of type constructors that are indexed in more interesting ways in later portions of this work. For example, $\fvar{LabeledTuple}$ has index kind $\klist{\kpair{\klabel}{\kTypeBlur}}$. 
% % 
% % Type constructors are not first-class; they do not themselves have arrow kind as in some kind systems (e.g.  \cite{watkins2008specifying}; Ch. 22 of \emph{PFPL} describes a related system \cite{pfpl}). The type-level language does, however, include total functions of arrow kind, written $\karrow{\kappa_1}{\kappa_2}$. Type constructor application can be wrapped in a type-level function to emulate a first-class or uncurried version of a type constructor for convenience (indeed, such a wrapper could be generated automatically, though we do not do so). 
% %
% %Two type-level terms of kind $\kTypeBlur$ are equivalent if they apply the same constructor, identified by name, to equivalent indices. Going further, we ensure that deciding type equivalence requires only checking for syntactic equality after normalization by imposing the restriction that equivalence at a type constructor's index kind must be decidable in this way. Our treatment of equivalence in the type-level language is thus quite similar to the treatment of term-level equality using ``equality types'' in a language like Standard ML.
% %% A kind $\kappa$ is an  \emph{equality kind} if $\kEq{\kappa}$ can be derived (see appendix). 
% %Conditional branching on the basis of equality at an equality kind can be performed in the type-level language. Equivalence at arrow kind is not decidable by our criteria, so type-level functions cannot appear within type indices. This also prevents general recursion from arising at the type level. Without this restriction, a type-level function taking a type as an argument could ``smuggle in'' a self reference as a type index, extracting it via case analysis (continuing our analogy to open datatypes, this is closely related to the positivity condition for inductive datatypes in total functional languages like Coq).% as maintaining the metatheoretic guarantee that typing respects type equivalence would impose a substantial burden in such a setting.% (a na\"ive approach to this would impose non-trivial extrinsic proof obligations onto extension developers that, unlike in others in this thesis, could threaten type safety).
% %
% %Every type constructor also defines a \emph{representation}, a type-level function that associates with every type an internal \emph{representation type} (analagous to \lip{trans_type} above). We will return to this after introducing the external forms and operator definitions.
% %
% %\subsection{Core External Forms and Desugaring}\label{opcons}
% %The syntax for external terms (Figure \ref{grammar}) contains variables, $\lambda$ terms, three generalized introductory forms and a single generalized elimination form. The introductory forms are either unascribed, ascribed with a type or ascribed with a type constructor, as in our discussion of \texttt{typy}. These generalized forms take a single a type-level value as an {index} and $n \geq 0$ arguments, which are other external terms. To better motivate this choice, we can give a purely syntactic desugaring of a Python-like syntax with labels to these forms, shown in Figure \ref{desugaring}. It is instructive to rewrite lines 27-28 of Figure \ref{record-theory} using these desugarings:% In \texttt{typy}, desugarings can be user-defined 
% %\noindent
% %\vspace{-5px}
% %$$
% %\begin{array}{l}
% %\FF{let}~id=\lambda x:\tvar{R}.\FF{intro}[\ell_1 :: []](x\cdot\FF{elim}[\ell_1]()) :: \fvar{record}\\
% %\FF{let}~triv=id\cdot\FF{elim}[()](\FF{intro}[\ell_1 :: []](\FF{intro}[[]]()))\cdot\FF{elim}[\ell_1]()
% %\end{array}
% %$$
% %\vspace{-5px}
% %\begin{figure}[t]
% %\small
% %\[
% %\begin{array}{rcl}
% %\{ \ell_1{=}~e_1, \ldots, \ell_n{=}~e_n \} & := & \FF{intro}[\ell_1 :: \ldots :: \ell_n :: []](e_1; \ldots; e_n)\\
% %(e_1, ..., e_n) & := & \FF{intro}[()](e_1; \ldots; e_n)\\
% %\#n & := & \FF{intro}[n]()\\
% %"s" & := & \FF{intro}[s]()\\
% %e.\ell & := & e\cdot\FF{elim}[\ell]()\\
% %e[e_1; \ldots; e_n] & := & e\cdot\FF{elim}[()](e_1; \ldots; e_n)\\
% %e.\ell(e_1; ...; e_n) & := & e\cdot\FF{elim}[\ell](e_1; \ldots; e_n)
% %%(e_1, ..., e_n) & := & intro[()](e_1; ...; e_n)
% %%n &  := & intro[n]()
% %%s & := & intro[s]()
% %%
% %%e.l & := & elim[l](e)
% %%e[e_1; ...; e_n] & := & elim[()](e_1, ..., e_n)
% %%e.l(e_1; ...; e_n) & := & elim[l]
% %\end{array}
% %\]
% %\caption{Desugaring from conventional syntax to core forms. We assume that the type-level language has numbers, $n$, and strings, $s$ (not shown for concision).}
% %\label{desugaring}
% %\end{figure}
% %
% %%User-defined operator constructors are declared using \textsf{opcon}.  For reasons that we will discuss, our calculus associates every operator  constructor with a type constructor. The \emph{fully-qualified name} of every operator constructor, e.g. $\fvar{nat}.\opvar{z}$, must be unique. Operator constructors, like type constructors, declare an index kind, $\kappaidx$. In our first example, all the operator constructors are indexed trivially (by index kind $\kunit$), but other examples use more interesting indices. For example, in SML, the projection operator \lip{#3} can be applied to an $n$-tuple, $e$, iff $n \geq 3$. Note that it thus cannot be a function with a standard arrow type. Notionally, \lip{#} is an operator constructor and \lip{3} is its index. In an active embedding of $n$-tuples into $\lam\texttt{typy}$, this would be written $\eop{Tuple}{prj}{3}{e}$ (we will nearly recover ML's syntax later). $\fvar{LabeledTuple}.\opvar{prj}$ is the operator constructor used to access a field of a labeled tuple, so it has index kind $\klabel$. An operator itself is, notionally, selected by indexing an operator constructor, e.g. $\fvar{nat}.\opvar{s}\langle \tunit \rangle$, but technically neither operator constructors nor operators are first-class at any level (additional machinery would be needed, e.g. an \textsf{Op} kind, but this is not fundamental to our calculus). 
% %%Instead, in the external language, an operator constructor is applied by simultaneously providing an index and  $n \geq 0$ \emph{arguments}, written $\eop{Tycon}{op}{\tauidx}{\splat{e}{1}{n}}$\footnote{It may be helpful to distinguish between type/operator constructors and \emph{term formers}. There are term formers at all levels in the calculus. For example, operator constructor application and $\lambda$ are  external term formers, and type constructor application is a type-level term former. We might write these following Harper's conventions for abstract syntax to highlight this distinction \cite{pfpl}: $\mathtt{lam}[\tau](x.e)$, $\mathtt{ocapp}[\fvar{Tycon}, \opvar{op}, \tauidx](\splat{e}{1}{n})$ and $\texttt{tcapp}[\fvar{Tycon}](\tau)$.}. For example, on line 18 of Fig. \ref{nat}, we see the operator constructors $\fvar{nat}.\opvar{z}$ and $\fvar{nat}.\opvar{s}$ being applied to compute $two$. %\footnote{Although our focus here is entirely on semantics, a brief note on syntax: in the expanded syntax, the trivial indices and empty argument lists can be omitted, so we could write \texttt{Nat.s(Nat.s(Nat.z))}. With the ability to ``open'' a type's operators into the context, we could shorten this still to \texttt{s(s(z))}. Alternatively, with the ability to define a TSL in a manner similar to that in Sec. \ref{aparsing}, we might instead just write \texttt{2}.}
% %
% %\subsection{Operator Definitions and Representational Consistency}
% %
% %%
% %\newcommand{\atjsynX}[3]{\Gamma \vdash_\fvalCtx #1 \Rightarrow #2 \leadsto #3}
% %\newcommand{\atjanaX}[3]{\Gamma \vdash_\fvalCtx #1 \Leftarrow #2 \leadsto #3}
% %\begin{figure}[t]
% %\small
% %$\fbox{\inferrule{}{\ecompilesAX{e}{\tau}{\iota}}}$
% %~~~~$\Gamma ::= \emptyset \pipe \Gamma, x \Rightarrow \tau$
% %\begin{mathpar}
% %\inferrule[att-var]{
% % x \Rightarrow \tau \in \Gamma
% %}{
% % \atjsynX{x}{\tau}{x}
% %}
% %
% %\inferrule[att-syn-to-ana]{
% % \atjsynX{e}{\tau}{\iota}
% %}{
% % \atjanaX{e}{\tau}{\iota}
% %}
% %
% %\inferrule[att-lam]{
% % \tau_1 \Downarrow_{\Gamma;\fvalCtx} \tau_1'\\
% %   \trepof{\tau_1'} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\\\
% % \Gamma, x \Rightarrow \tau_1' \vdash_\fvalCtx e \Rightarrow \tau_2 \leadsto \iota
% %%  \iota \hookrightarrow_\fvalCtx \iota'\\
% %%    \sigma \hookrightarrow_\fvalCtx \sigma'
% %%  \ddbar{\fvar{Arrow}}{\fvalCtx}{\trepof{\tau_1'}}{\sbar_1}\\
% % %\delfromtau{$\Xi_0$}{\fvalCtx}{\tau_1'}{\sabs}\\\\
% %}{
% % \atjsynX{\elam{x}{\tau_1}{e}}{\ttype{arrow}{(\tau_1', \tau_2)}}{\ilam{x}{\sigma'}{\iota}}
% %}
% %
% %\inferrule[att-i-unasc]{
% % \vdash_\fvalCtx \FF{iana}(\fvar{tycon})=\taudef\\
% % \taudef~(\tauidx, \tauidx')~((\Gamma; e_1)? :: \ldots :: (\Gamma; e_n)? :: []) \Downarrow_{\Gamma;\fvalCtx} \titerm{\iota}\\
% %%  \trepof{\ttype{tycon}{\tauidx'}} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\
% %   \trepof{\ttype{tycon}{\tauidx'}} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\
% %   \Gamma \vdash_\fvalCtx \iota : \sigma
% %}{
% % \atjanaX{\FF{intro}[\tauidx](e_1; \ldots; e_n)}{\ttype{tycon}{\tauidx'}}{\iota}
% %}
% %
% %\inferrule[att-i-asc-ty]{
% % \atjanaX{I}{\tau}{\iota}
% %}{
% % \atjsynX{I : \tau}{\tau}{\iota}
% %}
% %
% %\inferrule[att-i-asc-tycon]{
% % \vdash_\fvalCtx \FF{isyn}(\fvar{tycon})=\taudef\\\\
% % \taudef~\tauidx~((\Gamma; e_1)? {::}{\ldots}{::}(\Gamma; e_n)? {::} []) \Downarrow_{\Gamma;\fvalCtx} \tden{\titerm{\iota}}{\ttype{tycon}{\tauidx'}}\\
% %     \trepof{\tau_1'} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\
% % \Gamma \vdash_\fvalCtx \iota : \sigma
% %}{
% % \atjsynX{\FF{intro}[\tauidx](e_1; \ldots; e_n)] :: \fvar{tycon}}{\tau}{\iota'}
% %}
% %
% %\inferrule[att-elim]{
% % \atjsynX{e}{\ttype{tycon}{-}}{-}\\
% % \vdash_\fvalCtx \FF{esyn}(\fvar{tycon})=\taudef\\
% % \taudef~((\Gamma; e)? :: (\Gamma; e_1)? :: \ldots :: (\Gamma; e_n)? :: []) \Downarrow_{\Gamma;\fvalCtx} \tden{\tau}{\iota}\\
% %     \trepof{\tau} \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}\\
% % \Gamma \vdash_\fvalCtx \iota : \sigma
% %}{
% % \atjsynX{e\cdot\FF{elim}[\tauidx](e_1; \ldots; e_n)}{\tau}{\iota}
% %}
% %\end{mathpar}
% %\caption{\small The active typing judgement. The normalization judgement for type-level terms ($\Downarrow$) and the representational consistency check will be in an appendix.}
% %\label{atj}
% %\end{figure}
% %\begin{figure}
% %\small
% %\begin{mathpar}
% %\inferrule[repof]{
% % \tau \Downarrow_{\Gamma;\fvalCtx} \ttype{tycon}{\tauidx}\\
% % \vdash_\fvalCtx \FF{rep}(\fvar{tycon}) = \taurep\\
% % \taurep \tauidx \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}
% %}{
% % \FF{repof}(\tau) \Downarrow_{\Gamma;\fvalCtx} \titype{\sigma}
% %}
% %
% %\inferrule[syn]{
% %    \tau \Downarrow_{\Gamma;\fvalCtx} (\Gamma; e)?\\
% % \atjsynX{e}{\tau}{\iota}
% %}{
% % \FF{syn}(\tau) \Downarrow_{\Gamma;\fvalCtx} \tden{\titerm{\iota}}{\tau}
% %}
% %
% %\inferrule[ana]{
% % \tau_1 \Downarrow_{\Gamma;\fvalCtx} (\Gamma; e)?\\
% % \tau_2 \Downarrow_{\Gamma;\fvalCtx} \ttype{tycon}{\tauidx}\\\\
% % \atjanaX{e}{\ttype{tycon}{\tauidx}}{\iota}\\
% %}{
% % \FF{ana}(\tau_1; \tau_2) \Downarrow_{\Gamma;\fvalCtx} \titerm{\iota}
% %}
% %\end{mathpar}
% %\caption{\small Evaluation semantics for the type-level language. Missing rules (including error propagation rules, which immediately cause failure of typechecking) are unsurprising and will be given in an appendix.}
% %\label{tleval}
% %\end{figure}
% %The expressive and metatheoretic power of the calculus arises from how the rules for the active typing judgement handle these  generalized forms. Rather than fixing the specification of a finite collection of operator constructors and tasking the \emph{compiler} with deciding a typing derivation on its basis, the specification instead delegates to a type-level function associated with a type constructor. In the core calculus, this is one of three functions, called \FF{iana}, \FF{isyn} and \FF{esyn} in the grammar.\footnote{This means that any particular type constructor only supports a single introduction and elimination desugaring. This is a minor inconvenience in some cases that is resolved in \texttt{typy} by the use of methods.} We see how this is done with the rules for the active typing judgements, given in Figure \ref{atj}.
% %
% %The first two rules are standard in bidirectional type systems: variables synthesize types and if a term synthesizes a type, it can be analyzed against a type (cf. Listing \ref{context}). Variables translate to variables, and if we are simply converting from synthesis to analysis, translation is not affected.
% %
% %The rule $\rulename{att-lam}$ must take into account the fact that, because we support type-level computation, the type annotation on the argument may not be in normal form. Thus, we evaluate it to normal form. Note that the kinding rules (not shown) will guarantee that, because $\tau_1$ is of kind $\kTypeBlur$, its normal form, $\tau'_1$, is of the form $\ttype{tycon}{\tauidx}$ for some normal $\tauidx$. To generate an appropriate internal type annotation in the translation, we need to compute the representation type of $\tau_1'$. This involves calling the representation function associated with its type constructor (rule \fvar{repof} in Figure \ref{tleval}). The form $\titerm{\sigma}$ is a \emph{quoted internal type}. Note in the syntax that there is an unquote form, $\dup{\tau}$, which allows us to compose internal types compositionally without needing to expose an elimination form. Indeed, it is an interesting facet of our calculus that we never need to examine syntax trees directly to implement extensions. The evaluation semantics remove quotations, so the normal form of a quoted internal type contains an internal type with no quotations.
% %
% %Lambda functions are the only introductory form requiring special support in the calculus (because they need to  manipulate the context; see Discussion). The next three rules show how any other abstractions that we define (e.g. records, decimals, etc.) make use of a generalized introductory form. 
% %
% %The rule  $\rulename{att-i-unasc}$ shows that unascribed introductory forms can only be analyzed against a type. Given such a type, the rule extracts a definition, named $\FF{iana}$, from the type constructor (cf. the \lip{ana_Dict} and \lip{trans_Dict} methods earlier). It calls this function with a pair containing the operator and type index and a list of \emph{reified arguments}, which have kind $\Q$ and introductory form $(\Gamma; e)?$. These are only constructed by the compiler (there would be no corresponding form in the concrete syntax). Their purpose is to allow the definition to programatically invoke synthesis and analysis as needed using the $\FF{ana}$ and $\FF{syn}$ operators. We can see the $\fvar{record}$ type constructor doing so on line 5 of Figure \ref{record-theory} to ensure that the field value provided as an argument has the same type as the corresponding label (accomplished by simultaneously folding over all three pieces of input data). The rule for performing analysis, $\fvar{ana}$, is in Figure \ref{tleval}. If it succeeds, analysis returns a \emph{translation}, which is a quoted internal term with kind $\kITerm$ and introductory form $\titerm{\iota}$. Like quoted internal types, there is an unquote form that is eliminated during evaluation. The operator definition uses this to construct a translation for the record. As before, empty records translate to units and records with a single field are unadorned. In this example, records with two or more fields translate to nested tuples. 
% %
% %Because we have a representation type associated with the type, we can check that the translation is \emph{representationally consistent} (the final premise). As we will discuss, representational consistency combined with type safety of the internal language implies type safety overall (it arises as a strengthening of the inductive hypothesis needed to prove type safety, and is closely related to work on \emph{type-preserving compilation} in the TIL compiler for Standard ML, \cite{tarditi+:til-OLD}).
% %
% %The next rule, $\rulename{att-i-asc-ty}$ states that introductory forms ascribed with a type are analyzed against that type.
% %
% %Introductory forms ascribed with a type constructor can, according to the final introductory rule, $\rulename{att-i-asc-tycon}$, synthesize a type via the definition $\FF{isyn}$. It is passed the operator index (there is no type index, since we only have a type constructor) and a list of arguments, as before. In this case, it must return not just a translation, but also a type. We use the form $\tden{\tau_2}{\tau_1}$ for such a pairing, which has kind $\FF{TT}$. The type and translation can be extracted from it using the appropriately named elimination forms (indeed, as given, it is merely a pair, but we give it special syntax for clarity -- it looks like the conclusion of the typing judgement -- and for other reasons that will become clear in future work). To synthesize a type from a list of labels and arguments, we must be able to synthesize types from the arguments. The $\FF{syn}(\tau)$ operator permits this, per rule $\fvar{syn}$, seen being used in Listing \ref{record-theory}. We check representational consistency of the result, as before.
% %
% %Finally, we show the rule for elimination forms. It operates by first synthesizing a type for the ``primary'' subterm, then extracting the $\FF{esyn}$ definition from its type constructor. As before, it is called with the arguments and representational consistency is checked. Note that the primary subterm is itself the first argument (though we have already synthesized a type for it, it is more uniform and clear to allow the operator  to do so again, which is done in our example by the $\tvar{decons1}$ helper function).
% %
% %\subsubsection{Representational Consistency}
% %The representational consistency lemmas says that active typechecking and translation of an external term, if it succeeds (producing a type, $\hat\sigma$ and a translation $\iota$), always produces a well-typed translation. More specifically, the translation's type is the representation type of $\hat\sigma$.
% %
% %\begin{theorem}[Representational Consistency]
% %Given well-kinded contexts and an external term that is both well-kinded and well-typed:
% %\begin{enumerate}
% %\item $\vdash \Phi$ 
% %\item $\vdash_\Phi \Gamma$
% %\item $\vdash_\Phi e$
% %\item Either:
% %\begin{enumerate}
% %\item[a.] $\Gamma \vdash_\Phi e \Leftarrow \hat{\sigma} \leadsto \iota$
% %\item[b.] $\Gamma \vdash_\Phi e \Rightarrow \hat{\sigma} \leadsto \iota$
% %\end{enumerate}
% %\end{enumerate}
% %there exists an internal typing context, $\Omega$, and an internal type, $\tau$, such that  $\vdash_\Phi \Gamma \leadsto \Omega$ and $\FF{rep}(\hat\sigma) \Downarrow_\Phi \titype{\tau}$ and $\Omega \vdash \iota : \tau$.
% %\end{theorem}
% %\begin{proof}
% %The proof proceeds by induction on the typing derivation (4a or 4b). Because the representational consistency is explicitly checked in the rules for intro and elim forms, these cases follow directly. The remaining cases are standard and follow inductively, given standard lemmas about valid contexts and a lemma about type safety of the static language. This is a fixed, standard functional language with only a few simple additions (TODO detail this).
% %\end{proof}
% %
% %\subsubsection{Type Safety}
% %Representational safety implies that well-typed external terms produce well-typed internal terms. If the internal language is type safe, then we know that well-typed external terms do not ``go wrong''. The internal language is simply PCF with sums and products, and a base type of integers, so this is a completely standard result \cite{pfpl}.

% %\subsubsection{Decidability}
% %The metatheoretic properties of interest are: type safety and termination of the type-level language (guaranteed by our kind system, though the details will need to be provided in an appendix; termination is non-trivial), type safety and decidability of the internal language (it i.s a standard variant of PCF, so this is trivial) and representational consistency. Type safety for the language as a whole comes as a corollary of these lemmas. We plan to provide detailed proofs of these, but for now, they should be treated as conjectures and our formulation above as expository. 
% %We believe that despite this, the formalization is surprisingly elegant and concise, given its expressive power. It is a useful exercise to implement natural numbers, ala G\"odel's \textbf{T}, using this calculus, assuming that the type-level and internal languages include integers and one can lift them from the former into the latter.
% %%
% %
% %For the synthetic functions, it must decide the type assignment and the translation on the basis of the index and arguments, or decide that this is not possible. An operator constructor with index kind $\kappaidx$ must have a definition of kind \[\kappaidx \rightarrow \klist{\kDen} \rightarrow (\kDen + \kunit)\] 
% %As this suggests, the active typing judgement  will provide the operator index and a list of {recursively determined \emph{derivates}}, which have kind $\kDen$, for the arguments and ask the definition to return a value of ``option kind'', $\ksum{\kDen}{\kunit}$, where the trivial case indicates that a derivate cannot be constructed due to an invalid index, an incorrect number of arguments or an argument of an invalid type.\footnote{In practice, we would require operator constructor providers to report information about the precise location of the error (e.g. which argument was of incorrect type) and provide an appropriate error message and other metadata, but we omit this in the semantics.}
% %
% %
% %A \emph{program}, $\rho$, consists of a series of type constructor declarations followed by an external term, $e$. The syntax for external terms contains six forms: variables, $\lambda$ terms, a generalized unascribed introductory form, a generalized introductory form ascribed with a type, a generalized introductory form ascribed with a type constructor and a generalized elimination form. We will discuss how a more natural syntax  like Python's can be added by simple desugaring.\todo{syntax}
% %
% %
% %
% %Compiling a program consists of first \emph{kind checking} it (typechecking the type-level terms, which has no analog in \texttt{typy} because the type-level language is Python; discussed further below), then typechecking the external term and {translating} it to a term, $\iota$, in the {typed internal language}. %These correspond to the premises of the \emph{central compilation judgement} $\pkcompiles{\rho}{\iota}$:
% %%\[
% %%\inferrule[p-compiles]{%
% %%  \emptyset \vdash_{\fvalCtx_0} \rho\\
% %%  \progOK{\emptyset}{\fvalCtx_0}{\rho}\\
% %%  \pcompiles{\fvalCtx_0}{\rho}{\iota}
% %%}{\pkcompiles{\rho}{\iota}}
% %%\]
% %%his anchors our exposition; we will describe how it is derived (i.e. how to write a compiler for $\lam\texttt{typy}$) in the following sections. 
% %
% %The key judgement in the calculus is the \emph{active typing judgement}  (Fig. \ref{att}, which we describe starting in Sec. \ref{opcons}). It relates an external term, $e$, to a {type}, $\tau$, called its \emph{type assignment}, and an internal term, $\iota$, called its \emph{translation}, under \emph{typing context} $\Gamma$ and \emph{constructor context} $\Phi$: 
% %\[\ecompilesX{e}{\tau}{\iota}\]
% %
% %The typing context $\Gamma$ maps variables to types in essentially the conventional way (\cite{pfpl} contains the necessary background for this paper). The constructor context tracks user-defined type and operator constructors. % Weakening and exchange of the constructor context closely related to the issue of conservativity that we will return to. Each constructor is identified by name, so there is no analog to contraction.
% %There is no separate operational semantics for the external language. Instead, the dynamic behavior of an external term is determined by its translation to the internal language, which has a more conventional operational semantics. This form of semantics can be seen as lifting into the language specification the first stage of a type-directed compiler like the TIL compiler for Standard ML \cite{tarditi+:til-OLD} and has some parallels to the Harper-Stone semantics for Standard ML, where external terms were also given meaning by elaboration from the EL to an IL \cite{Harper00atype-theoretic}. %
% %
% %In $\lam\texttt{typy}$, the internal language (IL) provides partial functions (via the generic fixpoint operator of Plotkin's PCF), simple product and sum types and a base type of integers (to make our example interesting and as a nod toward speed on contemporary machines). In practice, the internal language could be any typed  language with a specification for which type safety and decidability of typechecking have been satisfyingly determined. The internal type system serves as a ``floor'': guarantees that must hold for \emph{all} well-typed terms, independent of the constructor context (e.g. that out-of-bounds access to memory never occurs), must be maintained by the internal type system. User-defined constructors can (only) enforce invariants stronger than those the internal type system maintains. Performance is also ultimately limited by the internal language and downstream compilation stages that we do not here consider (safe compiler extension has been discussed in previous work, e.g. \cite{conf/pldi/TatlockL10}).
% %


% %\begin{figure}
% %\small
% %\begin{flalign}
% %\small
% % & \family{Tuple}{\klist{\kTypeBlur}}{\\
% % & \quad \tops{new}{\kunit}{\_}{a}{\tlam{\_}{\kunit}{\tlam{a}{\klist{\kDen}}{\\
% % & \quad\quad
% %   \tfold{\tvar{a}}{\tden{{()}}{\ttype{Tuple}{\tnil{\kTypeBlur}}}}{d}{b}{r}{\\
% %     & \quad\quad\quad \tfamcase{\ttypeof{\tvar{r}}}{Ntuple}{i}{\\
% %     & \quad\quad\quad\quad \tfold{\tvar{b}}{\tden{\itransof{\tvar{d}}}{\ttype{Ntuple}{\tcons{\ttypeof{\tvar{d}}}{\tvar{i}}}}\\& \quad\quad\quad\quad}{\_}{\_}{\_}{\tden{\\& \quad\quad\quad\quad\quad {(\itransof{\tvar{d}},\itransof{\tvar{r}})}}{\ttype{Ntuple}{\tcons{{\ttypeof{\tvar{d}}}}{\tvar{i}}}}}
% %     }{\terr}
% % }
% % }}};\\
% % & \quad \tops{prj}{\kint}{i}{a}{\lambda \tvar{i}:\kint.\lambda \tvar{a}:\klist{\kDen}. 
% %   \tapp{\tvar{decons1}}{\tapp{\tvar{a}}{\tlam{d}{\kDen}{\\
% % & \quad\quad\tfamcase{\ttypeof{\tvar{d}}}{Ntuple}{nl}{\\
% % & \quad\quad \tfold{\tvar{nl}}{\terr}{t1}{j}{\_}{\\
% % & \quad\quad \tfold{\tvar{j}}{\tifeq{\tvar{i}}{1}{\dint}{\tden{\itransof{d}}{\tvar{t1}}}{\terr}\\&\quad\quad\quad}{\_}{\_}{\_}{\\
% % & \quad\quad\quad (\tlam{p}{\kpair{\kDen}{\dint}}{\tifeq{\tsnd{\tvar{p}}}{\tvar{i}}{\dint}{\tfst{\tvar{p}}}{\terr}})\\
% % & \quad\quad\quad (\tapp{\tvar{foldl}}{\tapp{\tvar{nl}}{\tapp{(\tden{\tvar{x}}{\tvar{nt}},0)}{
% %   \tlam{r}{\kpair{\kDen}{\dint}}{\tlam{t}{\kTypeBlur}{\tlam{ts}{\klist{\kTypeBlur}}{\\
% % & \quad\quad\quad\quad \tifeq{\tvar{i}}{\tsnd{\tvar{r}}}{\dint}{\tvar{r}}{\tdencase{\tfst{\tvar{r}}}{rx}{\_}{\\
% % & \quad\quad\quad\quad\quad \tifeq{\tvar{i}}{\tsnd{\tvar{r}}+1}{\dint}{\\
% % & \quad\quad\quad\quad\quad\quad \tifeq{\tvar{ts}}{\tnil{\kTypeBlur}}{\klist{\kTypeBlur}}{(\tden{\tvar{rx}}{\tvar{t}},\tvar{i})\\
% % & \quad\quad\quad\quad\quad\quad}{(\tden{\titerm{\ifst{\iup{\tvar{rx}}}}}{\tvar{t}},\tvar{i})}\\  
% % & \quad\quad\quad\quad\quad}{(\tden{\titerm{\isnd{\iup{\tvar{rx}}}}}{\tvar{t}},\tsnd{\tvar{r}}+1)}}{\terr}}}}}
% % }}})
% % }}{\terr}}}}}
% % }\\ & 
% %%& \quad \tops{pr}a}{b}{c}{d}
% % }{d}{i}{(\tlam{i}{\klist{\kTypeBlur}}{
% %   \\& \quad \quad \tfold{\tvar{i}}{\titype{\dunit}}{s}{j}{r}{
% %     \tfold{\tvar{j}}{\titype{\trepof{\tvar{s}}}}{\_}{\_}{\_}{\\
% %   &\quad\quad\quad
% %     \titype{\dpair{\trepof{\tvar{s}}}{\dup{\tvar{r}}}}
% %     }
% %   }
% % })}
% %\end{flalign}
% %\caption{ABC}
% %\label{tuple}
% %\end{figure}
% %\begin{figure}[t]
% %\small
% %$\fbox{\inferrule{}{\progOKX{\progsort}}}$
% %~~~$\tvarCtx ::= \emptyctx \pipe \tvarCtxX{t}{\kappa}$
% %~~~$\fvalCtx ::= \emptyctx \pipe \fvalCtxX{\fvalDf}$
% %%$\fCtx ::= \Sigma_0  \pipe \fvalCtxX$
% %\begin{mathpar}
% %\inferrule[k-tycon]{
% % \fvar{tycon} \notin \text{dom}(\fvalCtx)\\
% % \kEq{\kappaidx}\\
% % \tKind{\tvarCtx}{\fvalCtx}{\taurep}{\karrow{\kappaidx}{\kIType}}\\
% % \opType{\tvarCtx}{\fvalCtxX{\fvalDf}}{\theta}{\Theta}\\
% % \progOK{\tvarCtx}{\fvalCtxX{\fvalDf}}{\rho}
% %}{
% % \progOKX{\pfam{\familyDf}{\rho}}
% %}
% %
% %%\inferrule[def-kinding]{
% %%  \tKindX{\tau}{\kappa}\\
% %%  \progOK{\tvarCtxX{t}{\kappa}}{\fvalCtx}{\rho}
% %%}{
% %%  \progOKX{\pdef{t}{\kappa}{\tau}{\rho}}
% %%}
% %%
% %\inferrule[k-e-prog]{
% % \exprOK{\tvarCtx}{\fvalCtx}{e}
% %}{
% % \progOKX{e}
% %}
% %\end{mathpar}
% %$\fbox{$\opType{\tvarCtx}{\fvalCtx}{\theta}{\Theta}$}$
% %\begin{mathpar}
% %\inferrule[k-opcon]{
% % \tKind{\tvarCtx}{\fvalCtx}{\taudef}{\karrow{\kappaidx}{\karrow{\klist{\kDen}}{(\ksum{\kDen}{\kunit})}}}
% %}{
% % \opType{\tvarCtx}{\fvalCtx}{{\tops{op}{\kappaidx}{i}{a}{\taudef}}}{
% % \kOpS{op}{\kappaidx}}
% %}
% %
% %\inferrule[k-opcons]{
% % \opType{\tvarCtx}{\fvalCtx}{\theta_1}{\Theta_1}\\
% % \opType{\tvarCtx}{\fvalCtx}{\theta_2}{\Theta_2}\\\\
% % \text{dom}(\theta_1) \cap \text{dom}(\theta_2) = \emptyset
% %}{
% % \opType{\tvarCtx}{\fvalCtx}{\theta_1; \theta_2}{\Theta_1, \Theta_2}
% %}
% %\end{mathpar}
% %$\fbox{$\exprOKX{e}$}$
% %\begin{mathpar}
% %\inferrule[k-e-var]{ }{
% % \exprOK{\tvarCtx}{\fvalCtx}{\evar{x}}
% %}
% %
% %\inferrule[k-e-lam]{
% % \tKindX{\tau}{\kTypeBlur}\\
% % \exprOK{\tvarCtx}{\fvalCtx}{e}
% %}{
% % \exprOKX{\elam{\evar{x}}{\tau}{e}}
% %}
% %
% %\inferrule[k-e-op]{
% % \fval{tycon}{-}{-}{\theta} \in \fvalCtx\\
% % \tops{op}{\kappaidx}{i}{a}{-} \in \theta\\
% % \tKindX{\tauidx}{\kappaidx}\\
% % \exprOKX{e_1}\\
% % \cdots\\
% % \exprOKX{e_n}
% %}{
% % \exprOKX{\eop{tycon}{op}{\tauidx}{\splat{e}{1}{n}}}
% %}
% %\end{mathpar}
% %$\fbox{$\kEq{\kappa}$}$
% %\begin{mathpar}
% %\inferrule[t-eq]{ }{
% % \kEq{\kTypeBlur}
% %}
% %
% %\inferrule[i-eq]{ }{
% % \kEq{\kint}
% %}
% %
% %\inferrule[l-eq]{ }{
% % \kEq{\klabel}
% %}
% %
% %\inferrule[list-eq]{
% % \kEq{\kappa}
% %}{
% % \kEq{\klist{\kappa}}
% %}
% %\\
% %\inferrule[u-eq]{ }{
% % \kEq{\kunit}
% %}
% %
% %\inferrule[p-eq]{
% % \kEq{\kappa_1}\\
% % \kEq{\kappa_2}
% %}{
% % \kEq{\kpair{\kappa_1}{\kappa_2}}
% %}
% %
% %\inferrule[s-eq]{
% % \kEq{\kappa_1}\\
% % \kEq{\kappa_2}
% %}{
% % \kEq{\ksum{\kappa_1}{\kappa_2}}
% %}
% %\end{mathpar}
% %\caption{\small Kinding for type and operator constructors and external terms, and equality kinds (see text).  Type-level terms stored in the constructor context are not needed during kinding, indicated using a dash.}
% %\label{kindprog}
% %\vspace{-10pt}
% %\end{figure}


% %\section{Interactive Compilation and Invocation}\label{targets}
% %\subsection{OpenCL as an Active Library}
% %The code in this section uses \lip{clx}, an example library implementing the semantics of the OpenCL programming language and extending it with some additional useful types, which we will discuss shortly. \texttt{typy} itself has no built-in support for OpenCL.
% %
% %To briefly review, OpenCL provides a data-parallel SPMD programming model where developers define functions, called {\em kernels}, for execution on \emph{compute devices} like GPUs or multi-core CPUs \cite{opencl11}. Each thread executes the same kernel but has access to a unique index, called its \emph{global ID}. Kernel code is written in a variant of C99 extended with some new primitive types and operators, which we will introduce  as needed in our examples below.
% %
% %\subsection{Generic Functions}\label{genfn}
% %%\begin{codelisting}[t]
% %%\lstinputlisting[commentstyle=\color{mauve}]{listing7.py}
% %%\caption{[\texttt{listing\ref{metaprogramming}.py}] Metaprogramming with \texttt{typy}, showing how to construct generic functions from abstract syntax trees.}
% %%\label{metaprogramming}
% %%\end{codelisting}
% %
% %%\begin{codelisting}
% %%\lstinputlisting[linebackgroundcolor={\btLstHL{5-8}}]{listing3.py}
% %%\caption{[\texttt{listing\ref{map}.py}] A generic data-parallel higher-order map function targeting OpenCL.}
% %%\label{map}
% %%\end{codelisting}
% %%\begin{codelisting}[t]
% %%\lstinputlisting[commentstyle=\color{mauve}]{listing7.py}
% %%\caption{[\texttt{listing\ref{metaprogramming}.py}] Metaprogramming with \texttt{typy}, showing how to construct generic functions from abstract syntax trees.}
% %%\label{metaprogramming}
% %%\end{codelisting}
% %Lines 3-4 introduce \lip{map}, an \texttt{typy} function of three arguments that is governed by the \emph{active base} referred to by \lip{clx.base} and targeting the \emph{active target} referred to by \lip{clx.opencl}. The active target determines which language the function will compile to (here, the OpenCL kernel language) and mediates code generation. 
% %
% %The body of this function, highlighted in grey for emphasis, does not have Python's semantics. Instead, it will be governed by the active base together with the active types used within it. No such types have been provided explicitly, however. Because our type system is extensible, the code inside could be meaningful for many different assignments of types to the arguments. We call functions awaiting types \emph{generic functions}. Once types have been assigned, they are called \emph{concrete functions}.
% %
% %Generic functions are represented at compile-time as instances of \lip{ace.GenericFn} and consist of an abstract syntax tree, an {active base} and an {active target}. The purpose of the \emph{decorator} on line 3 is to replace the Python function on lines 4-8 with an \texttt{typy} generic function having the same syntax tree and the provided active base and active target. 
% %Decorators in Python are simply syntactic sugar for applying the decorator function directly to the function  being decorated \cite{python}. In other words, line 3 could be replaced by inserting the following  statement on line 9:
% %\vspace{-0.28cm}
% %\begin{verbbox}
% %map = ace.fn(clx.base, clx.opencl)(map)
% %\end{verbbox}
% %\begin{figure}[h!]
% %\centering
% %\theverbbox
% %\end{figure}
% %\vspace{-0.28cm}
% %
% %The abstract syntax tree for \lip{map} is extracted using the Python standard  library packages  \lip{inspect} (to retrieve its source code) and \lip{ast} (to parse it into a syntax tree). 
% %
% %%\subsection{Metaprogramming in \texttt{typy}}
% %%Generic functions can be generated directly from ASTs as well, providing \texttt{typy} with support for  straightforward metaprogramming. Listing \ref{metaprogramming} shows how to generate two more generic functions, \lip{scale} and \lip{negate}. The latter is derived from the former by using a library for manipulating Python syntax trees, \lip{astx}. In particular, the \lip{specialize} function replaces uses of the second argument of \lip{scale} with the literal \lip{-1} (and changes the function's name), leaving a function of one argument.
% % 
% %\subsection{Concrete Functions and Explicit Compilation}
% %To compile a generic function to a particular \emph{concrete function}, a type must be provided for each argument, and typechecking and translation must then succeed. Listing \ref{compscript} shows how to explicitly provide type assignments to \lip{map} using the subscript operator (implemented using Python's operator overloading mechanism). We attempt to do so three times in Listing \ref{compscript}. The first, on line \ref{compscript}.7, fails due to a type error, which we handle so that the script can proceed. The error occurred  because the ordering of the argument types was incorrect. We provide a valid ordering on line \ref{compscript}.9 to generate the concrete function \lip{map_neg_f32}. We then provide a different type assignment to generate the concrete function \lip{map_neg_ci32}.
% %Concrete functions are instances of \lip{ace.TypedFn}, consisting of an abstract syntax tree annotated with types and translations along with a reference to the original generic function. %The typing and translation process is mediated by the logic in the active base, types and target that have been provided, as we will describe in more detail below. 
% %
% %To produce an output file from an \texttt{typy} ``compilation script'' like \lip{listing}\texttt{\ref{compscript}}\lip{.py}, the command \lip{acec} can be invoked from the shell, as shown in Listing \ref{mapc}. 
% %\subsection{Types}
% %\begin{codelisting}
% %\lstinputlisting{listing4.py}
% %\caption{[\texttt{listing\ref{compscript}.py}] The generic \texttt{map} function compiled to map the \texttt{negate} function over two  types of input.}
% %\label{compscript}
% %\end{codelisting}
% %\begin{codelisting}
% %\begin{lstlisting}[style=Bash]
% %$ `acec listing3.py`
% %Hello, compile-time world!
% %[ace] TypeError in listing1.py (line 6, col 28): 
% %      'GenericFnType(negate)' does not support [].
% %[acec] listing3.cl successfully generated.
% %\end{lstlisting}
% %\caption{Compiling \texttt{listing\ref{compscript}.py} using the \texttt{acec} compiler.}
% %\label{mapc}
% %\end{codelisting}
% %\begin{codelisting}
% %\lstinputlisting[style=OpenCL]{listing5.cl}
% %\caption{[\texttt{listing\ref{compscript}.cl}] The OpenCL file generated by Listing \ref{mapc}.}
% %\label{mapout}
% %\end{codelisting}
% %%Lines \ref{compscript}.3-\ref{compscript}.5 construct the types assigned to the arguments of \lip{map} on lines \ref{compscript}.7-\ref{compscript}.10. In \texttt{typy}, types are themselves values that can be manipulated at compile-time. This stands in contrast to other contemporary languages, where user-defined types (e.g. datatypes, classes, structs) are written declaratively at compile-time but cannot be constructed, inspected or passed around programmatically. More specifically, types are instances of a Python class that implements the \lip{ace.ActiveType} interface (see Sec. \ref{atypes}). 
% %%As Python values, types can be assigned to variables when convenient (removing the need for  facilities like \lip{typedef} in C or \lip{type} in Haskell). Types, like all compile-time objects derived from \texttt{typy} base classes, do not have visible state and operate in a referentially transparent manner (by constructor memoization, which we do not detail here).% These types are all implemented in the \lip{clx} library imported on line 1, none are built into \texttt{typy} itself.
% %
% %The type named \lip{T1} on line \ref{compscript}.3 corresponds to the OpenCL type \lip{global float*}: a pointer to a 32-bit floating point number stored in the compute device's global memory (one of four address spaces defined by OpenCL \cite{opencl11}). It is constructed by applying \lip{clx.Ptr}, which is an \texttt{typy} type constructor corresponding to pointer types, to a value representing the  address space, \lip{clx.global_}, and the type being pointed to. That type, \lip{clx.float}, is in turn the \texttt{typy} type corresponding to \lip{float} in OpenCL (which, unlike C99, is always 32 bits). 
% %The \lip{clx} library contains a full implementation of the OpenCL type system (including behaviors, like promotions, inherited from C99).
% %\texttt{typy} is \emph{unopinionated} about issues like memory safety and the wisdom of such promotions. We will discuss how to implement, as libraries, abstractions that are higher-level than raw pointers in Sec. \ref{examples}, but \texttt{typy} does not prevent users from choosing a low level of abstraction or ``interesting'' semantics if the need arises (e.g. for compatibility with existing libraries; see the discussion in Sec. \ref{discussion}). We also note that we are being more verbose than necessary for the sake of pedagogy. The \lip{clx} library includes more concise shorthand for OpenCL's types: \lip{T1} is equal to \lip{clx.gp(clx.f32)}. %Similarly, the decorators in Listings \ref{map} and \ref{metaprogramming} could have been written \lip{clx.cl_fn}.\todo{move this back there probably}
% %
% %The type \lip{T2} on line \ref{compscript}.4 is a pointer to a \emph{complex integer} in global memory. It does not correspond direrctly to a type in OpenCL, because OpenCL does not include primitive support for complex numbers. Instead, it uses an active type constructor \lip{clx.Cplx}, which includes the necessary logic for typechecking operations on complex numbers and translating them to OpenCL (Sec. \ref{atypes}). This constructor is parameterized by the numeric type that should be used for the real and imaginary parts, here \lip{clx.int}, which corresponds to 32-bit OpenCL integers. Arithmetic operations with other complex numbers, as well as with plain numeric types (treated as if their imaginary part was zero), are supported. When targeting OpenCL, \texttt{typy} expressions assigned type \lip{clx.Cplx(clx.int)} are compiled to OpenCL expressions of type \lip{int2}, a  \emph{vector type} of two 32-bit integers (a type that itself is not inherited from C99). This can be observed in several places on lines \ref{mapout}.14-\ref{mapout}.21. This choice is merely an implementation detail that can be kept private to \lip{clx}, however. An \texttt{typy} value of type \lip{clx.int2} (that is, an actual OpenCL vector) \emph{cannot} be used when a \lip{clx.Cplx(clx.int)} is expected (and attempting to do so will result in a static type error): \lip{clx.Cplx} truly extends the type system, it is not a type alias.
% %
% %The type \lip{TF} on line \ref{compscript}.5 is extracted from the generic function \lip{negate} constructed in Listing \ref{metaprogramming}. Generic functions, according to Sec. \ref{genfn}, have not yet had a type assigned to them, so it may seem perplexing that we are nevertheless assigning a type to \lip{negate}. Although a conventional arrow type cannot be assigned to \lip{negate}, we can give it a \emph{singleton type}: a type that simply means ``this expression is the \emph{particular} generic function \lip{negate}''. This type could also have been explicitly written as \lip{ace.GenericFnType(listing2.negate)}. During typechecking and translation of \lip{map_neg_f32} and \lip{map_neg_ci32}, the call to \lip{f} on line \ref{map}.6 uses the type of the provided argument to compile the generic function that inhabits the singleton type of \lip{f} (\lip{negate} in both of these cases) to a concrete function. This is why there are two versions of \lip{negate} in the output in Listing \ref{mapout}. In other words, types \emph{propagate} into generic functions -- we didn't need to compile \lip{negate} explicitly. This also explains the error printed on line \ref{mapc}.3-\ref{mapc}.4: when this type was inadvertently assigned to the first argument \lip{input}, the indexing operation on line \ref{map}.6 resulted in an error. A generic function can only be \emph{statically} indexed by a list of types to turn it into a concrete function, not \emph{dynamically} indexed with a value of type \lip{clx.size_t} (the return type of the OpenCL primitive function \lip{get_global_id}).
% %
% %In effect, this scheme enables higher-order functions even when targeting languages, like OpenCL, that have no support for higher-order functions (OpenCL, unlike C99, does not support function pointers). Interestingly, because they have a singleton type, they are higher-order but not first-class functions. That is, the type system would prevent you from creating a heterogeneous list of generic functions. Concrete functions, on the other hand, can be given both a singleton type and a true function type. For example, \lip{listing2.negate[[clx.int]]} could be given type \lip{ace.Arrow(clx.int, clx.int)}. The base determines how to convert the \texttt{typy} arrow type to an arrow type in the target language (e.g. a function pointer for C99, or an integer that indexes into a jump table constructed from knowledge of available functions of the appropriate type in OpenCL).
% %
% %Type assignment to generic functions is similar in some ways to template specialization in C++. In effect, both a template header and type parameters at call sites are being generated automatically by \texttt{typy}. This simplifies a sophisticated feature of C++ and enables its use with other targets like OpenCL. %Other uses for C++ templates (and the preprocessor) are subsumed by the metaprogramming features discussed above\todo{cite template metaprogramming paper}.
% %
% %%\subsection{Within-Function Type Resolution}
% %%\begin{codelisting}
% %%\lstinputlisting{listing6.py}
% %%\caption{\texttt{[listing6.py]} A function demonstrating whole-function type inference when multiple values with differing types are assigned to a single identifier, \texttt{y}.}
% %%\label{inference}
% %%\end{codelisting}
% %%On line 5 in the generic \lip{map} function in Listing \ref{map}, the variable \lip{gid} is initialized with the result of calling the OpenCL primitive \lip{get_global_id}.  The type for \lip{gid} is never given explicitly. This is a simple case of \texttt{typy}'s {\em within-function type resolution} strategy (we hesitate to call it \emph{type inference} because it does not, strictly speaking, take a constraint-solving approach). In this case, the type of \lip{gid} will resolve to \lip{size_t} because that is the return type of \lip{get_global_id} (as defined in the OpenCL specification, which the \lip{ace.OpenCL} module follows). The result can be observed on Lines 11 and 24 in Listing \ref{mapout}. 
% %%
% %%Inference is not restricted within single assignments, as in the \lip{map} example, however. Multiple assignments to the same identifier with values of differing types, or multiple return statements, can be combined if the types in each case are compatible with one another (e.g. by a subtyping relation or an implicit coercion). In Listing \ref{inference}, the \lip{threshold_scale} function assigns different values to \lip{y} in each branch of the conditional. In the first branch, the value \lip{0} is an \lip{int} literal. However, in the second branch of the loop, the type depends on the types of both arguments, \lip{x} and \lip{scale}. We show two choices for these types on Lines 11 and 12. Type inference correctly combines these two types according to OpenCL's C99-derived rules governing numeric types (defined by the user in the \lip{OpenCL} module, as we will describe in Section \ref{att}). We can verify this programmatically on Lines 12 and 13. Note that this example would also work correctly if the assignments to \lip{y} were replaced with \lip{return} statements (in other words, the return value of a function is treated as an assignable for the purpose of type inference).
% %
% %\subsection{Implicit Compilation and Interactive Execution}\label{compenv}\label{backend}\label{implicit}
% %%Professional end-user programmers (e.g. scientists and engineers \cite{professional-end-users}) today generally use dynamically-typed high-level languages like MATLAB, Python, R or Perl for tasks that are not performance-sensitive, such as small-scale data analysis and plotting \cite{nguyen2010survey}. For portions of their analyses where the performance overhead of dynamic type checking and automatic memory management is too high, they will typically call into code written in a statically-typed, low-level language, most commonly C or Fortran, that uses low-level parallel abstractions like pthreads and MPI \cite{4222616,basili2008understanding}. Unfortunately, these low-level languages and abstractions are notoriously difficult to use and automatic verification is intractable in general.\todo{integrate this}
% %
% %
% %A common workflow for \emph{professional end-user programmers} (e.g. scientists and engineers) is to use a simple scripting language for orchestration, small-scale data analysis and visualization and call into a low-level language for performance-critical sections. Python is both designed for this style of use and widely adopted for such tasks \cite{sanner1999python,nguyen2010survey}. Developers can call into native functions using Python's foreign function interface (FFI), for example. A more recent trend is to generate and compile code without leaving Python, using a Python wrapper around a compiler. For example, \lip{weave} works with C and C++, and \lip{pycuda} and \lip{pyopencl} work with CUDA and OpenCL, respectively \cite{klockner2011pycuda}. 
% %\begin{codelisting}
% %\lstinputlisting{listing8.py}
% %\caption{[\texttt{listing\ref{py}.py}] A full OpenCL program using the \texttt{clx} Python bindings, including data transfer to and from a device and direct invocation of a generic function, \texttt{map}.}
% %\label{py}
% %\end{codelisting}
% %The OpenCL language was designed for this workflow, exposing a retargetable compiler and data management  routines as an API, called the \emph{host API} \cite{opencl11}. The \lip{pyopencl} library exposes this API to Python and simplifies interoperation with \lip{numpy}, a popular package for manipulating contiguously-allocated numeric arrays in Python \cite{klockner2011pycuda}.
% %
% %\texttt{typy} supports a refinement to this workflow, as an alternative to the \lip{acec} compiler described above, for targets that have wrappers like this available, including \lip{clx.opencl}. Listing \ref{py} shows an example of this workflow where the user chooses a compute device (line \ref{py}.3),  constructs a \lip{numpy} array (line \ref{py}.5), transfers it to the device (line \ref{py}.6), allocates an empty equal-sized buffer for the result of the computation (line \ref{py}.7), launches the {generic} kernel \lip{map} from Listing \ref{map} with these device arrays as well as the function \lip{negate} from Listing \ref{metaprogramming} (line \ref{py}.9) choosing a number of threads equal to the number of elements in the input array (line \ref{py}.10), and transfers the result back into main memory to check that the device computed the expected result (line \ref{py}.12).
% %
% %For developers experienced with the usual OpenCL or CUDA workflow, the fact that this can be accomplished in a total of 6 statements may be surprising. This simplicity is made possible by \texttt{typy}'s implicit tracking of types throughout the code. First, \lip{numpy} keeps track of the type, shape and order of its arrays. The type of \lip{input}, for example, is \lip{numpy.float64} by default, its shape is \lip{(1024,1024)} and its order is row-major by default. The \lip{pyopencl} library, which the \lip{clx} mechanism is built upon, uses this metadata to automatically call the underlying OpenCL host API function for transferring byte arrays to the device without requiring the user to calculate the size. The \lip{clx} wrapper further retains this metadata in \lip{d_input} and \lip{d_output}, the Python wrappers around the allocated device arrays. The \texttt{typy} active type of these wrappers is an instance of \lip{clx.NPArray} parameterized by this metadata. This type knows how to typecheck and translate operations, like indexing with a multi-dimensional thread index, automatically.
% %
% %By extracting the types from the arguments, we can call the generic function \lip{map} without first requiring an explicit type assignment, like we needed when using \lip{acec} above. In other words, dynamic types and other metadata can propagate from Python data structures into an \texttt{typy} generic function as static type information, in the same manner as it propagated \emph{between} generic functions in the previous section. In both cases, typechecking and translation of \lip{map} happens the first time a particular type assignment is encountered and cached for subsequent use. When called from Python, the generated OpenCL source code is compiled for the device we selected using the OpenCL retargetable compilation infrastructure, and cached for subsequent calls. 
% %
% %The same program written using the OpenCL C API directly is an order of magnitude longer and significantly more difficult to comprehend. OpenCL does not support higher-order functions nor is there any way to write \lip{map} in a type-generic or shape-generic manner. If we instead use the \lip{pyopencl} library and apply the techniques described in \cite{klockner2011pycuda}, the program is still twice as large and less readable than this code. Both the \lip{map} and \lip{negate} functions must be explicitly specialized with the appropriate types using string manipulation techniques, and custom shapes and orders can be awkward to handle. Higher order functions are still not available, and must also be simulated by string manipulation. That approach also does not permit the use any of the language extensions that \texttt{typy} enables (beyond the useful type for \lip{numpy} arrays just described; see Sec. \ref{examples} for more interesting possibilities).
% %
% %%Not shown are several additional conveniences, such as delegated kernel sizing and \lip{In} and \lip{Out} constructs that can reduce the size and improve the clarity of this code further; due to a lack of space, the reader is referred to the language documentation.


\clearpage
% We recommend abbrvnat bibliography style.
% %\newpage
\bibliographystyle{abbrv}

% % The bibliography should be embedded for final submission.

\bibliography{../research}
% %\softraggedright
% %P. Q. Smith, and X. Y. Jones. ...reference text...
% %
% %\appendix
% %\section{Source Code Emission}
% %Portions of the \lip{Target} class related to source code emission are shown. If compilation occurred externally, the \lip{acec} library asks the targets used by the top-level typed functions to generate one or more files by deciding on a name and file extension on the basis of the name of the compilation script, and emitting code and \emph{snippets}, which are simply strings corresponding to functions, type declarations, imports and other top-level entities in the target language, each inserted at a \emph{location}. Each file being generated is a location, and locations might also be nested, depending on the language (e.g. the import block may be a separate location when generating Java, or the body of the \lip{<head>} tag when generating HTML+CSS+Javascript). Snippets are only added once to a particular location (so imports are not duplicated, for example).
% %%We leave layering extensions to Python to created typed targets as future work.
% %
% %\begin{codelisting}
% %\begin{lstlisting}
% %class PyTarget(ace.Target):
% %  class PyAST(object): """Base class for ASTs"""
% %  target_type = ast
% %  class Attribute(PyAST, ast.Attribute):
% %    def emit(self, cg):
% %      cg.append(self.value.emit(cg), '.', self.attr)
% %  # ... 
% %  class AnonModule(object): 
% %    def __init__(self, name): 
% %      self.name = name
% %      self.guid = Guid()
% %      
% %    def emit(self, cg):
% %      import_loc = cg['imports']
% %      _cache = AnonModule._idx_cache[import_loc]
% %      if self not in _cache:
% %        anon_name = '__ace_' + str(len(_cache)+1)
% %        imp_stmt = ('import ' + self.name + ' as ' + 
% %          anon_name + cg.newline)
% %        _cache[self] = (anon_name, imp_stmt)
% %      else: 
% %        anon_name, imp_stmt = _cache[self]
% %      imports.add_snippet(imp_stmt)
% %      cg.append(anon_name)
% %      
% %  def emit(self, script_name, cg):
% %    file_name = '_' + script_name + '.py'
% %    if file_name not in generator:
% %      if 'imports' not in generator[file_name]:
% %        generator[file_name].push_loc('imports')
% %      if 'main' not in format_set[file_name]:
% %        generator[file_name].push_loc('main')
% %    translation.emit((file_name, 'main'))
% %\end{lstlisting}
% %\caption{Portions of the target showing how non-local code emission works.}
% %\label{cg}
% %\end{codelisting}
% %
% %\section{Kinding}
% %\begin{figure}[t]
% %\small
% %%\vspace{-15pt}
% %$\fbox{\inferrule{}{\tKindX{\tau}{\kappa}}}$
% %%~~~$\chi ::= \cdot \pipe \checkmark$
% %\begin{mathpar}
% %\small\inferrule[k-var]{
% %}{
% %  \tKind{\tvarCtxX{t}{\kappa}}{\fvalCtx}{\tvar{t}}{\kappa}
% %}
% %~~~~
% %\inferrule[k-arrow-i]{
% %  \tKind{\tvarCtxX{t}{\kappa_1}}{\fvalCtx}{\tau}{\kappa_2}
% %}{
% %  \tKindX{\tlam{t}{\kappa_1}{\tau}}{\karrow{\kappa_1}{\kappa_2}}
% %}
% %~~~~
% %\inferrule[k-arrow-e]{
% %  \tKindX{\tau_1}{\karrow{\kappa_1}{\kappa_2}}\\\\
% %  \tKindX{\tau_2}{\kappa_1}
% %}{
% %  \tKindX{\tapp{\tau_1}{\tau_2}}{\kappa_2}
% %}
% %
% %\text{\color{gray} (kinding for integers, labels, lists, products and sums also standard)}
% %
% %%%
% %%%\inferrule{ }{
% %%% \tKindX{\tstr{str}}{\kstr}
% %%%}(\text{str}^I_\tau)
% %%%
% %%%\inferrule{ }{
% %%% \tKindX{\tunit}{\kunit}
% %%%}(\text{1}^I_\tau)
% %%%
% %%%\inferrule{
% %%% \tKindX{\tau_1}{\kappa_1}\\
% %%% \tKindX{\tau_2}{\kappa_2}
% %%%}{
% %%% \tKindX{\tpair{\tau_1}{\tau_2}}{\kpair{\kappa_1}{\kappa_2}}
% %%%}({\times}^I_\tau)
% %%%
% %%%\inferrule{
% %%% \tKindX{\tau}{\kpair}{\kappa_1}{\kappa_2}
% %%%}{
% %%% \tKindX{\tfst{\tau}}{\kappa_1}
% %%%}({\times}^{E1}_\tau)
% %%%
% %%%\inferrule{
% %%% \tKindX{\tau}{\kpair}{\kappa_1}{\kappa_2}
% %%%}{
% %%% \tKindX{\tsnd{\tau}}{\kappa_2}
% %%%}({\times}^{E2}_\tau)
% %%%\inferrule{ }{
% %%%\inferrule{
% %%% \tKindX{\tau_{1}}{\kappa_{1}}\\
% %%% \tKindX{\tau_{2}}{\kappa_{2}}
% %%%}{
% %%% \tKindX{\tpair{\tau_{1}}{\tau_{2}}}{\kpair{\kappa_{1}}{\kappa_{2}}}
% %%%}~(\times_\tau)
% %%%
% %%%\inferrule{
% %%% \tKindX{\tau}{\kpair{\kappa_{1}}{\kappa_{2}}}
% %%%}{
% %%% \tKindX{\tfst{\tau}}{\kappa_{1}}
% %%%}~(\text{fst}_\tau)
% %%%
% %%%\inferrule{
% %%% \tKindX{\tau}{\kpair{\kappa_{1}}{\kappa_{2}}}
% %%%}{
% %%% \tKindX{\tsnd{\tau}}{\kappa_{2}}
% %%%}~(\text{snd}_\tau)
% %%%
% %%% TODO: list
% %\inferrule[k-eq]{
% % \kEq{\kappa}\\
% % \tKindX{\tau_1}{\kappa}\\
% % \tKindX{\tau_2}{\kappa}\\\\
% % \tKindX{\tau_3}{\kappa'}\\
% % \tKindX{\tau_4}{\kappa'}
% %}{
% % \tKindX{\tifeq{\tau_1}{\tau_2}{\kappa}{\tau_3}{\tau_4}}{\kappa'}
% %}
% %
% %\inferrule[k-ty-i]{
% % \fval{tycon}{\kappaidx}{-}{-} \in \fvalCtx\\\\
% % \tKind{\tvarCtx}{\fvalCtx}{\tauidx}{\kappaidx}
% %}{
% % \tKindX{\ttype{tycon}{\tauidx}}{\kTypeBlur}
% %}
% %
% %\inferrule[k-ty-e]{
% % \tKindX{\tau}{\kTypeBlur}\\
% % \fval{tycon}{\kappaidx}{-}{-} \in \fvalCtx\\\\
% % \tKind{\tvarCtxX{x}{\kappaidx}}{\fvalCtx}{\tau_1}{\kappa}\\
% % \tKindX{\tau_2}{\kappa}
% %}{
% % \tKind{\tvarCtx}{\fvalCtx}{\tfamcase{\tau}{tycon}{x}{\tau_1}{\tau_2}}{\kappa}
% %}
% %~~~~~~
% %\inferrule[k-d-i]{
% % \tKindX{\tau_1}{\kTypeBlur}\\\\
% % \tKindX{\tau_2}{\kITerm}
% %}{
% % \tKindX{\tden{\iota}{\tau}}{\kDen}
% %}
% %
% %%\inferrule[k-d-i-checked]{
% %%  \tKind{\Delta}{\fvalCtx}{\tau}{\kTypeBlur}\\\\
% %%  \isItermX{\ibar}
% %%}{
% %%  \tKind{\Delta}{\fvalCtx}{\tden{\ibar}{\tau}^\checkmark}{\kDen}
% %%}
% %%~~~~~~~
% %\inferrule[k-d-e]{
% % \tKindX{\tau}{\kDen}
% %}{
% % \tKindX{\ttypeof{\tau}}{\kTypeBlur}
% %}
% %%
% %%\inferrule[iterm-intro]{
% %%  \isIterm{\tvarCtx}{\emptyctx}{\fvalCtx}{\iota}
% %%}{
% %%  \tKindX{\titerm{\iota}}{\kITerm}
% %%}
% %
% %\inferrule[k-reptype-i]{
% % \isItype{\tvarCtx}{\fvalCtx}{\sbar}
% %}{
% % \tKindX{\titype{\sigma}}{\kIType}
% %}
% %\end{mathpar}
% %%$\fbox{$\isItermX{\ibar}$}$
% %%\begin{mathpar}
% %%\inferrule[k-i-var]{ }{
% %%  \isItermX{\evar{x}}
% %%}
% %%~~~~~~
% %%\inferrule[k-i-fix]{
% %%  \isItypeX{\sbar}\\
% %%  \isItermX{\ibar}
% %%}{
% %%  \isItermX{\ifix{\evar{x}}{\sbar}{\ibar}}
% %%}
% %%~~~~~~
% %%\inferrule[k-i-transof]{
% %%  \tKindX{\tau}{\kDen}
% %%}{
% %%  \isItermX{\itransof{\tau}}
% %%}
% %%%\inferrule[i-lam-kinding]{
% %%% \isItypeX{\sbar}\\\\
% %%% \isItermX{\ibar}
% %%%}{
% %%% \isItermX{\ilam{\evar{x}}{\sbar}{\ibar}}
% %%%}
% %%
% %%\text{\color{gray} (omitted rules are trivially recursive, like \textsc{k-i-fix})}
% %%%\inferrule[iterm-dereify-kinding]{
% %%% \tKindX{\tau}{\kITerm}
% %%%}{
% %%% \isItermX{\iup{\tau}}
% %%%}
% %%%
% %%\end{mathpar}
% %$\fbox{$\isItypeX{\sbar}$}$
% %\begin{mathpar}
% %%\inferrule[i-int-kinding]{ }{
% %%  \isItypeX{\dint}
% %%}
% %%
% %%\inferrule[i-prod-kinding]{
% %%  \isItypeX{\sigma_1}\\\\
% %%  \isItypeX{\sigma_2}
% %%}{
% %%  \isItypeX{\dpair{\sigma_1}{\sigma_2}}
% %%}
% %%
% %\inferrule[k-s-unquote]{
% % \tKindX{\tau}{\kIType}
% %}{
% % \isItypeX{\dup{\tau}}
% %}
% %
% %%\inferrule[k-s-repof]{
% %%  \tKindX{\tau}{\kTypeBlur}
% %%}{
% %%  \isItypeX{\trepof{\tau}}
% %%}
% %%
% %\text{\color{gray} (omitted forms are trivially recursive)}
% %\end{mathpar}
% %\caption{\small Kinding for type-level terms. The kinding context $\tvarCtx$ maps type variables to kinds.}
% %\label{tlkind}
% %\end{figure} 
% %Some of the kinding rules for type-level terms are shown.
% %
% %\section{Complex Numbers}
% %We have implemented complex numbers (which have a logic similar to decimals, which we added late in the paper cycle). The API is somewhat dated, apologies for
% %
% %If \lip{c} is a complex number, then \lip{c.ni} and \lip{c.i} are its non-imaginary and imaginary components, respectively. These expressions are of the form \lip{Attribute}, so the typechecker calls \lip{type_Attribute} (line \ref{cplx}.7).
% %This method receives the compilation context, \lip{context}, and the abstract syntax tree of the expression, \lip{node} and must return a type assignment for the node, or raise an \lip{ace.TypeError} if there is an error. In this case, a type assignment is possible if the attribute name is either \lip{"ni"} or \lip{"i"}, and an error is raised otherwise (lines \ref{cplx}.8-\ref{cplx}.10). We note that error messages are an important and sometimes overlooked facet of {ease-of-use} \cite{marceau2011measuring}. A common frustration with using general-purpose abstraction mechanisms to encode an abstraction is that they can produce  verbose and cryptic error messages that reflect the implementation details instead of the semantics. \texttt{typy} supports custom error messages.%Active types permit direct encoding of semantics and customization of error messages.
% %
% %Complex numbers also support binary arithmetic operations partnered with both other complex numbers and with non-complex numbers, treating them as if their imaginary component is zero. The typechecking rules for this logic is implemented on lines \ref{cplx}.17-\ref{cplx}.29. Arithmetic operations are usually symmetric, so the dispatch protocol checks the types of both subexpressions for support. To ensure that the semantics remain deterministic in the case that both types support the binary operation, \texttt{typy} asks the left first (via \lip{type_BinOp_left}), asking the right (via \lip{type_BinOp_right}) only if the left indicates an error. In either position, our implementation begins by recursively assigning a type to the other operand in the current context via the \lip{context.type} method (line \ref{cplx}.24). If supported, it applies the C99 rules for arithmetic operations to determine the resulting type (via \lip{c99_binop_t}, not shown). 
% %
% %Finally, a complex number can be constructed inside an \texttt{typy} function using \texttt{typy}'s special constructor form: \lip{[clx.Cplx](3,4)} represents $3+4i$, for example. The term within the braces is evaluated at \emph{compile-time}. Because \lip{clx.Cplx} evaluates not to an active type, but to a class, this form is assigned a type by handing control to the class object via the \emph{class method} \lip{type_New}. It operates as expected, extracting the types of the two arguments to construct an appropriate complex number type (lines \ref{cplx}.50-\ref{cplx}.57), raising a type error if the arguments cannot be promoted to a common type according to the rules of C99 or if two arguments were not provided.% (an exercise for the reader: modify this method to also allow a single argument for when the imaginary part is 0). 
% %
% %As seen in Listing \ref{mapout}, we are implementing complex numbers internally using OpenCL vector types, like \lip{int2}. Let us look first at \lip{trans_New} on lines \ref{cplx}.54-\ref{cplx}.60, where new complex numbers are translated to vector literals by invoking \lip{target.VecLit}. This will ultimately  generate the necessary OpenCL code, as a string, to complete compilation (these strings are not directly manipulated by extensions, however, to avoid problems with, e.g. precedence). For it to be possible to reason compositionally about the correctness of compilation, all complex numbers must translate to terms in the target language that have a consistent target type. The \lip{trans_type} method of the \lip{ace.ActiveType} associates a type in the target language, here a vector type like \lip{int2}, with the active type. \texttt{typy} supports a mode where this \emph{representational consistency} is dynamically checked during compilation (requiring that the active target know how to assign types to terms in the target language, which can be done  for our OpenCL target as of this writing).
% %
% %The translation methods for attributes (line \ref{cplx}.12) and binary operations  (line \ref{cplx}.31) proceed in a straightforward manner. The context provides a method, \lip{trans}, for recursively determining the translation of subexpressions as needed. Of note is that the translation methods can assume that typechecking succeeded. For example, the implementation of \lip{trans_Attribute} assumes that if \lip{node.attr} is not \lip{'ni'} then it must have been \lip{'i'} on line \ref{cplx}.14, consistent with the implementation of \lip{type_Attribute} above it. Typechecking and translation are separated into two methods to emphasize that typechecking is not target-dependent, and to allow for more advanced uses, like type refinements and hypothetical typing judgements, that we do not describe here.
% %
% %\begin{codelisting}
% %\lstinputlisting{listing9.py}
% %\caption{\texttt{[}in \texttt{examples/clx.py]} The active type family \texttt{Ptr} implements the semantics of OpenCL pointer types.}
% %\label{cplx}
% %\end{codelisting}
% %\section{Expressiveness}\label{examples}
% %Thus far, we have focused mainly on the OpenCL target and shown examples of fairly low-level active types: those that implement OpenCL's primitives (e.g. \lip{clx.Ptr}), extend them in simple but convenient ways (e.g. \lip{clx.Cplx}) and those that make interactive execution across language boundaries safe and convenient (e.g. \lip{clx.NPArray}). 
% %\texttt{typy} was first conceived to answer the question: \emph{can we build a statically-typed language with the semantics of a C but the syntax and ease-of-use of a Python?} We submit that the work described above has answered this question in the affirmative. 
% %
% %But \texttt{typy} has proven useful for more than low-level tasks like programming a GPU with OpenCL. We now describe several interesting extensions that implement the semantics of primitives drawn from a range of different language paradigms, to justify our claim that these mechanisms are highly expressive. 
% %
% %\subsection{Growing a Statically-Typed Python Inside an \texttt{typy}}
% %\texttt{typy} comes with a target, base and type implementing Python itself: \lip{ace.python.python}, \lip{ace.python.base} and \lip{ace.python.dyn}. These can be supplemented by additional active types and used as the foundation for writing actively-typed Python functions. These functions can either be compiled ahead-of-time to an untyped Python file for execution, or be immediately executed with just-in-time compilation, just like the OpenCL examples we have shown. Many of the examples in the next section support this target in addition to the OpenCL target we have focused on thus far.
% %
% %\section{Recursive Labeled Sums}
% %Listing 12 shows how recursive labeled sums (i.e. functional datatypes) would look.
% %\begin{codelisting}
% %\lstinputlisting{datatypes_t.py}
% %\caption{\texttt{[datatypes\_t.py]} An example using statically-typed functional datatypes.}
% %\label{datatypest}
% %\end{codelisting}
% %\begin{codelisting}
% %\lstinputlisting{datatypes.py}
% %\caption{\texttt{[datatypes.py]} The dynamically-typed Python code generated by running \texttt{acec datatypes\_t.py}.}
% %\label{datatypes}
% %\end{codelisting}
% %Listing \ref{datatypest} shows an example of the use of statically-typed functional datatypes (a.k.a. recursive labeled sum types) together with the Python implementation just described. It shows two syntactic conveniences that were not mentioned previously: 1) if no target is provided to \lip{ace.fn}, the base can provide a default target (here, \lip{ace.python.python}); 2) a concrete function can be generated immediately by providing a type assignment after \lip{ace.fn} using braces. Listing \ref{datatypest} generates Listing \ref{datatypes}.
% %
% %Lines \ref{datatypest}.3-\ref{datatypest}.11 define a function that generates a recursive algebraic datatype representing a tree given a name and another \texttt{typy} type for the data at the leaves. This type implemented by the active type family \lip{fp.Datatype}. A name for the datatype and the case names and types are provided programmatically on lines \ref{datatypest}.3-\ref{datatypest}.8. To support recursive datatypes, the case names are enclosed within a lambda term that will be passed a reference to the datatype itself. These lines also show two more active type families: units (the type containing just one value), and immutable pairs. Line \ref{datatypest}.13 calls this function with the \texttt{typy} type for dynamic Python values, \lip{py.dyn}, to generate a type, aliased \lip{DT}. This type is implemented using class inheritance when the target is Python, as seen on lines \ref{datatypes}.1-\ref{datatypes}.13. For C-like targets, a \lip{union} type can be used (not shown).
% %
% %The generic function \lip{depth_gt_2} demonstrates two features. First, the base has been setup to treat the final expression in a top-level branch as the return value of the function, consistent with typical functional languages. Second, the \lip{case} ``method'' on line \ref{datatypest}.17 creatively reuses the syntax for dictionary literals to express a nested pattern matching construct. The patterns to the left of the colons are not treated as expressions (that is, a type and translation is never recursively assigned to them). Instead, the active type implements the standard algorithm for ensuring that the cases cover all possibilities and are not redundant \cite{pfpl}. If the final case were omitted, for example, the algorithm would statically indicate an error, just as in statically-typed functional languages like ML.
% %
% %%\subsection{Nested Data Parallelism}
% %%Functional constructs have shown promise as the basis for a nested data-parallel parallel programming model. Many powerful parallel algorithms can be specified by composing primitives like \lip{map} and \lip{reduce} on persistent data structures like lists, trees and records. Data dependencies are directly encoded in these specifications, and automatic code generation techniques have shown promise in using this information to automatically execute these specifications on concurrent hardware and networks. The Copperhead programming language, for example, is based in Python as well and allows users to express functional programs over lists using common functional primitives, compiling them to CUDA code \cite{catanzaro2011copperhead}. By combining active types and the metaprogramming facilities described in Sec. 2 to implement optimizations, like fusion, on the typed syntax trees available from concrete functions, these same techniques can be implemented as an \texttt{typy} library as well.
% %
% %\subsection{Product Types}
% %Product types (types that group heterogeneous values together) like structs, unions, tuples, records and objects can all be implemented as \texttt{typy} type families  parameterized by a dictionary mapping field names (indices, in the case of tuples) to types. Each family differs slightly in the semantics of the operations available. Structs support mutation, for example, while records do not. Object systems typically introduce additional logic for calling methods, binding special variables like \lip{this}, and accessing information through an inheritance hierarchy. Listing \ref{ooclxfppy} shows an example of each of these. The prototypic object system delegates to a backing record if a field is not available in the foreground. This example also demonstrates cross-compilation to C99, supported for a subset of operations in \lip{clx}. All three abstractions are implemented as simple \lip{struct}s, as can be seen in Listing \ref{ooclxfpc}. This example further demonstrates the use of argument annotations to generate concrete functions (line \ref{ooclxfppy}.9), which is a feature only available when using Python 3.3+. 
% %
% %\subsection{Distributed Programming}
% %Many distributed programming abstractions can be understood as implementing object models with complex forms of dynamic dispatch. For example, in Charm++, dynamic dispatch involves message passing over a dynamically load-balanced network \cite{kale2009charm++}. While Charm++ is a sophisticated system, and we do not anticipate that implementing it as an \texttt{typy} extension would be easy, it is possible to do so by targeting a system like Adaptive MPI, which exposes the Charm++ runtime system \cite{kale2009charm++}. %Erlang and other message-passing and actor-based systems can also be considered in this manner -- processes can be thought of as objects, channels as methods and messages as calls.
% %
% %%\subparagraph{Annotation and Extension Inference}
% %%In addition to type annotations, OpenCL normally asks for additional annotations in a number of other situations.  Users can annotate functions that meet certain requirements  with the \lip{__kernel} attribute, indicating that they are callable from the host. The \lip{OpenCL} backend can check these and add this annotation automatically. Several types (notably, \lip{double}) and specialized primitive functions require that an OpenCL extension be enabled via a \lip{#pragma}. The OpenCL backend automatically detects many of these cases as well, adding the appropriate \lip{#pragma} declaration. An example of this can be seen in Listing \ref{mapout}, where the use of the \lip{double} type triggers the insertion of the \lip{cl_khr_f64} extension.
% %%
% %
% %A number of recent languages designed for distributed programming on clusters use a partitioned global address space model, e.g. Chapel \cite{chapel}. These languages provide first-class support for accessing data transparently across a massively parallel cluster. Their type systems track information about \emph{data locality} so that the compiler can emit more efficient code. The extension mechanism can also track this information in a manner analogous to how address space information is tracked in the OpenCL example above, and target an existing portable run-time such as GASNet \cite{bonachea2002gasnet}.
% %
% %\subsubsection{Units of Measure}
% %A number of domain-specific type systems can be implemented within \texttt{typy} as well. For example, prior work has considered tracking units of measure (e.g. grams) statically to validate scientific code \cite{conf/cefp/Kennedy09}. This cannot easily be implemented using many existing abstraction mechanisms because this information should only be maintained statically to avoid excessive run-time overhead associated with tagging/boxing, and the typechecking logic is reasonably complex. The \texttt{typy} extension mechanism allows this information to be tracked in the type system, but not included during translation, by a mechanism similar to how address space information is tracked with pointers. Because \texttt{typy} extensions can use Python, the needed logic can be implemented directly.
% %
% %
% %\subsection{OOFPCLX}
% %\begin{codelisting}[t]
% %\lstinputlisting{ooclxfp.py}
% %\caption{\texttt{[ooclxfp.py]} An example combining structs and immutable records using a prototype-based object system, cross-compiled to C99. Uses Python 3 argument annotations.}
% %\label{ooclxfppy}
% %\end{codelisting}
% %\begin{codelisting}[t]
% %\lstinputlisting[style=OpenCL]{ooclxfp.c}
% %\caption{\texttt{[ooclxfp.c]} The C99 code generated by running \texttt{acec ooclxfp.py}.}
% %\label{ooclxfpc}
% %\end{codelisting}
\clearpage
\appendix
\section*{Appendix}
\section{Syntactic Support for Python 2.x}\label{sec:python2-support}
\begin{codelisting}[h!]
\vspace{-3px}
\begin{lstlisting}
@fn
def map(f, t):
  {f : fn[+a, +b], t : tree(+a)} > tree(+b)
  [t].match 
  with Empty: Empty 
  with Node(left, right): 
    Node(map(f, left), map(f, right))
  with Leaf(x):
    Leaf(f(x)) 
\end{lstlisting}
\caption{An alternative syntax for argument and return type annotations.}
\label{fig:python2-alt-syntax}
\end{codelisting}

\noindent Python 3.0 introduced syntax for argument and return type annotations on function definitions \cite{pep3107}. However, Python 2.6+ remains widely adopted because Python 3.0+ did not maintain backwards compatibility. As such, our implementations of \li{typy.std.fn} and \li{typy.opencl.kernel} support an alternative syntax for argument and return type annotations that supports Python 2.6+ as well as Python 3.0+. For example, Listing \ref{fig:python2-alt-syntax} shows the function \li{map} from Listing \ref{fig:patterns} written using this alternative syntax.



% \section{Type Equivalence}\label{sec:type-equality}
% Our technique for checking type equivalence is essentially standard in most respects \cite{StoneHarper00}. The main departure has to do with types in canonical form: two types in canonical form are equivalent if they arise from the same fragment and have equivalent index values as determined by Python's \lip{==} operator. This operator can be overloaded to introduce interesting equivalences. For example,  we can freely reorder the fields in the definition of \lip{Account} because Python dictionaries are not order-preserving. Fragments are responsible for ensuring for the operations that they define that typing respects type equivalence. 

\section{Python Interoperability}\label{appendix:py}
\begin{codelisting}[h!]
\begin{lstlisting}
from typy import component
from typy.std import str
x = 3
@component
def TestLift():
  y = x # y has type std.py

  with let[z : str]: # block let
    [x].match
    with int(n): "n : std.int"
    with float(f): "f : std.float"
    with str(s): "s : std.string"
    with list(xs): "xs : std.list[py]"
    with tpl[2](xs): "xs : std.tpl[py, py]"
    with {'a': a}: "a : py"
    with instance[C](x): "x : py"
    with _: "otherwise"
\end{lstlisting}
\caption{Lifting and pattern matching over \li{py}.}
\label{fig:lifting}
\end{codelisting}

\noindent The \li{py} fragment defines a single type, \li{py[()]} (which can be abbreviated \li{py}, per Sec. \ref{sec:types}.) This type classifies Python values. Variables that refer to values in the static environment  are assigned type \li{py} by default. (Technically, the default is the canonical type that \li{typy.lift_type} is set to. Importing \li{typy.std} sets \li{typy.lift_type} to \li{typy.CanonicalType(py, ())}.) 
 For example, the component member \li{y} in Listing \ref{fig:lifting} refers to \li{x}, so it has type \li{py}.

In addition to standard operations, \li{py} supports pattern matching on the Python dynamic type tag or class, as shown in Listing \ref{fig:lifting}. This is consistent with a view of Python as a uni-typed language with a rich dynamic tag structure \cite{pfpl,scott1980lambda}. 

Most fragments in \li{typy.std} also support a coercion back to type \li{py} by accessing the ``attribute'' \li{.to_py}. For example, if \li{x : std.tpl[py, py]} (a pair of \li{py} values), then \li{x.to_py} is of type \li{py}. All arguments to Python functions of type \li{py} must be of type \li{py} (we are exploring coercions between \li{std.fn} and \li{std.py}, but have not implemented this as of this writing.)

The recommended way to interact with $\typy$ code from a Python script is simply to define a $\typy$ component. This is straightforward because $\typy$ is itself a library -- it does not require that an entire project be compiled using a different toolchain. However, for convenience, Python code can directly access values of type \li{py} that appear in a $\typy$ component. Internally, the fragment system asks fragments that wish to expose themselves to untyped code to indicate this by inheriting from \li{typy.PythonInterop}. They must also implement a coercion method \li{coerce_python} that is called to transform the value upon first access. For \li{py}, this is simply the identity function. We have also experimented with allowing calls to \li{std.fn} values that take and return \li{py} values -- here, \li{coerce_python} wraps the function with a check to ensure that the correct number of arguments have been provided. 


\section{OCaml Example}\label{sec:ocaml-examples}
\begin{codelisting}[h!]
\vspace{-3px}
\begin{lstlisting}[language=ML, morekeywords={module,match}]
module Listing7 = 
struct
  type 'a tree = 
  | Empty
  | Node of 'a tree * 'a tree
  | Leaf of 'a

  let map (f : 'a -> b, 
           tree : 'a tree) : 'b tree  =
    match tree with 
    | Empty -> Empty
    | Node(left, right) -> 
        Node(map(f, left), map(f, right))
    | Leaf(x) -> 
        Leaf(f(x))
end
\end{lstlisting}
\caption{An OCaml module analagous to the component defined in Listing \ref{fig:patterns}.}
\label{fig:ocaml-example-1}
\end{codelisting}

$\typy$'s fragment system, together with its primitive support for recursive and polymorphic types, is powerful enough to capture standard idioms in languages like OCaml at very similar syntactic cost -- compare Listing \ref{fig:ocaml-example-1} to Listing \ref{fig:patterns}. 

\section{Match Expressions}\label{appendix:match-expressions}
\begin{codelisting}[h!]
\vspace{-3px}
\begin{lstlisting}
[scrutinee].match[
  pat^$_1$^: branch^$_n$^, 
  # ...
  pat^$_n$^: branch^$_n$^]\end{lstlisting}
\caption{Expression-level pattern matching.}
\label{fig:match-exp}
\end{codelisting}

Python's syntax distinguishes statements from expressions. In Sec. \ref{sec:pattern-matching}, we described a statement-level match expression. It is also sometimes useful to pattern match inside an expression, so $\typy$ supports an expression-level match expression, shown in Listing \ref{fig:match-exp}, that operates analagously to the statement-level match expression. 

\begin{lstlisting}

\end{lstlisting}

\end{document}
