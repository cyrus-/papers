\documentclass{llncs}
\usepackage{amsmath}
\usepackage{llncsdoc}
\usepackage{amssymb} 
%\usepackage{amsthm}
\usepackage{ stmaryrd }
\usepackage{mathpartir}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\newcommand{\copyleft}{\reflectbox{\sffamily\copyright}}
%\input{macros-atlam}
\input{macros-catlam}
%\usepackage{cite}
%\renewcommand{\citepunct}{,\,} % IEEEtran wants to use ],\,[ for this but that looks dumb...

\renewcommand{\ttdefault}{txtt}
\usepackage{alltt}
\usepackage{listings}
\lstset{language=ML,
showstringspaces=false,
basicstyle=\ttfamily\footnotesize,
morekeywords={newcase,extends}}

\usepackage{float}
\floatstyle{ruled}
\newfloat{codelisting}{tp}{lop}
\floatname{codelisting}{Listing}

\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.

\usepackage{placeins}

%\lefthyphenmin=4
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{todonotes}
\lefthyphenmin=7
\sloppy

\begin{document}
\mainmatter  % start of an individual contribution

% first the title is needed
\title{Modular Type Constructors}
\subtitle{Conservatively Composing Typed Language Fragments}
% a short form should be given in case it is too long for the running head
\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Cyrus Omar%
\and Jonathan Aldrich}
%
\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Carnegie Mellon University, Pittsburgh, PA 15213, USA\\
\texttt{\{comar,aldrich\}@cs.cmu.edu}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
Abstraction providers sometimes need to extend a programming language with new base types and their corresponding operators. 
Ideally, such language fragments could be defined and reasoned about separately. Unfortunately, existing tools and techniques do not come equipped with strong modular reasoning principle -- fundamentally, %comparable to those available for library-based embeddings packaged using a modern module system. 
each combination of fragments becomes its own dialect of the language for which metatheoretic and compiler correctness results must be established monolithically. 
Recent work has started to address this problem, e.g. by showing how to modularly reason about desugarings atop a fixed type system. Our focus here is on safely composing type system fragments. We organize fragments around type constructors, as is usual practice when describing type systems, and sidestep the difficulties of composing abstract syntax (i.e. the expression problem)  by delegating control over a small, uniform abstract syntax in a type-directed manner to logic associated with a type constructor.
The paper is organized around a minimal calculus, @$\lambda$, that permits surprisingly practical examples. We establish several strong semantic guarantees, notably \emph{type safety}, \emph{stability of typing} under extension and \emph{conservativity}: 
that the \emph{type invariants} that a finite set of fragments maintain are conserved under  extension. 
This involves lifting typed compilation techniques into the semantics and enforcing an abstraction barrier around each fragment using a form of ``internal'' type abstraction. 
Conservativity then follows from classical parametricity results, so these \emph{modular type constructors} can be reasoned about like modules in an ML-like language. 
%\keywords{extensible languages; typed compilation; type-level computation; type abstraction}
\end{abstract}

\section{Introduction}
Typed programming languages are often described in \emph{fragments}, each defining contributions to a language's concrete syntax, abstract syntax, static semantics and dynamic semantics. 
In his textbook, Harper organizes fragments around type constructors, each introduced in a different chapter \cite{pfpl}. A language is then identified by a set of type constructors, e.g. $\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\keyw{1}}\,{\times}\,{+}\}$ is the language that builds in partial function types,  polymorphic types, recursive types, nullary and binary product types and binary sum types (its syntax is shown in Figure \ref{syntax-IL}, discussed further below).
It is left implied that the metatheory developed separately is conserved when fragments like these are composed to form a language.  

Luckily, fragment composition is not an everyday programming task, because fragments like these are ``general purpose'' in that they make it possible to construct \emph{isomorphic embeddings} of many other fragments as ``libraries''. For example, lists can be placed in isomorphism with polymorphic recursive sum of products, $\iforall{\alpha}{\imu{t}{\iunit + (\alpha \times t))}}$. Languages providing datatypes in the style of ML  are perhaps most directly oriented around embeddings like this (preserving type disequality requires adding some form of generativity, as ML datatypes also expose). % Leaving the core language simple makes it easier to establish its metatheory and verify that tools, like compilers, are implemented correctly.% They also have encouraging connections to logic. %For example, we do not need to define the type constructor $\fvar{list}$ (indexed by a type) as a fragment (though it is possible to do so) because there is a user-defined datatype constructor, $\texttt{list}$, parameterized by a type, that is isomorphic. If we added the $\fvar{list}$ fragment to the language, it would be entirely redundant semantically: every well-typed term of a type constructed by $\fvar{list}$ corresponds to a well-typed term and a polymorphic recursive sum type such th%Although strings and numbers can be embedded as recursive sum types, it is recognized that this is impractical, so these are also usually included as primitives.
%Establishing an isomorphic embedding of a desirable fragment in terms of general-purpose fragments is not always possible, nor are such embeddings always practical. 

Unfortunately, situations do sometimes arise where using these fragments to establish an isomorphic embedding that preserves a desirable fragment's  static and dynamic semantics (including bounds specified by its cost semantics) is not possible. 
Embeddings can also sometimes be unsatisfyingly \emph{complex}, as measured by the cost of the extralinguistic computations that are needed to map in and out of the embedding and, if these  must be performed mentally, considering  cognitive metrics like  error message comprehensibility. %We will discuss specific examples below.

When an embedding is too complex, abstraction providers have a few options, discussed in Sec. \ref{prior-work}. When an embedding is not possible, however, or when these options are  insufficient, providers are often compelled to introduce these fragments by extending an existing language, thereby forming a new \emph{dialect}. To save effort, they may do so by forking existing artifacts or leverage tools like compiler generators or language frameworks, also reviewed in Sec. \ref{prior-work}. %Indeed, the proliferation of language dialects constructed for this reason might be taken as an evidence that the core language is not as ``general'', when considered comprehensively, as might be hoped. 
Within the ML lineage, dialects that go  beyond $\mathcal{L}\{{\rightharpoonup}\,{\forall}\,{\mu}\,{\keyw{1}}\,{\times}\,{+}\}$ abound:
%Reynolds, in a remark that recalls the ``Turing tarpit'' of Perlis \cite{Perl82a}, summarizes the issue \cite{Reynolds94anintroduction}: 
%\begin{quote}
%To say that any reasonable function can be expressed by some program is not to say that it can be expressed by the most reasonable program. It is clear that the language requires a novel programming style. Moreover, it is likely that certain important functions cannot be expressed by their most efficient algorithms.
%\end{quote}
%

\begin{enumerate}
\compresslist
\vspace{5px}
\item 
\textbf{General Purpose Fragments:} 
A number of variations on product types, for example, have been introduced in dialects: 
$n$-ary tuples, 
labeled tuples, 
records (identified up to reordering), 
structurally typed or row polymorphic records \cite{Cardelli:1984:SMI:1096.1098}, 
records with update and extension operators \cite{ocaml-manual}, 
mutable fields \cite{ocaml-manual}, 
%field delegation \cite{atlang-gpce14} \todo{gpce submission} 
and 
``methods'' (i.e. pure objects \cite{conf/oopsla/Aldrich13, TSLs}).\footnote{The Haskell wiki notes that ''No, extensible records are not implemented in GHC. The problem is that the record design space is large, and seems to lack local optima. And all reasonable variants break backward compatibility. As a result, nothing much happens.'' \cite{GHCFAQ}} 
 Sum types are also exposed in various ways: 
finite datatypes, 
open datatypes \cite{conf/ppdp/LohH06}, 
hierarchically open datatypes \cite{journals/toplas/MillsteinBC04}, 
polymorphic variants \cite{ocaml-manual} and 
ML-style exception types. Other generally useful fragments are also built in, e.g. \verb|sprintf| in the OCaml dialect statically distinguishes format strings from strings.

\item
\textbf{Specialized Fragments:} Fragments that track specialized static invariants to provide stronger correctness guarantees, manage unwieldy lower-level abstractions or control cost are also often introduced in dialects, e.g. for data parallelism  \cite{chakravarty2007data}, distributed programming \cite{Murphy:2007:TDP:1793574.1793585}, reactive programming \cite{mandel2005reactiveml}, authenticated data structures \cite{Miller:2014:ADS:2535838.2535851}, databases \cite{Ohori:2011:MSM:2034773.2034815} and units of measure \cite{conf/cefp/Kennedy09}.% All of these are implemented as dialects of existing languages, presumably because a strong encoding was not feasible.

\item
\textbf{Foreign Fragments:} A safe and natural foreign function interface (FFI) can be a valuable feature (particularly given this proliferation of dialects). However, this requires enforcing the type system of the foreign language in the calling language. %Using  a FFI that does not do this can lead to safety issues, even when both languages are separately known to be safe. 
%Safe FFIs generally require direct extensions to the language. 
For example, MLj builds in a safe FFI to Java \cite{Benton:1999:IWW:317636.317791}.
\end{enumerate}
\vspace{-5px}


This dialect-oriented state of affairs is, we argue, unsatisfying: a programmer can choose either a dialect supporting a principled approach to distributed programming, or one that builds in support for statically reasoning about units of measure, but there may not be an available dialect supporting both. Combining dialects is non-trivial in general, as we will discuss. Using different dialects separately for different components of a program is also untenable: components written in different dialects cannot always interface safely with one another (i.e. an FFI, item 3  above, is needed). 

These problems do not arise when a fragment can be exposed as an isomorphic embedding because modern \emph{module systems} can enforce abstraction barriers that ensure that the isomorphism need only be established in the ``closed world'' of the module. This is useful because it does not impose proof obligations on clients in the ``open world''.  %Mechanisms that can help decrease the complexity of an embedding without violating abstraction barriers are thus valuable, and we will lead into our work in Sec. \ref{prior-work} by summarizing them. 
For situations where an embedding is not evident, however, mechanisms are needed that make specifying, implementing and reasoning about direct fragment composition similarly modular. Such mechanisms could ultimately be integrated directly into a language, blurring the distinction between fragments and libraries and decreasing the need for new dialects. Importing fragments that introduce new syntax and semantics   would be as easy as importing a new module is today. In the limit, the community could rely on modularly mechanized metatheory and compiler correctness results, rather than requiring heroic efforts on the part of individual research groups as the situation exists today. %Recent work has shown progress on modularly introducing new concrete syntax, reviewed in Sec. \ref{desugaring}. Our focus is on the problem of introducing new semantics.

\paragraph{Contributions} In this paper, we take substantial steps towards this goal by  constructing a minimal but powerful core calculus, @$\lambda$ (the ``actively typed'' lambda calculus). %Despite its minimality, it can host a variety of practical semantic extensions like those described above, while maintaining strong metatheoretic guarantees and, crucially, providing modular reasoning properties. %a calculus introduced briefly in recent work on \emph{active type constructors}  \cite{atlang-gpce14}\todo{TR? Arxiv?}, reviewed in Sec. \ref{overview}. 
This calculus is structured in a manner similar to the Harper-Stone semantics for Standard ML \cite{Harper00atype-theoretic}, consisting of an \emph{external language} (EL) governed by a {typed translation semantics} targeting a fixed \emph{internal language} (IL). 
Rather than building in a monolithic set of type constructors, however, the typechecking judgement is indexed by a \emph{tycon context}. Each tycon defines the semantics of its associated operators via functions written in a \emph{static language} (SL). Types are values in the SL.%, which provides a form of type-level computation. %The authors demonstrate the expressive power of this technique primarily with an implementation and a number of examples of fragments as libraries, and their core calculus .% (unlike systems that treat the type system as a ``bag of rules'', where non-determinism can arise). 

We introduce @$\lambda$ by example in Sec. \ref{overview}, demonstrating its surprising expressive power and detailing its semantics, then examine its key metatheoretic properties in Sec. \ref{calculus}, beginning with \emph{type safety} and \emph{unicity of typing} and touching on  \emph{decidability of typechecking}. We then consider properties that relate to composition of tycon definitions: \emph{hygiene}, \emph{stability of typing} and a key modularity result, which we refer to as  \emph{conservativity}: any invariants that can be established about all values of a type under \emph{some} tycon context (i.e. in some  ``closed world'') are conserved in any further extended tycon context (i.e. in the ``open world''). 

We combine several interesting type theoretic techniques, applying them to novel ends: 1)  a bidirectional type system permits flexible reuse of a fixed syntax; 2) the SL serves both as an extension language and as the type-level language; we give it its own statics (i.e. a \emph{kind system}); 3) we use a typed intermediate language and leverage corresponding \emph{typed compilation} techniques, here lifted into the semantics of the EL; 4) we leverage internal type abstraction implicitly as an effect during normalization of the SL to enforce abstraction barriers between type constructors. 
As a result, conservativity follows from the same elegant parametricity results that underly  abstraction theorems for module systems. 
Like modules, reasoning about these \emph{modular type constructors} does not require  mechanized specifications or proofs: correctness issues in the type constructor logic necessarily causes typechecking to fail, so even extensions that are not proven correct can be distributed and ``tested'' in the wild without compromising the integrity of an entire program (at worst, only values of types constructed by the tycon being tested may exhibit undesirable properties). 
% But mechanized proofs can be provided to verify that such failures (which do not indicate mistakes in client code but are more analagous to ``translation validation failures'' arising from the compiler \cite{Pnueli-Siegel-Singerman98}, will not occur during typechecking), 
% and that a type constructor adequately implements a separately specified fragment. We take the first steps towards such a \emph{modularly mechanized metatheory} by briefly discussing how to construct contextual embeddings of our semantics into Coq (Sec. \ref{coq}). 
%Though surprisingly expressive, we discuss some limitations of the core calculus, and suggest variants  that increase its expressive power while maintaining these key guarantees (Sec. \ref{variants}). 
We conclude by discussing some limitations, proposing some richer variants of the calculus and discussing related work (Sec. \ref{prior-work}). %Note that readers unfamiliar with prior approaches may wish to read this section first. %so that even fragments like those enumerated above can be defined as safely composable libraries, rather than in dialects that must be considered monolithically. %In the limit, building safe FFIs between every pair languages in a project is more effort than attempting to combine the languages. 
%Libraries are preferable to dialects, so there has been considerable interest in mechanisms that can make it possible to implement and reason about fragments like those above orthogonally and compose them automatically.% Mechanisms that guarantee composition will \emph{always} be safe can be integrated directly into a language as a general-purpose abstraction mechanism.  %that can fully automate the process of combining dialects defined atop a common framework (\emph{language-external mechanisms}).


% CANT GUARANTEE THAT SPECIFICATIONS ARE ACTUALLY DECIDABLE 
\bibliographystyle{abbrv}
\bibliography{../research}
\appendix
\section{Appendix}

\end{document}

