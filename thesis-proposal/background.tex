% !TEX root = omar-thesis-proposal.tex
\subsubsection{Background}\label{alibs}\label{background}
The term \emph{active libraries} was first introduced by Veldhuizen et al. \cite{activelibraries, active-libraries-thesis} to describe ``libraries that take an active role in compilation, rather than being passive collections of subroutines''. The authors suggested a number of reasons libraries might benefit by being able to influence the programming system at compile-time or edit-time, including high-level program optimization, checking programs for correctness against specialized criteria, reporting domain-specific errors and warnings, and ``rendering domain-specific textual and non-textual program representations and for interacting with such representations'' (anticipating interactions between libraries and tools other than just the compiler). 

% This stuff isn't actually hooking into the compiler...
The first concrete realizations of active libraries in statically typed settings, prompting the introduction of the term, were libraries that performed domain-specific program optimization at compile-time by exploiting language mechanisms that allow for limited compile-time computation. A prominent example in the literature is Blitz++, a library that uses C++ template metaprogramming to optimize compound operations on vectors and matrices by eliminating intermediate allocations \cite{veldhuizen2000blitz++}. Although this and several other interesting optimizations are possible by this technique, its expressiveness is fundamentally limited because template expansion allows for only the substitution of compile-time constants into pre-written code, and template metaprograms are notoriously difficult to read, write, debug and reason about (see discussion in \cite{Robison:2001:IEC:376656.376751}). %They are, however, composable, because templates are associated with a single function or class, and  the only metatheoretic issue is that template expansion can be non-terminating.

More powerful and direct compile-time \emph{term rewriting mechanisms} available in some languages can also be used for optimization, as well as for introducing specialized error checking logic and implementing new abstractions. These mechanisms  suffer from problems of composability, safety and expressiveness. For example, compile-time macros, such as those in MetaML \cite{Sheard:1999:UMS}, Template Haskell \cite{SheardPeytonJones:Haskell-02} and Scala \cite{scala-macros}, take full control over all of the code that they enclose. This can be problematic, however, as outer macros can interfere with the functionality of inner macros. Moreover, once a value escapes a macro's scope, there is no way to rely on the guarantees and features that were available within its scope, because the output of a macro is simply a term in the underlying language (a problem fundamentally related to the problem of relying on a common intermediate language,  described in Section \ref{external-approaches}). Thus, macros can be used to automate code generation, but not to globally extend the syntax or static guarantees of a language. It can also be difficult to reason about the semantics of code when any number of enclosing macros may be manipulating it, and to build tools that operate robustly in their presence.

Some term rewriting systems replace the explicitly delimited scoping of macros with global pattern-based dispatch. Xroma (pronounced ``Chroma''), for example, is designed around active libraries and allows users to insert custom rewriting passes into the compiler from within libraries \cite{activelibraries}. Similarly, the derivative of Haskell implemented by the Glasgow Haskell Compiler (GHC) allows providers to introduce custom compile-time term rewriting logic if an appropriate flag is passed in \cite{jones2001playing}. In both cases, the user-defined logic can dispatch on arbitrary patterns of code throughout the component or program the extension is activated within, so these mechanisms permit non-local extensions to the static and dynamic semantics. But libraries containing such global rewriting logic cannot be safely composed because two different libraries may attempt to rewrite the same piece of code differently. It is also difficult to guarantee that such logic is correct and difficult to reason about code when simply importing a library can change the semantics of the program in a highly non-local manner.
% http://www.haskell.org/ghc/docs/latest/html/users_guide/rewrite-rules.html

Another example of an active library approach to extensibility with non-local scope is SugarJ \cite{erdweg2011sugarj} and other languages generated by Sugar* \cite{erdweg2013framework}, like SugarHaskell \cite{erdweg2012layout}. These languages permit libraries to extend the base syntax of the core language in a nearly arbitrary manner, and these extensions are imported transitively throughout a program. Unfortunately, this flexibility again means that  extensions are  not safely composable. For example, a library that defines a literal syntax for HTML would conflict with another that defines a literal syntax for XML because they define differing semantics for some of the same syntactic forms. If SugarJ was used by two different regular expression engines to provide literal syntax for regular expression patterns, there could easily be conflicts at link-time because both will introduce many of the same notations but back them with differing implementations. 
And again, it is difficult to predict what an unfamiliar piece of syntax desugars into, leading to difficulties reading and reasoning about code.

%Another example comes from the Tau package for tuning and analyzing parallel programs, which allows libraries to instrument themselves declaratively so that internal details can be hidden from users of the tool \cite{tau}. 

%The researchers behind these tools often claim that such conflicts are rare. There have, to our knowledge, been no studies of this claim. Nevertheless, the core fact remains: conflicts can arise, which makes it difficult to rely on such libraries 

%There also does not appear to be a clear, well-founded theoretical foundation for this approach, nor for active libraries in general.

%\todo{Smooth this transition}Developers specify type-specific compile-time and edit-time features by equipping type families, when they are defined, with functions written in an appropriate metalanguage. These are selectively invoked by tools, such as the parser, type checker, translator and editor, when they work with expressions of that type, and only in those situations.

%\subsection{Example: First-Class Natural Numbers}
%To make our approach more concrete and emphasize its foundational character, let us begin with a very simple example, first-class natural numbers. If a programming system included first-class support for natural numbers, it might support behaviors like the following:
%
%\begin{enumerate}
%\item type checking rules that constrain uses of the three operators associated with the $\nat$ type, as defined in G\"{o}del's System \textbf{T} \cite{tapl}: $\natz$, $\nats{e}$ and $\natrec{e_1}{e_2}{e_3}$\todo{phrasing of his}
%\item type checker error messages that provide domain-specific assistance, like detecting when the trailing $\textbf{z}$ in a constant has been omitted: \todo{phrasing / merge this into 1?}
%\begin{verbatim}
%    s(s(s))
%        ^ (Line 1, Character 5)
%    Error: Operator s requires 1 argument of type nat. Provided 0 arguments.
%    Did you mean s(z)?
%\end{verbatim}
%\item translation into an efficient integer representation at run-time
%\item syntax support \todo{elaborate on syntax support for nat literals}
%\item editor support, such as the ability to instantiate nat constants more easily \todo{elaborate on this / reference Graphite section}
%%generate statistical predictions useful for code completion (e.g. biasing smaller constants) and other editor interactions, customized syntactic/visual representations and so on.
%\end{enumerate}
%


%\section{Related Work}
%We review existing approaches that are available to researchers and domain experts who wish to develop novel constructs below.
%
%\subsection{Language-Oriented Approaches}
%\subsubsection{Language Frameworks}
%A number of tools have been developed to assist with this task of developing new languages and their associated tools, including compiler generators, language workbenches and domain-specific language frameworks (see \cite{fowler2010domain} for a review). In some cases, these tools allow language features to be defined modularly and composed differentially to produce a variety of different languages. To our knowledge, none of these mechanisms guarantee that any combination of features can be safely composed, nor do they guarantee safety properties about the resulting languages. User-defined code is still ultimately compiled against a particular language, and because language composition cannot be automatic or guaranteed safe, interoperability with code written in other languages is thus limited by the difficulties described above despite the modular construction.
%
%\subsubsection{Extensible Compilers and Tools}
%A related methodology is to implement language features as compiler and tool extensions directly. A number of extensible compilers have been developed to support this approach (see \cite{clements2008comparison}). As with language frameworks, this approach can lead to \emph{de facto} implementation of a new language, since user libraries must be compiled against a particular combination of extensions, and these extensions cannot be guaranteed to compose in general. Moreover, a language may have multiple competing compilers and other tools. By relying on implementation-specific features of a single tool to define core semantics and behaviors, the clean conceptual separation between languages and tools is broken, leading to compatibility issues and hard-to-anticipate behaviors for user code. We argue that this approach should be considered harmful.
%
%\subsection{Library-Oriented Approaches}
%\subsubsection{Embedded DSLs}
%Embedded domain-specific languages are languages that creatively repurpose existing language constructs to create interfaces that resemble those of a distinct language. In languages with rich type systems, such as Haskell, this approach can be quite successful (e.g. \cite{svensson2011obsidian}). Ultimately, however, this approach is limited by the host system, and as discussed in Section 2, thus limits experts who want to express particularly novel constructs, relative to the host language. 
%
%\subsubsection{Term Rewriting Systems}
%Many languages and tools allow developers to rewrite expressions according to custom rules. These can broadly be classified as {\it term rewriting systems}. Macro systems, such as those characteristic of the LISP family of languages \cite{mccarthy1978history}, are the most prominent example. Some compile-time metaprogramming systems also allow users to manipulate syntax trees (e.g. MetaML \cite{Sheard:1999:UMS}), and external rewrite systems also exist for many languages. These systems are expressive if used correctly, but verifying correctness and non-interference properties is difficult for the same reason. Manipulating source trees directly is a complex task, even in languages with simple grammars like LISP. Finally, term rewriting systems focus on rewriting terms to support alternative language semantics but do not intrinsically support extensions that cover the full programming system.
%
%\subsubsection{Active Libraries}
%Active libraries, as proposed by Czarnecki et al. \cite{activelibraries}, ``are not passive collections of routines or objects, as are traditional libraries, but take an active role in generating code''. They go on to suggest a number of areas in which the library could interact with the programming system, including optimizing code, checking source code for correctness, reporting domain-specific errors and warnings, and ``rendering domain-specific textual and non-textual program representations and for interacting with such representations''. 
%
%This paper was largely a proposal. The concrete implementations of this concept have largely been term rewriting systems described above. One prominent example within the active libraries literature is Blitz++, which uses the C++ template expansion system as a metalanguage to support optimizations of array operations. An example of tool support comes from the Tau package for tuning and analyzing parallel programs. Tau allows libraries to instrument themselves declaratively to hide internal details and complex internal representations from users. A more extensive system with support for active libraries is Xroma. Xroma allows users to provide annotations that intercept compilation of a component at various stages of compilation to support rewriting, custom error handling and custom error checking. The approach taken by Xroma, by still requiring direct syntax manipulation and allowing arbitrary interception, is flexible but not compositional, and generally more complex than may be necessary. There also does not appear to be a clear, well-founded theoretical foundation for this approach, nor for active libraries in general.

%\subsection{Type-Level Computation} %Haskell, Ur and $\Omega$mega
%System XX with simple case analysis provides the basis of type-level computation in Haskell (where type-level functions are called type families \cite{Chakravarty:2005:ATC}). Ur uses type-level records and names to support typesafe metaprogramming, with applications to web programming \cite{conf/pldi/Chlipala10}. $\Omega$mega adds algebraic data types at the type-level, using these to increase the expressive power of algebraic data types at the expression level \cite{conf/cefp/SheardL07}. Dependently-typed languages blur the traditional phase separation between types and expressions, so type-level computation is often implicitly used (though not always in its most general form, e.g. Deputy \cite{conf/icfp/ChenX05}, ATS \cite{conf/esop/ConditHAGN07}.)

%\subsubsection{Run-Time Indirection}
%{\it Operator overloading} \cite{vanWijngaarden:Mailloux:Peck:Koster:Sintzoff:Lindsey:Meertens:Fisker:acta:1975} and {\it metaobject dispatch} \cite{Kiczales91} are run-time protocols that translate operator invocations into function calls. The function is typically selected according to the type or value of one or more operands. These protocols share the notion of {\it inversion of control} with type-level specification. However, type-level specification is a {\it compile-time} protocol focused on enabling specialized verification and implementation strategies, rather than simply enabling run-time indirection.

%\subsection{Specification Languages}
%Several {\it specification languages} (or {\it logical frameworks}) based on these theoretical formulations exist, including the OBJ family of languages (e.g. CafeOBJ \cite{Diaconescu-Futatsugi01}). They provide support for verifying a program against a language specification, and can automatically execute these programs as well in some cases. The  language itself specifies which verification and execution strategies are used.
%
%Type-determined compilation takes a more concrete approach to the problem, focusing on combining {\it implementations} of different\- logics, rather than simply their specifications. In other words, it focuses on combining {\it type checkers} and {\it implementation strategies} rather than more abstract representations of a language's type system and dynamic semantics. In Section 4, we outlined a preliminary approach based on proof assistant available for the type-level language to unify these approaches, and we hope to continue this line of research in future work.
%
